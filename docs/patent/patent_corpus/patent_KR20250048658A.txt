## Patent Reference

- Title: AI-based detection of genetic conservation and expression conservation at base resolution
- URL: https://patents.google.com/patent/KR20250048658A/en

### Abstract

Abstract Translated from Korean 개시된 기술은 유전자 보존 및 발현 보존을 검출하는 것에 관한 것이다. 특히, 개시된 기술은 진화 보존, 전사 개시 또는 후생적 신호를 나타낼 수 있는 염색질 형태로 시퀀스의 복수의 대체 표현(300)의 생성을 통해 염기 해상도에서 기준 유전 시퀀스(302)의 변이체와 비교하여 기준 유전 시퀀스(302)에 대한 유전자 보존 및 후생적 신호를 검출하는 것, 유전자 발현 대체 분류기(2700)에 복수의 대체 염색질 시퀀스를 매핑하여 변이체에 대한 유전자 발현 클래스 예측을 생성하는 것, 및 병원성 예측기에 대체 염색질 시퀀스를 매핑하여 변이체의 병원성을 검출하는 것에 관한 것이다.The disclosed technology relates to detecting genetic conservation and expression conservation. In particular, the disclosed technology relates to detecting genetic conservation and epigenetic signals for a reference genetic sequence (302) by comparing variants of the reference genetic sequence (302) to the reference genetic sequence (302) at base resolution by generating multiple alternative representations (300) of the sequence in a chromatin form that may represent evolutionary conservation, transcription initiation or epigenetic signals, mapping the multiple alternative chromatin sequences to a gene expression alternative classifier (2700) to generate gene expression class predictions for the variants, and detecting pathogenicity of the variants by mapping the alternative chromatin sequences to a pathogenicity predictor.

### Description

Description Translated from Korean 염기 해상도에서 유전자 보존 및 발현 보존의 인공 지능 기반 검출AI-based detection of genetic conservation and expression conservation at base resolution 개시된 기술은 인공 지능 유형 컴퓨터 및 디지털 데이터 처리 시스템 및 해당하는 데이터 처리 방법 및 지능 에뮬레이션 제품(즉, 지식 기반 시스템, 추론 시스템 및 지식 획득 시스템)에 관한 것이며; 불확실성이 있는 추론을 위한 시스템(예: 퍼지 로직 시스템), 적응 시스템, 기계 학습 시스템 및 인공 신경망을 포함한다. 특히, 개시된 기술은 염기 해상도에서 유전자 보존 및 발현 보존의 인공 지능 기반 검출에 관한 것이다.The disclosed technology relates to artificial intelligence type computer and digital data processing systems and corresponding data processing methods and intelligence emulation products (i.e., knowledge-based systems, inference systems and knowledge acquisition systems); including systems for inference with uncertainty (e.g., fuzzy logic systems), adaptive systems, machine learning systems and artificial neural networks. In particular, the disclosed technology relates to artificial intelligence-based detection of genetic conservation and expression conservation at base resolution. 기술분야Technical field 관련 출원의 교차 참조Cross-reference to related applications 본 출원은 발명의 명칭이 'ARTIFICIAL INTELLIGENCE-BASED EPIGENETICS AT BASE RESOLUTION'이고 동시에 출원된 미국 특허 출원(대리인 문서 번호 ILLM 1040-1/IP-2045-PRV)과 관련되며, 이는 본원에 완전히 명시된 것처럼 모든 목적을 위해 참조로서 통합된다.This application is related to concurrently filed U.S. patent application entitled 'ARTIFICIAL INTELLIGENCE-BASED EPIGENETICS AT BASE RESOLUTION' (Attorney Docket No. ILLM 1040-1/IP-2045-PRV), which is incorporated by reference for all purposes as if fully set forth herein. 참조 문헌References 다음은 본원에 전체적으로 명시된 것처럼 모든 목적을 위해 참고로 포함된다:The following is incorporated by reference herein for all purposes as if fully set forth herein: 발명의 명칭이 'ARTIFICIAL INTELLIGENCE-BASED EPIGENETICS'이고 2019년 9월 20일자로 출원된 미국 특허 출원 제62/903,700호(대리인 문서 번호 ILLM 1025-1/IP-1898-PRV);U.S. Patent Application No. 62/903,700, filed September 20, 2019, entitled “ARTIFICIAL INTELLIGENCE-BASED EPIGENETICS” (Attorney Docket No. ILLM 1025-1/IP-1898-PRV); 문헌[Sundaram, L. 등, Predicting the clinical impact of human mutation with deep neural networks. Nat. Genet. 50, 1161~1170 (2018년)];Sundaram, L. et al., Predicting the clinical impact of human mutation with deep neural networks. Nat. Genet . 50, 1161-1170 (2018)]; 문헌[Jaganathan, K. 등, Predicting splicing from primary sequence with deep learning. Cell 176, 535~548 (2019년)];Jaganathan, K. et al., Predicting splicing from primary sequence with deep learning. Cell 176, 535-548 (2019)]; 2017년 10월 16일자로 출원되고 발명의 명칭이 'TRAINING A DEEP PATHOGENICITY CLASSIFIER USING LARGE-SCALE BENIGN TRAINING DATA'인 미국 특허 출원 제62/573,144호(대리인 문서 번호 ILLM 1000-1/IP-1611-PRV);U.S. Patent Application No. 62/573,144, filed Oct. 16, 2017, entitled “TRAINING A DEEP PATHOGENICITY CLASSIFIER USING LARGE-SCALE BENIGN TRAINING DATA” (Attorney Docket No. ILLM 1000-1/IP-1611-PRV); 2017년 10월 16일자로 출원되고 발명의 명칭이 'PATHOGENICITY CLASSIFIER BASED ON DEEP CONVOLUTIONAL NEURAL NETWORKS (CNNs)'인 미국 특허 출원 제62/573,149호(대리인 문서 번호 ILLM 1000-2/IP-1612-PRV);U.S. Patent Application No. 62/573,149, filed Oct. 16, 2017, entitled “PATHOGENICITY CLASSIFIER BASED ON DEEP CONVOLUTIONAL NEURAL NETWORKS (CNNs)” (Attorney Docket No. ILLM 1000-2/IP-1612-PRV); 2017년 10월 16일자로 출원되고 발명의 명칭이 'DEEP SEMI-SUPERVISED LEARNING THAT GENERATES LARGE-SCALE PATHOGENIC TRAINING DATA'인 미국 특허 출원 제62/573,153호(대리인 문서 번호 ILLM 1000-3/IP-1613-PRV);U.S. Patent Application No. 62/573,153, filed Oct. 16, 2017, entitled “DEEP SEMI-SUPERVISED LEARNING THAT GENERATES LARGE-SCALE PATHOGENIC TRAINING DATA” (Attorney Docket No. ILLM 1000-3/IP-1613-PRV); 2017년 11월 7일자로 출원되고 발명의 명칭이 'PATHOGENICITY CLASSIFICATION OF GENOMIC DATA USING DEEP CONVOLUTIONAL NEURAL NETWORKS (CNNs)'인 미국 특허 출원 제62/582,898호(대리인 문서 번호 ILLM 1000-4/IP-1618-PRV);U.S. Patent Application No. 62/582,898, filed November 7, 2017, entitled “PATHOGENICITY CLASSIFICATION OF GENOMIC DATA USING DEEP CONVOLUTIONAL NEURAL NETWORKS (CNNs)” (Attorney Docket No. ILLM 1000-4/IP-1618-PRV); 2018년 10월 15일자로 출원되고 발명의 명칭이 'DEEP LEARNING-BASED TECHNIQUES FOR TRAINING DEEP CONVOLUTIONAL NEURAL NETWORKS'인 미국 특허 출원 제16/160,903호(대리인 문서 번호 ILLM 1000-5/IP-1611-US);U.S. Patent Application No. 16/160,903, filed Oct. 15, 2018, entitled “DEEP LEARNING-BASED TECHNIQUES FOR TRAINING DEEP CONVOLUTIONAL NEURAL NETWORKS” (Attorney Docket No. ILLM 1000-5/IP-1611-US); 2018년 10월 15일자로 출원되고 발명의 명칭이 'DEEP CONVOLUTIONAL NEURAL NETWORKS FOR VARIANT CLASSIFICATION'인 미국 특허 출원 제16/160,986호(대리인 문서 번호 ILLM 1000-6/IP-1612-US);U.S. Patent Application No. 16/160,986, filed Oct. 15, 2018, entitled “DEEP CONVOLUTIONAL NEURAL NETWORKS FOR VARIANT CLASSIFICATION” (Attorney Docket No. ILLM 1000-6/IP-1612-US); 2018년 10월 15일자로 출원되고 발명의 명칭이 'SEMI- SUPERVISED LEARNING FOR TRAINING AN ENSEMBLE OF DEEP CONVOLUTIONAL NEURAL NETWORKS'인 미국 특허 출원 제16/160,968호(대리인 문서 번호 ILLM 1000-7/IP-1613-US);U.S. Patent Application No. 16/160,968, filed Oct. 15, 2018, entitled “SEMI- SUPERVISED LEARNING FOR TRAINING AN ENSEMBLE OF DEEP CONVOLUTIONAL NEURAL NETWORKS” (Attorney Docket No. ILLM 1000-7/IP-1613-US); 2019년 5월 8일자로 출원되고 발명의 명칭이 'DEEP LEARNING-BASED TECHNIQUES FOR PRE-TRAINING DEEP CONVOLUTIONAL NEURAL NETWORKS'인 미국 특허 출원 제16/407,149호(대리인 문서 번호 ILLM 1010-1/IP-1734-US).U.S. Patent Application No. 16/407,149, filed May 8, 2019, entitled “DEEP LEARNING-BASED TECHNIQUES FOR PRE-TRAINING DEEP CONVOLUTIONAL NEURAL NETWORKS” (Attorney Docket No. ILLM 1010-1/IP-1734-US). 2021년 4월 15일자로 출원되고 발명의 명칭이 'DEEP CONVOLUTIONAL NEURAL NETWORKS TO PREDICT VARIANT PATHOGENICITY USING THREE-DIMENSIONAL (3D) PROTEIN STRUCTURES'인 미국 특허 출원 제17/232,056호(대리인 문서 번호 ILLM 1037-2/IP-2051-US);U.S. Patent Application No. 17/232,056, filed April 15, 2021, entitled “DEEP CONVOLUTIONAL NEURAL NETWORKS TO PREDICT VARIANT PATHOGENICITY USING THREE-DIMENSIONAL (3D) PROTEIN STRUCTURES” (Attorney Docket No. ILLM 1037-2/IP-2051-US); 2021년 4월 15일자로 출원되고 발명의 명칭이 'MULTI-CHANNEL PROTEIN VOXELIZATION TO PREDICT VARIANT PATHOGENICITY USING DEEP CONVOLUTIONAL NEURAL NETWORKS'인 미국 특허 출원 제63/175,495호(대리인 문서 번호 ILLM 1047-1/IP-2142-PRV);U.S. Patent Application No. 63/175,495, filed April 15, 2021, entitled “MULTI-CHANNEL PROTEIN VOXELIZATION TO PREDICT VARIANT PATHOGENICITY USING DEEP CONVOLUTIONAL NEURAL NETWORKS” (Attorney Docket No. ILLM 1047-1/IP-2142-PRV); 2021년 4월 16일자로 출원되고 발명의 명칭이 'EFFICIENT VOXELIZATION FOR DEEP LEARNING'인 미국 특허 출원 제63/175,767호(대리인 문서 번호 ILLM 1048-1/IP-2143-PRV); 및U.S. Patent Application No. 63/175,767, filed April 16, 2021, entitled “EFFICIENT VOXELIZATION FOR DEEP LEARNING” (Attorney Docket No. ILLM 1048-1/IP-2143-PRV); and 2021년 9월 7일자로 출원되고 발명의 명칭이 'ARTIFICIAL INTELLIGENCE-BASED ANALYSIS OF PROTEIN THREE-DIMENSIONAL (3D) STRUCTURES'인 미국 특허 출원 제17/468,411호(대리인 문서 번호 ILLM 1037-3/IP-2051A-US).U.S. Patent Application No. 17/468,411, filed September 7, 2021, entitled “ARTIFICIAL INTELLIGENCE-BASED ANALYSIS OF PROTEIN THREE-DIMENSIONAL (3D) STRUCTURES” (Attorney Docket No. ILLM 1037-3/IP-2051A-US). 이 섹션에서 논의되는 주제는 단지 이 섹션 내에서의 그의 언급의 결과로서 종래 기술이라고 가정되어서는 안 된다. 유사하게, 이 섹션에서 언급되거나 배경기술로서 제공되는 주제와 연관된 문제는 종래 기술에서 이전에 인식되었다고 가정되어서는 안 된다. 이 섹션에서의 주제는 단지 상이한 접근법을 표현할 뿐이며, 그 접근법 자체는 청구되는 기술의 구현에 또한 상응할 수 있다.The topics discussed in this section should not be assumed to be prior art merely as a result of their mention within this section. Similarly, it should not be assumed that problems related to the subject matter mentioned in this section or provided as background have been previously recognized in the prior art. The topics in this section merely represent different approaches, which themselves may also correspond to implementations of the claimed technology. 광범위한 의미에서, 기능 유전체학으로도 지칭되는 유전체학은 게놈 시퀀스 분석, 전사체 프로파일링 및 단백질체학과 같은 게놈 스케일 분석을 사용하여 유기체의 모든 게놈 요소의 기능을 특징짓는 것을 목표로 한다. 유전체학은 데이터 중심 과학으로서 발생하였으며 그것은 선입견이 있는 모델 및 가설을 검정하기보다는 게놈 스케일 데이터의 탐구로부터 신규 속성을 발견함으로써 작동한다. 유전체학의 응용은 유전자형과 표현형 사이의 연관성을 찾는 것, 환자 계층화에 대한 바이오마커를 발견하는 것, 유전자의 기능을 예측하는 것 및 전사 인핸서 및 단일 염기 다형성(SNP)과 같은 생물화학적 활성 게놈 영역 및 잔기를 차트화하는 것을 포함한다.In a broad sense, genomics, also referred to as functional genomics, aims to characterize the function of all genomic elements of an organism using genome-scale analyses such as genome sequence analysis, transcriptome profiling, and proteomics. Genomics emerged as a data-driven science that operates by discovering novel properties from the exploration of genome-scale data rather than testing preconceived models and hypotheses. Applications of genomics include finding associations between genotype and phenotype, discovering biomarkers for patient stratification, predicting gene function, and charting biochemically active genomic regions and residues such as transcriptional enhancers and single nucleotide polymorphisms (SNPs). 유전체학 데이터는 쌍별 상관의 시각적 연구만으로 조사하기에는 너무 크고 너무 복잡하다. 예를 들어, 단백질 시퀀스는 조상 단백질에서 유래되어 유사한 구조와 기능을 공유하는 상동 단백질 계열로 분류될 수 있다. 상동 단백질의 다중 시퀀스 정렬(MSA)의 분석은 기능적, 구조적 제약에 대한 중요한 정보를 제공한다. 아미노산 부위를 나타내는 MSA 열의 통계는 진화 중 보존되는 기능적 잔기를 식별한다. MSA 칼럼 사이의 아미노산 사용의 상관관계는 기능적 부분과 구조적 접촉에 대한 중요한 정보를 담고 있다.Genomic data are too large and complex to be explored solely through visual studies of pairwise correlations. For example, protein sequences can be classified into families of homologous proteins that are derived from an ancestral protein and share similar structures and functions. Analysis of multiple sequence alignments (MSAs) of homologous proteins provides important information about functional and structural constraints. Statistics of the MSA column representing amino acid sites identify functional residues that are conserved during evolution. Correlations of amino acid usage between MSA columns contain important information about functional moieties and structural contacts. 대신, 예상되지 않은 관계의 발견을 지원하기 위해, 신규한 가설 및 모델을 도출하기 위해, 그리고 예측을 행하기 위해 분석 툴이 요구된다. 가정 및 도메인 전문 지식이 하드 코딩되는 일부 알고리즘과는 달리, 기계 학습 알고리즘은 데이터에서 패턴을 자동으로 검출하도록 설계된다. 따라서, 기계 학습 알고리즘은 데이터 중심 과학, 특히 유전체학에 적합하다. 그러나, 기계 학습 알고리즘의 성능은 데이터가 표현되는 방법, 즉 각각의 변수(특징으로도 불림)가 계산되는 방법에 강하게 의존할 수 있다. 예를 들어, 형광 현미경 이미지로부터 종양을 악성 또는 양성으로 분류하기 위해, 전처리 알고리즘이 셀을 검출할 수 있고, 셀 유형을 식별할 수 있고, 각각의 셀 유형에 대한 셀 카운트의 목록을 생성할 수 있다.Instead, analytical tools are required to support the discovery of unexpected relationships, to generate novel hypotheses and models, and to make predictions. Unlike some algorithms where assumptions and domain expertise are hard-coded, machine learning algorithms are designed to automatically detect patterns in data. Therefore, machine learning algorithms are well suited to data-driven science, especially genomics. However, the performance of a machine learning algorithm can strongly depend on how the data is represented, that is, how each variable (also called a feature) is computed. For example, to classify a tumor as malignant or benign from a fluorescence microscopy image, a preprocessing algorithm could detect cells, identify cell types, and generate a list of cell counts for each cell type. 기계 학습 모델은 추정된 셀 카운트를 취할 수 있으며, 이러한 카운트는 종양을 분류하기 위한 입력 특징으로서, 수작업으로 작성된 특징의 예이다. 중심 문제는 분류 성능이 이러한 특징의 품질 및 관련성에 크게 의존한다는 것이다. 예를 들어, 셀 형태학, 셀 사이의 거리 또는 기관 내의 국지성과 같은 관련 시각적 특징은 셀 카운트에서 캡처되지 않고, 데이터의 이러한 불완전한 표현은 분류 정확도를 감소시킬 수 있다.A machine learning model can take the estimated cell counts and use these counts as input features to classify tumors, an example of hand-crafted features. The central problem is that classification performance strongly depends on the quality and relevance of these features. For example, relevant visual features such as cell morphology, distance between cells, or locality within an organ are not captured in cell counts, and this incomplete representation of the data can reduce classification accuracy. 기계 학습의 하위 구분인 심층 학습은 기계 학습 모델 자체에 특징의 계산을 임베딩하여 종단 간 모델을 산출함으로써 이러한 문제를 다룬다. 이러한 결과는 심층 신경망, 즉 연속적인 기본 연산을 포함하는 기계 학습 모델의 개발을 통해 실현되었으며, 이들은 선행 연산의 결과를 입력으로서 취함으로써 점점 더 복잡한 특징을 계산한다. 심층 신경망은 위의 예에서 셀 형태학 및 셀의 공간 구성과 같은 높은 복잡도의 관련 특징을 발견함으로써 예측 정확도를 개선할 수 있다. 심층 신경망의 구성 및 훈련은 데이터의 폭증, 알고리즘 진보 및 계산 용량의 실질적인 증가에 의해, 특히 그래픽 처리 장치(GPU)의 사용을 통해 가능하게 되었다.Deep learning, a subset of machine learning, addresses this problem by embedding the computation of features into the machine learning model itself, yielding an end-to-end model. These results were realized through the development of deep neural networks, machine learning models that involve successive elementary operations, which compute increasingly complex features by taking as input the results of previous operations. Deep neural networks can improve prediction accuracy by discovering high-complexity relevant features, such as cell morphology and spatial organization of cells in the above examples. The construction and training of deep neural networks has been made possible by the explosion of data, advances in algorithms, and substantial increases in computational power, particularly through the use of graphics processing units (GPUs). 지도 학습의 목표는, 특징을 입력으로서 취하고 이른바 타깃 변수에 대한 예측을 반환하는 모델을 획득하는 것이다. 지도 학습 문제의 일례는 정규 스플라이스 부위 시퀀스의 존재 여부 및 스플라이싱 분기점의 위치 또는 인트론 길이와 같은 RNA 상의 특징을 고려하여 인트론이 (타깃을) 스플라이스-아웃(splice out)되는지의 여부를 예측하는 것이다. 기계 학습 모델을 훈련시키는 것은 이의 파라미터를 학습하는 것을 지칭하며, 이는 보통 보이지 않은 데이터에 대한 정확한 예측을 행하는 목적으로 훈련 데이터에 대한 손실 함수를 최소화하는 것을 수반한다.The goal of supervised learning is to obtain a model that takes features as input and returns predictions for the so-called target variable. An example of a supervised learning problem is predicting whether an intron will be spliced out (of its target) given features of the RNA such as the presence of a canonical splice site sequence and the location of a splice branch point or the length of the intron. Training a machine learning model refers to learning its parameters, which usually involves minimizing a loss function on the training data with the goal of making accurate predictions on unseen data. 계산 생물학에서의 많은 지도 학습 문제의 경우, 입력 데이터는 예측을 행하는 데 잠재적으로 유용한 수치 또는 카테고리 데이터를 각각 포함하는 다수의 열 또는 특징을 갖는 표로서 표현될 수 있다. 일부 입력 데이터는 (온도 또는 시간과 같이) 표 내의 특징으로서 자연적으로 표현되는 반면, 다른 입력 데이터는 (k-량체 카운트로의 데옥시리보핵산(DNA) 시퀀스와 같이) 표로 나타낸 표현에 피팅하기 위해 특징 추출로 불리는 프로세스를 사용하여 먼저 변환될 필요가 있다. 인트론 스플라이싱 예측 문제의 경우, 표준 스플라이스 부위 시퀀스의 존재 여부, 스플라이싱 분기점의 위치 및 인트론 길이는 표로 나타낸 포맷으로 수집된 전처리된 특징일 수 있다. 표로 나타낸 데이터는, 로지스틱 회귀와 같은 단순한 선형 모델로부터 신경망과 같은 더 유연한 비선형 모델에 이르는 광범위한 지도 기계 학습 모델 및 많은 다른 것에 대한 표준이다.For many supervised learning problems in computational biology, the input data may be represented as a table with multiple columns or features, each containing numerical or categorical data that is potentially useful for making predictions. While some input data are naturally represented as features in a table (such as temperature or time), other input data (such as deoxyribonucleic acid (DNA) sequences in k-mer counts) need to first be transformed using a process called feature extraction to fit a tabular representation. For the intron splicing prediction problem, the presence of canonical splice site sequences, the locations of splicing branch points, and the intron lengths can be preprocessed features collected in a tabular format. Tabular data is the standard for a wide range of supervised machine learning models, from simple linear models like logistic regression to more flexible nonlinear models like neural networks, and many others. 로지스틱 회귀는 이진 분류자, 즉 이진 타깃 변수를 예측하는 지도 학습 모델이다. 구체적으로, 로지스틱 회귀는 한 유형의 활성화 함수인 시그모이드 함수를 사용하여 [0, 1] 구간에 매핑된 입력 특징의 가중 합을 계산함으로써 포지티브 클래스의 확률을 예측한다. 로지스틱 회귀의 파라미터 또는 상이한 활성화 함수를 사용하는 다른 선형 분류자는 가중 합에서의 가중치이다. 선형 분류자는 클래스, 예를 들어, 스플라이스-아웃되거나 스플라이스-아웃되지 않은 인트론의 클래스가 입력 특징의 가중 합으로 잘 구별될 수 없을 때 실패한다. 예측 성능을 개선하기 위해, 예를 들어, 거듭제곱 또는 쌍별 곱을 취함으로써 새로운 방식으로 기존의 특징을 변환 또는 조합함으로써 새로운 입력 특징이 수동으로 추가될 수 있다.Logistic regression is a binary classifier, i.e. a supervised learning model that predicts a binary target variable. Specifically, logistic regression predicts the probability of the positive class by computing a weighted sum of input features mapped to the interval [0, 1] using a type of activation function, the sigmoid function. The parameters of logistic regression or other linear classifiers using different activation functions are the weights in the weighted sum. A linear classifier fails when the classes, for example, the classes of spliced-out and non-spliced introns, cannot be well distinguished by a weighted sum of the input features. To improve prediction performance, new input features can be manually added by transforming or combining existing features in novel ways, for example, by taking exponentiation or pairwise products. 신경망은 은닉 계층을 사용하여 이러한 비선형 특징 변환을 자동으로 학습한다. 각각의 은닉 계층은 시그모이드 함수 또는 더 대중적인 정류 선형 유닛(ReLU)과 같은 비선형 활성화 함수에 의해 변환된 출력을 갖는 다수의 선형 모델로서 생각될 수 있다. 이러한 계층은 함께 입력 특징을 관련된 복잡한 패턴으로 구성하며, 이는 2개의 클래스를 구별하는 작업을 용이하게 한다.Neural networks automatically learn these nonlinear feature transformations using hidden layers. Each hidden layer can be thought of as a number of linear models with their outputs transformed by a nonlinear activation function, such as the sigmoid function or the more popular rectified linear unit (ReLU). These layers work together to organize input features into complex, related patterns, which facilitates the task of distinguishing between two classes. 심층 신경망은 많은 은닉 계층을 사용하고, 계층은 각각의 뉴런이 선행 계층의 모든 뉴런으로부터 입력을 수신할 때 완전 연결된 것으로 간주된다. 신경망은 일반적으로, 매우 큰 데이터 세트에 대해 모델을 훈련시키는 데 적합한 알고리즘인 확률론적 경사 하강법을 사용하여 훈련된다. 최신 심층 학습 프레임워크를 사용한 신경망의 구현은 상이한 아키텍처 및 데이터 세트로 신속한 프로토타이핑을 가능하게 한다. 완전 접속 신경망은 다수의 유전체학 응용예에 사용될 수 있으며, 이는 시퀀스 보존 또는 스플라이스 인자의 결합 모티프의 존재와 같은 시퀀스 특징으로부터의 주어진 시퀀스에 대해 스플라이스-인(splice in)된 엑손의 백분율을 예측하는 것; 잠재적인 질환 유발 유전자 변이체를 우선순위화하는 것; 및 염색질 마크, 유전자 발현 및 진화 보존과 같은 특징을 사용하여 주어진 게놈 영역 내의 시스(cis)-조절 요소를 예측하는 것을 포함한다.Deep neural networks use many hidden layers, and a layer is considered fully connected when each neuron receives input from all neurons in the preceding layer. Neural networks are typically trained using stochastic gradient descent, an algorithm that is suitable for training models on very large data sets. Implementation of neural networks using modern deep learning frameworks enables rapid prototyping with different architectures and data sets. Fully-connected neural networks can be used in a number of genomics applications, including predicting the percentage of exons spliced in for a given sequence from sequence features such as sequence conservation or the presence of splice factor binding motifs; prioritizing potential disease-causing genetic variants; and predicting cis-regulatory elements within a given genomic region using features such as chromatin marks, gene expression, and evolutionary conservation. 효과적인 예측을 위해 공간적 및 종방향 데이터에서의 로컬 종속성이 고려되어야 한다. 예를 들어, DNA 시퀀스 또는 이미지의 픽셀을 섞으면 정보성 패턴이 심하게 손상된다. 이러한 로컬 종속성은 표로 나타낸 데이터와는 분리된 공간적 또는 종방향 데이터를 설정하며, 이를 위한 특징의 순서화는 임의적이다. 특정 전사 인자에 의해 게놈 영역을 결합 대 비결합으로 분류하는 문제를 고려하는데, 이때 결합 영역은 시퀀스 분석(ChIP-seq) 데이터가 뒤에 오는 염색질 면역 침전에서의 고신뢰 결합 이벤트로서 정의된다. 시퀀스 모티프를 인식함으로써 전사 인자가 DNA에 결합된다. 시퀀스에서의 k-량체 인스턴스의 수 또는 위치 가중치 행렬(PWM) 매칭과 같은 시퀀스 파생 특징에 기초한 완전 연결 계층이 이러한 작업에 사용될 수 있다. 따라서, k-량체 또는 PWM 인스턴스 빈도는 시퀀스 내의 모티프 이동에 강하기 때문에, 이러한 모델은 상이한 위치에 위치한 동일한 모티프를 갖는 시퀀스에 대해 양호하게 일반화될 수 있다. 그러나, 이는 전사 인자 결합이 잘 정의된 간격을 갖는 다수의 모티프의 조합에 의존하는 패턴을 인식하지 못할 것이다. 또한, 가능한 k-량체의 수는 k-량체 길이에 따라 기하급수적으로 증가하며, 이는 저장 및 오버피팅 문제를 모두 제기한다.For effective prediction, local dependencies in spatial and longitudinal data should be considered. For example, mixing up DNA sequences or pixels in an image severely damages the informative patterns. These local dependencies establish spatial or longitudinal data that is separate from the tabular data, and the ordering of features for this is arbitrary. We consider the problem of classifying genomic regions as bound versus unbound by a specific transcription factor, where bound regions are defined as high-confidence binding events in chromatin immunoprecipitation followed by sequence analysis (ChIP-seq) data. Transcription factors bind to DNA by recognizing sequence motifs. Fully-connected layers based on sequence-derived features, such as the number of k-mer instances in a sequence or positional weight matrix (PWM) matching, can be used for these tasks. Therefore, since k-mer or PWM instance frequencies are robust to motif movement within a sequence, these models can generalize well to sequences with the same motif located at different positions. However, this would not recognize the pattern in which transcription factor binding depends on combinations of multiple motifs with well-defined spacing. Additionally, the number of possible k-mers grows exponentially with the k-mer length, which poses both storage and overfitting problems. 컨볼루션 계층은 완전 연결 계층의 특수 형태이며, 동일한 완전 연결 계층은, 예를 들어, 6 bp 윈도우에서, 모든 시퀀스 위치에 국부적으로 적용된다. 이러한 접근법은 또한, 예를 들어, 전사 인자 GATA1 및 TAL1에 대해, 다수의 PWM을 사용하여 시퀀스를 스캐닝하는 것으로 보일 수 있다. 모든 위치에서 동일한 모델 파라미터를 사용함으로써, 파라미터의 총 수는 급격히 감소되고, 네트워크는 훈련 동안 보이지 않는 위치에서 모티프를 검출할 수 있다. 각각의 컨볼루션 계층은 모든 위치에서 스칼라 값을 생성함으로써 여러 필터로 시퀀스를 스캐닝하며, 이는 필터와 시퀀스 사이의 매칭을 정량화한다. 완전 연결 신경망에서와 같이, 비선형 활성화 함수(일반적으로, ReLU)가 각각의 계층에 적용된다. 다음으로, 풀링 연산이 적용되며, 이는 위치 축에 걸친 인접 빈에서의 활성화를 집약하여, 일반적으로, 각각의 채널에 대해 최대 또는 평균 활성화를 취한다. 풀링은 유효 시퀀스 길이를 감소시키고, 신호를 거칠게 만든다. 후속 컨볼루션 계층은 이전 계층의 출력을 조정하며, GATA1 모티프 및 TAL1 모티프가 일부 거리 범위에 존재하였는지의 여부를 검출할 수 있다. 마지막으로, 컨볼루션 계층의 출력은 최종 예측 작업을 수행하기 위해 완전 접속 신경망에 대한 입력으로서 사용될 수 있다. 따라서, 상이한 유형의 신경망 계층(예: 완전 연결 계층 및 컨볼루션 계층)이 단일 신경망 내에서 조합될 수 있다.A convolutional layer is a special form of a fully-connected layer, where the same fully-connected layer is applied locally to all sequence positions, for example, in a 6 bp window. This approach can also be seen by scanning sequences using multiple PWMs, for example for the transcription factors GATA1 and TAL1. By using the same model parameters at all locations, the total number of parameters is drastically reduced, and the network can detect motifs at locations unseen during training. Each convolutional layer scans the sequence with multiple filters, producing a scalar value at every location, which quantifies the match between the filter and the sequence. As in a fully-connected neural network, a nonlinear activation function (typically ReLU) is applied to each layer. Next, a pooling operation is applied, which aggregates the activations in adjacent bins across the position axis, typically taking the maximum or average activation for each channel. Pooling reduces the effective sequence length and roughens the signal . The subsequent convolutional layers adjust the output of the previous layer and can detect whether GATA1 motifs and TAL1 motifs exist within a certain distance range. Finally, the output of the convolutional layer can be used as input to a fully-connected neural network to perform the final prediction task. Thus, different types of neural network layers (e.g., fully-connected layers and convolutional layers) can be combined within a single neural network. 컨볼루션 신경망(CNN)은 DNA 시퀀스 하나에만 기초하여 다양한 분자 표현형을 예측할 수 있다. 응용예는 전사 인자 결합 부위를 분류하는 것과, 염색질 특징, DNA 접촉 맵, DNA 메틸화, 유전자 발현, 변환 효율, RBP 결합 및 마이크로RNA(miRNA) 타깃과 같은 분자 표현형을 예측하는 것을 포함한다. 시퀀스로부터 분자 표현형을 예측하는 것에 더하여, 컨볼루션 신경망은 수작업으로 작성된 생물정보학 파이프라인에 의해 전통적으로 다루어진 더 많은 기술적 작업에 적용될 수 있다. 예를 들어, 컨볼루션 신경망은 가이드 RNA의 특이성을 예측할 수 있고, ChIP-seq를 노이즈 제거할 수 있고, Hi-C 데이터 해상도를 향상시킬 수 있고, DNA 시퀀스로부터인 근원의 실험을 예측할 수 있고, 유전자 변이체를 호출할 수 있다. 컨볼루션 신경망은 또한 게놈에서 장거리 종속성을 모델링하기 위해 채용되었다. 상호 작용하는 조절 요소가 전개된 선형 DNA 시퀀스 상에서 원거리에 위치될 수 있지만, 이러한 요소는 종종 실제 3D 염색질 형태에서 근위에 있다. 따라서, 선형 DNA 시퀀스로부터 분자 표현형을 모델링하는 것은, 염색질의 대강의 근사화에도 불구하고, 장거리 종속성을 허용하고 모델이 프로모터-인핸서 루핑과 같은 3D 구성의 양태를 암시적으로 학습할 수 있게 함으로써 개선될 수 있다. 이것은 최대 32 kb의 수용 필드를 갖는 확장된 컨볼루션을 사용하여 달성된다. 확장된 컨볼루션은 또한 스플라이스 부위가 10 kb의 수용 필드를 사용하여 시퀀스로부터 예측될 수 있도록 하며, 이로 인해, 전형적인 인간 인트론만큼 긴 거리에 걸쳐 유전자 시퀀스의 통합이 가능하게 한다(문헌[Jaganathan, K. 등, Predicting splicing from primary sequence with deep learning. Cell 176, 535~548 (2019년)] 참조).Convolutional neural networks (CNNs) can predict a variety of molecular phenotypes based on just a single DNA sequence. Applications include classifying transcription factor binding sites and predicting molecular phenotypes such as chromatin features, DNA contact maps, DNA methylation, gene expression, translational efficiency, RBP binding, and microRNA (miRNA) targets. In addition to predicting molecular phenotypes from sequences, convolutional neural networks can be applied to many more technical tasks traditionally handled by hand-crafted bioinformatics pipelines. For example, convolutional neural networks can predict the specificity of guide RNAs, denoise ChIP-seq, improve the resolution of Hi-C data, predict the origin of an experiment from DNA sequences, and call genetic variants. Convolutional neural networks have also been employed to model long-range dependencies in the genome. Although interacting regulatory elements can be located distantly on the unfolded linear DNA sequence, these elements are often proximal in the actual 3D chromatin configuration. Thus, modeling molecular phenotypes from linear DNA sequences can be improved by allowing for long-range dependencies and allowing the model to implicitly learn aspects of 3D organization, such as promoter-enhancer looping, despite being a rough approximation of chromatin. This is achieved using dilated convolutions with a receptive field of up to 32 kb. Extended convolutions also allow splice sites to be predicted from sequences using a receptive field of 10 kb, enabling the integration of genetic sequences over distances as long as a typical human intron (see the literature [Jaganathan, K. et al., Predicting splicing from primary sequence with deep learning. Cell 176, 535–548 (2019)]). 상이한 유형의 신경망은 이들의 파라미터 공유 체계에 의해 특성화될 수 있다. 예를 들어, 완전 연결 계층은 파라미터 공유를 갖지 않는 반면, 컨볼루션 계층은 이들의 입력의 모든 위치에서 동일한 필터를 적용함으로써 변환 불변성을 부과한다. 순환 신경망(RNN)은 상이한 파라미터 공유 체계를 구현하는, DNA 시퀀스 또는 시계열과 같은 순차적 데이터를 처리하기 위한 컨볼루션 신경망에 대한 대안이다. 순환 신경망은 각각의 시퀀스 요소에 동일한 연산을 적용한다. 상기 연산은 이전 시퀀스 요소의 메모리 및 새로운 입력을 입력으로서 취한다. 이는 메모리를 업데이트하고, 선택적으로는 후속 계층으로 전달되거나 모델 예측으로서 직접 사용되는 출력을 방출한다. 각각의 시퀀스 요소에 동일한 모델을 적용함으로써, 순환 신경망은 처리된 시퀀스에서 위치 인덱스에 대해 불변이다. 예를 들어, 순환 신경망은 시퀀스에서의 위치에 관계없이 DNA 시퀀스에서 개방 판독 프레임을 검출할 수 있다. 이러한 작업은 프레임 내 정지 코돈이 이어지는 시작 코돈과 같은 특정 입력 시리즈의 인식을 요구한다.Different types of neural networks can be characterized by their parameter sharing schemes. For example, fully-connected layers have no parameter sharing, whereas convolutional layers impose translation invariance by applying the same filter at every location of their inputs. Recurrent neural networks (RNNs) are an alternative to convolutional neural networks for processing sequential data, such as DNA sequences or time series, that implement different parameter sharing schemes. Recurrent neural networks apply the same operation to each sequence element. The above operation takes as input the memory of the previous sequence element and the new input. It updates the memory and optionally emits output that is passed to subsequent layers or used directly as model predictions. By applying the same model to each sequence element, recurrent neural networks are invariant to position indices in the processed sequence. For example, recurrent neural networks can detect open reading frames in a DNA sequence regardless of their location in the sequence. These tasks require the recognition of specific input series, such as a start codon followed by an in-frame stop codon. 컨볼루션 신경망에 비해 순환 신경망의 주요 이점은, 이론적으로, 메모리를 통해 무한히 긴 시퀀스를 거쳐 정보를 전달할 수 있다는 것이다. 또한, 순환 신경망은 mRNA 시퀀스와 같은 광범위하게 변화하는 길이의 시퀀스를 자연적으로 처리할 수 있다. 그러나, 다양한 트릭(예: 확장된 컨볼루션)과 조합된 컨볼루션 신경망은 오디오 합성 및 기계 변환과 같은 시퀀스 모델링 작업에 대해 순환 신경망과 유사하거나 심지어 그보다 더 양호한 성능에 도달할 수 있다. 순환 신경망은 단일 셀 DNA 메틸화 상태, RBP 결합, 전사 인자 결합 및 DNA 접근성을 예측하기 위한 컨볼루션 신경망의 출력을 집계할 수 있다. 더욱이, 순환 신경망이 순차적인 연산을 적용하기 때문에, 이는 쉽게 병렬화될 수 없고, 따라서, 컨볼루션 신경망보다 계산하는 데 훨씬 더 느리다.The main advantage of recurrent neural networks over convolutional neural networks is that, theoretically, they can propagate information through infinitely long sequences through memory. Additionally, recurrent neural networks can naturally handle sequences of widely varying lengths, such as mRNA sequences. However, convolutional neural networks combined with various tricks (e.g. dilated convolutions) can achieve performance similar to or even better than recurrent neural networks for sequence modeling tasks such as audio synthesis and machine translation. Recurrent neural networks can aggregate the outputs of convolutional neural networks to predict single-cell DNA methylation status, RBP binding, transcription factor binding, and DNA accessibility. Moreover, because recurrent neural networks apply sequential operations, they cannot be easily parallelized and are therefore much slower to compute than convolutional neural networks. 각각의 인간은 고유한 유전자 코드를 갖지만, 인간 유전자 코드의 대부분은 모든 인간에 대해 공통적이다. 일부 경우에 있어서, 인간 유전자 코드는 비교적 작은 그룹의 인간 모집단의 개체 사이에서 공통적일 수 있는 유전자 변이체로 불리는 이상치를 포함할 수 있다. 예를 들어, 특정 인간 단백질은 특정 시퀀스의 아미노산을 포함할 수 있는 반면, 그 단백질의 변이체는 그 외의 동일한 특정 시퀀스에서의 하나의 아미노산만큼 상이할 수 있다.Although each human being has a unique genetic code, most of the human genetic code is common to all humans. In some cases, the human genetic code may contain outliers, called genetic variants, that may be common among individuals in a relatively small group of the human population. For example, a particular human protein may contain a particular sequence of amino acids, while variants of that protein may differ by as little as one amino acid from an otherwise identical particular sequence. 유전자 변이체는 병원성이어서, 질환으로 이어질 수 있다. 이러한 유전자 변이체의 대부분이 자연적인 선택에 의해 게놈으로부터 대폭 감소되었지만, 어느 유전자 변이체가 병원성일 가능성이 있는지를 식별하는 능력은 연구자가 이러한 유전자 변이체에 초점을 맞추어 해당하는 질환 및 이의 진단, 처치 또는 치유에 대한 이해를 얻는 데 도움이 될 수 있다. 수백만 개의 인간 유전자 변이체의 임상적 해석은 여전히 불명확하다. 가장 빈번한 병원성 변이체 중 일부는 단백질의 아미노산을 변화시키는 단일 뉴클레오티드 미스센스 돌연변이이다. 그러나, 모든 미스센스 돌연변이가 병원성인 것은 아니다.Genetic variants are pathogenic and can lead to disease. Although most of these genetic variants have been greatly reduced from the genome by natural selection, the ability to identify which genetic variants are likely to be pathogenic could help researchers focus on these genetic variants to gain insight into the underlying disease and its diagnosis, treatment, or cure. The clinical implications of millions of human genetic variants remain unclear. Some of the most frequent pathogenic variants are single nucleotide missense mutations that change the amino acid of a protein. However, not all missense mutations are pathogenic. 생물학적 시퀀스로부터 직접적으로 분자 표현형을 예측할 수 있는 모델은 유전자 변이체와 표현형 변이체 사이의 연관성을 프로브하기 위해 컴퓨터 내(in silico) 섭동 툴로서 사용될 수 있고, 정량적 형질 유전자좌 식별 및 변이체 우선 순위화를 위한 새로운 방법으로서 부상하였다. 복잡한 표현형의 전장 유전체 연관성 분석에 의해 식별된 변이체의 대부분이 비-코딩이라면, 이러한 접근법은 매우 중요하며, 이는 표현형에 대한 이의 효과 및 기여도를 추정하는 것을 어렵게 만든다. 더욱이, 연관 불균형은 변이체의 블록이 동시-유전되는 결과를 초래하며, 이는 개별 인과 변이체를 정확하게 찾아내는 것에 어려움을 야기한다. 따라서, 이러한 변이체의 영향을 평가하기 위한 심문 툴로서 사용될 수 있는 시퀀스 기반 심층 학습 모델은 복잡한 표현형의 잠재적인 드라이버를 찾는 것에 대한 유망한 접근법을 제공한다. 하나의 예는 전사 인자 결합, 염색질 접근성 또는 유전자 발현 예측의 측면에서 2개의 변이체 사이의 차이로부터 간접적으로 짧은 삽입 또는 결실(인델) 및 비-코딩 단일 뉴클레오티드 변이체의 효과를 예측하는 것을 포함한다. 다른 예는 스플라이싱에 대한 유전자 변이체의 시퀀스 또는 정량적 효과로부터 신규한 스플라이스 부위 생성을 예측하는 것을 포함한다.Models that can directly predict molecular phenotypes from biological sequences can be used as in silico perturbation tools to probe associations between genetic variants and phenotypic variants, and have emerged as a novel method for quantitative trait locus identification and variant prioritization. This approach is crucial given that most of the variants identified by genome-wide association analyses of complex phenotypes are non-coding, making it difficult to estimate their effect and contribution to the phenotype. Moreover, linkage disequilibrium results in blocks of variants being co-inherited, which makes it difficult to pinpoint individual causal variants. Therefore, sequence-based deep learning models that can be used as interrogation tools to assess the impact of these variants provide a promising approach to discovering potential drivers of complex phenotypes. An example includes predicting the effects of short insertions or deletions (indels) and non-coding single nucleotide variants indirectly from differences between two variants in terms of transcription factor binding, chromatin accessibility, or gene expression prediction. Other examples include predicting the creation of novel splice sites from the sequence or quantitative effect of genetic variants on splicing. 변이체 효과 예측을 위한 종단 간 심층 학습 접근법은 시퀀스 보존 데이터 및 단백질 시퀀스로부터의 미스센스 변이체의 병원성을 예측하기 위해 적용된다(본원에서 'PrimateAI'로 지칭되는, 문헌[Sundaram, L. 등, Predicting the clinical impact of human mutation with deep neural networks. Nat.Genet.50, 1161~1170 (2018년)] 참조). PrimateAI는 종간(cross-species) 정보를 사용한 데이터 증강에 의한 기지의 병원성의 변이체에 대해 훈련된 심층 신경망을 사용한다. 특히, PrimateAI는 차이를 비교하고 훈련된 심층 신경망을 사용하여 돌연변이의 병원성을 결정하기 위해 야생형 및 변종 단백질의 시퀀스를 사용한다. 병원성 예측을 위한 단백질 시퀀스를 활용하는 그러한 접근법은, 순환성 문제 및 이전 지식에 대한 과적합화를 회피할 수 있기 때문에 유망하다. 그러나, 심층 신경망을 효과적으로 훈련시키기 위한 적절한 수의 데이터와 비교하면, ClinVar에서 이용 가능한 임상 데이터의 수는 비교적 작다. 이러한 데이터 부족을 극복하기 위해, PrimateAI는 공통적인 인간 변이체 및 영장류로부터의 변이체를 양성 데이터로서 사용하지만, 트리뉴클레오티드 콘텍스트에 기초한 시뮬레이션된 변이체가 라벨링되지 않은 데이터로서 사용되었다.An end-to-end deep learning approach for variant effect prediction is applied to predict the pathogenicity of missense variants from sequence conservation data and protein sequences (see Sundaram, L. et al., Predicting the clinical impact of human mutation with deep neural networks. Nat. Genet. 50, 1161–1170 (2018), referred to herein as “PrimateAI”). PrimateAI uses deep neural networks trained on known pathogenic variants by data augmentation using cross-species information. In particular, PrimateAI uses the sequences of wild-type and mutant proteins to compare differences and determine the pathogenicity of mutations using trained deep neural networks. Such approaches that utilize protein sequences for pathogenicity prediction are promising because they can avoid circularity problems and overfitting to prior knowledge. However, the number of clinical data available in ClinVar is relatively small compared to the adequate number of data required to effectively train a deep neural network. To overcome this data deficiency, PrimateAI uses common human variants and variants from primates as positive data, while simulated variants based on trinucleotide context are used as unlabeled data. PrimateAI는 시퀀스 정렬에 대해 직접적으로 훈련될 때 이전 방법을 능가한다. PrimateAI는 약 120,000개의 인간 샘플로 이루어진 훈련 데이터로부터 직접적으로 중요한 단백질 도메인, 보존 아미노산 위치 및 시퀀스 종속성을 학습한다. PrimateAI는 후보 발달 장애 유전자에서 양성 및 병원성 신생 돌연변이를 구별하고 ClinVar에서 이전 지식을 재생하는 데 있어서 다른 변이체 병원성 예측 툴의 성능을 실질적으로 초과한다. 이러한 결과는 PrimateAI가 이전 지식에 대한 임상 보고의 의존을 줄일 수 있는 변이체 분류 툴을 위해 중요한 진전임을 시사한다.PrimateAI outperforms previous methods when trained directly on sequence alignment. PrimateAI learns important protein domains, conserved amino acid positions, and sequence dependencies directly from training data of approximately 120,000 human samples. PrimateAI substantially outperforms other variant pathogenicity prediction tools in distinguishing benign and pathogenic de novo mutations in candidate developmental disorder genes and in reproducing prior knowledge from ClinVar. These results suggest that PrimateAI is an important advance for variant classification tools that can reduce clinical reporting reliance on prior knowledge. 구조 요소가 관찰된 기능을 발생시키는 방법에 대한 이해는 단백질 생물학에 대해 중심이 된다. 방대한 단백질 구조 데이터는 구조적-기능적 관계를 지배하는 규칙을 체계적으로 도출하기 위한 계산 방법의 개발을 가능하게 한다. 그러나, 이러한 방법의 성능은 단백질 구조 표현의 선택에 중대하게 의존한다.Understanding how structural elements give rise to observed functions is central to protein biology. The vast array of protein structural data enables the development of computational methods to systematically derive rules governing structural-functional relationships. However, the performance of these methods critically depends on the choice of protein structure representation. 단백질 부위는 이의 구조적 또는 기능적 역할에 의해 구별되는 단백질 구조 내의 미세 환경이다. 부위는 3차원(3D) 위치 및 구조 또는 기능이 존재하는 이러한 위치 주위의 국부적 이웃에 의해 정의될 수 있다. 아미노산의 구조적 배열이 단백질 부위 내에서 기능적 특성을 생성하는 방법에 대한 이해는 합리적인 단백질 공학에 대해 중심이 된다. 단백질 내의 개별 아미노산의 구조적 및 기능적 역할의 결정은 공학자를 돕고 단백질 기능을 변경하기 위한 정보를 제공한다. 기능적으로 또는 구조적으로 중요한 아미노산을 식별하는 것은 타깃화된 단백질 기능적 속성을 변경하기 위한 부위 유도 돌연변이 유발과 같은 집중된 공학 노고를 허용한다. 대안적으로, 이러한 지식은 원하는 기능을 무효화할 공학 설계를 회피하는 데 도움이 될 수 있다.Protein sites are microenvironments within a protein structure that are distinguished by their structural or functional roles. A region can be defined by its three-dimensional (3D) location and the local neighborhood around that location where the structure or function exists. Understanding how the structural arrangement of amino acids within a protein molecule generates functional properties is central to rational protein engineering. Determination of the structural and functional roles of individual amino acids within a protein aids engineers and provides information for altering protein function. Identification of functionally or structurally important amino acids allows for focused engineering efforts, such as site-directed mutagenesis, to alter targeted protein functional properties. Alternatively, this knowledge can help avoid engineering designs that would defeat the desired functionality. 구조가 시퀀스보다 훨씬 더 많이 보존된다는 것이 확립되었기 때문에, 단백질 구조 데이터의 증가는 데이터 중심 접근법을 사용하여 구조적-기능적 관계를 지배하는 기본 패턴을 체계적으로 연구할 기회를 제공한다. 임의의 계산 단백질 분석의 기본 양태는 단백질 구조 정보가 표현되는 방법이다. 기계 학습 방법의 성능은 종종 채용된 기계 학습 알고리즘보다 데이터 표현의 선택에 더 많이 의존한다. 양호한 표현은 가장 중대한 정보를 효율적으로 캡처하는 반면, 불량한 표현은 기본 패턴이 없는 노이즈 분포를 생성한다.Because it is established that structure is much more conserved than sequence, the growing number of protein structural data provides opportunities to systematically study the underlying patterns governing structure-function relationships using data-driven approaches. A fundamental aspect of any computational protein analysis is how protein structural information is represented. The performance of machine learning methods often depends more on the choice of data representation than on the machine learning algorithm employed. A good representation efficiently captures the most important information, whereas a bad representation produces a noisy distribution devoid of underlying patterns. 방대한 단백질 구조 및 심층 학습 알고리즘의 최근의 성공은 단백질 구조의 작업 특정적 표현을 자동으로 추출하기 위한 툴을 개발할 기회를 제공한다.Recent successes in massive protein structures and deep learning algorithms provide opportunities to develop tools to automatically extract task-specific representations of protein structures. 유전체학 연구의 계산 분석은 관심 있는 유전적 인자와 무관한 교란 변이로 인해 어려움을 겪는다. 유전자 발현의 높은 또는 낮은 수준을 유발하는 변이체의 식별은 유전 질환의 병원성을 진단하는 데 가장 중요하다. 그러나, 병원성 변이체의 식별을 방해할 수 있는 수많은 교란 인자가 있다. 특정 병리학과 관련될 수 있는 희귀 변이체를 검사하여 변이체를 분리하면 문제를 단순화할 수 있다. 또한, 교란자에 의해 유입되는 노이즈를 제거하면 신호 대 노이즈 비를 증가시킬 수 있다.Computational analysis of genomics studies is hampered by confounding mutations that are unrelated to the genetic factors of interest. Identification of variants that cause high or low levels of gene expression is of utmost importance in diagnosing the pathogenicity of genetic diseases. However, there are numerous confounding factors that can hinder the identification of pathogenic variants. Testing for rare variants that may be associated with a specific pathology and isolating the variants can simplify the problem. Additionally, removing noise introduced by the disruptor can increase the signal-to-noise ratio. 이에 후생유전학에 인공 지능을 적용하여 변이가 있는 유전자좌 및 개별 유전자의 발현 수준 간의 유전적 연관성 회복에 대한 민감도를 크게 높일 수 있는 기회가 발생한다.This presents an opportunity to apply artificial intelligence to epigenetics to greatly increase the sensitivity of recovering genetic associations between mutated loci and expression levels of individual genes. 특허 또는 출원 파일은 컬러로 작성된 적어도 하나의 도면을 포함한다. 컬러 도면(들)을 갖는 이러한 특허 또는 특허 출원 공보의 사본은 요청 및 필요한 수수료의 지불에 따라 특허청에 의해 제공될 것이다. 컬러 도면은, 또한, Supplemental Content 탭을 통해 PAIR에서 입수 가능할 수 있다. 도면에서, 유사한 도면 부호는 일반적으로 상이한 도면 전체에 걸쳐서 유사한 부분을 지칭한다. 또한, 도면은 반드시 일정한 비율로 표시될 필요는 없으며, 대신 일반적으로 개시된 기술의 원리를 설명하는 데 중점을 두고 있다. 다음 설명에서, 개시된 기술의 다양한 구현이 다음 도면을 참조하여 기술된다. 도 1은 유전자 시퀀스의 진화적, 후생적 특성을 결정하기 위한 시스템의 프로세스를 설명하는 흐름도이다. 도 2는 시퀀스 데이터베이스로부터 추출된 뉴클레오티드 염기를 포함하는 예시적인 입력 염기 시퀀스를 개략적으로 도시하며, 여기서 타깃 염기 시퀀스는 상류 컨텍스트 염기를 함유하는 좌측 시퀀스 및 하류 컨텍스트 염기를 함유하는 우측 시퀀스에 의해 플랭킹된다. 도 3은 2개의 예시적인 대체 시퀀스를 갖는 예시적인 기준 유전자 시퀀스로부터의 대체 시퀀스의 일례를 도시하며, 여기서, 대체 시퀀스는 단일 염기 위치에서 각각의 단일 뉴클레오티드 변이체를 보유하지만, 그 외에는 기준 시퀀스와 동일한 조성을 보유한다. 도 4는 개시된 기술의 한 가지 구현을 위해 개발된 훈련 데이터세트에 속하는 시퀀스의 유전자 구성을 도시한다. 도 5는 도 4에 설명된 제1 훈련 세트로 훈련된 모델로 도 1의 시스템에 적용된 다음, 도 4에 설명된 제2 훈련 세트로 재훈련을 받고 있는 훈련 절차의 일 구현을 개략적으로 도시한다. 도 6은 도 4에 설명된 제1 훈련 세트를 사용하여 도 1의 시스템에 적용된 다음, 도 4에 설명된 제2 훈련 세트의 하위 세트와의 재훈련 진행하고, 제2 훈련 세트로부터의 샘플의 나머지 하위 세트와의 모델 검증이 뒤따르는 훈련 절차의 다른 구현을 개략적으로 도시한다. 도 7은 변이체 분류를 위한 도 1의 시스템 구현의 개략도이며, 여기서 상기 시스템을 사용하여 각 시퀀스에 대한 각각의 모델 출력을 비교함으로써 염기 해상도에서 기준 시퀀스와 대체 시퀀스를 비교한다. 도 8은 기준 시퀀스와 대체 시퀀스의 비교가 평균 델타 값에 의해 정량화되는 개시된 기술의 일 구현의 흐름도이다. 도 9는 기준 시퀀스와 대체 시퀀스의 비교가 최종 합 델타 값에 의해 정량화되는 개시된 기술의 일 구현의 흐름도이다. 도 10은, 생물학적 양 모델이 종단 간으로 훈련되는 제1 가중치 세트 및 제2 가중치 세트를 통해 입력 염기 시퀀스로부터 2개의 생물학적 양 출력 시퀀스를 생성하는, 개시된 기술의 일 구현의 흐름도이다. 도 11은, 생물학적 양 모델이 종단 간으로 훈련되는 제1 가중치 세트 및 제2 가중치 세트를 통해 입력 염기 시퀀스로부터 3개의 생물학적 양 출력 시퀀스를 생성하는, 개시된 기술의 일 구현의 흐름도이다. 도 12는, 생물학적 양 모델이 입력 염기 시퀀스의 대체 표현을 생성하는 제1 가중치 세트 및 처음부터 훈련되어 입력 염기 시퀀스의 대체 표현으로부터 복수의 생물학적 양 출력 시퀀스를 생성하는 제2 가중치 세트를 포함하는, 개시된 기술의 일 구현의 흐름도이다. 도 13은, 생물학적 양 모델이 종단 간으로 훈련되고 재훈련되어 단일 기반으로 후속 생물학적 양 출력 시퀀스를 생성하는 제1 가중치 세트 및 제2 가중치 세트를 통해 입력 염기 시퀀스로부터 하나의 생물학적 양 출력 시퀀스를 생성하는, 개시된 기술의 일 구현의 흐름도이다. 도 14는, 생물학적 양 모델이 종단 간으로 훈련되어 입력 염기 시퀀스로부터 복수의 생물학적 양 출력 시퀀스를 생성하는 제1 가중치 세트 및 제2 가중치 세트, 및 종단 간으로 훈련되어 입력 복수의 생물학적 양 출력 시퀀스로부터 유전자 발현 출력 시퀀스를 생성하는 제3 및 제4 가중치 세트를 포함하는, 개시된 기술의 일 구현의 흐름도이다. 도 15는, 생물학적 양 모델이 입력 염기 시퀀스의 대체 표현을 생성하는 제1 가중치 세트, 처음부터 훈련되어 입력 염기 시퀀스의 대체 표현으로부터 복수의 생물학적 양 출력 시퀀스를 생성하는 제2 가중치 세트, 및 종단 간으로 훈련되어 입력 복수의 생물학적 양 출력 시퀀스로부터 유전자 발현 출력 시퀀스를 생성하는 제3 및 제4 가중치 세트를 포함하는, 개시된 기술의 일 구현의 흐름도이다. 도 16은, 생물학적 양 모델이 입력 염기 시퀀스의 대체 표현을 생성하는 제1 가중치 세트, 처음부터 훈련되어 복수의 생물학적 양 출력 시퀀스를 생성하는 제2 가중치 세트, 처음부터 훈련되어 복수의 생물학적 양 출력 시퀀스로부터 대체 생물학적 양 표현을 생성하는 제3 가중치 세트, 및 처음부터 훈련되어 대체 생물학적 양 표현으로부터 유전자 발현 출력 시퀀스를 생성하는 제4 가중치 세트를 포함하는, 개시된 기술의 일 구현의 흐름도이다. 도 17은, 생물학적 양 모델이 종단 간으로 훈련되어 입력 염기 시퀀스로부터 복수의 생물학적 양 출력 시퀀스를 생성하는 제1 가중치 세트 및 제2 가중치 세트, 처음부터 훈련되어 복수의 생물학적 양 출력 시퀀스로부터 대체 생물학적 양 표현을 생성하는 제3 가중치 세트, 및 처음부터 훈련되어 대체 생물학적 양 표현으로부터 유전자 발현 출력 시퀀스를 생성하는 제4 가중치 세트를 포함하는, 개시된 기술의 일 구현의 흐름도이다. 도 18은, 생물학적 양 모델이 입력 염기 시퀀스로부터 복수의 생물학적 양 출력 시퀀스를 생성하도록 훈련된 다음, 제3 가중치 세트의 치환으로 재훈련되어 복수의 생물학적 양 출력 시퀀스로부터 대체 생물학적 양 표현을 생성하는 제1 가중치 세트, 및 치환된 제1 가중치 세트로 종단 간으로 훈련되어 대체 생물학적 양 표현으로부터 유전자 발현 출력 시퀀스를 생성하는 제4 가중치 세트를 포함하는, 개시된 기술의 일 구현의 흐름도이다. 도 19는, 생물학적 양 모델이 입력 염기 시퀀스로부터 대체 시퀀스 표현을 생성하도록 훈련된 다음, 제3 가중치 세트의 치환으로서 재훈련되어 복수의 생물학적 양 출력 시퀀스로부터 대체 생물학적 양 표현을 생성하는 제1 가중치 세트, 및 처음부터 훈련되어 대체 생물학적 양 표현으로부터 유전자 발현 출력 시퀀스를 생성하는 제4 가중치 세트를 포함하는, 개시된 기술의 일 구현의 흐름도이다. 도 20은, 생물학적 양 모델이 입력 염기 시퀀스로부터 대체 시퀀스 표현을 생성하도록 훈련되는 제1 가중치 세트, 입력 염기의 대체 표현으로부터 복수의 생물학적 양 출력 시퀀스를 생성하도록 훈련되는 제2 가중치 세트, 복수의 생물학적 양 출력 시퀀스로부터 대체 생물학적 양 표현을 생성하기 위해 제3 가중치 세트의 치환으로서 사용되는 재훈련된 제1 가중치 세트, 및 처음부터 훈련되어 대체 생물학적 양 표현으로부터 유전자 발현 출력 시퀀스를 생성하는 제4 가중치 세트를 포함하는, 개시된 기술의 일 구현의 흐름도이다. 도 21은, 생물학적 양 모델이 입력 염기 시퀀스로부터 대체 시퀀스 표현을 생성하도록 훈련되는 제1 가중치 세트, 입력 염기의 대체 표현으로부터 복수의 생물학적 양 출력 시퀀스를 생성하도록 훈련되는 제2 가중치 세트, 복수의 생물학적 양 출력 시퀀스로부터 대체 생물학적 양 표현을 생성하기 위해 제3 가중치 세트의 치환으로서 사용되는 재훈련된 제1 가중치 세트, 및 치환된 제1 가중치 세트로 종단 간으로 훈련되어 대체 생물학적 양 표현으로부터 유전자 발현 출력 시퀀스를 생성하는 제4 가중치 세트를 포함하는, 개시된 기술의 일 구현의 흐름도이다. 도 22는, 모델이 염기 해상도에서 기준 생물학적 양 출력 시퀀스와 대체 생물학적 양 출력 시퀀스를 비교하는 병원성 예측 로직을 추가로 포함하도록 구성되는 개시된 기술의 일 구현의 흐름도이며, 여기서 제5 가중치 세트는 복수의 생물학적 양 출력 시퀀스로부터 대체 시퀀스 병원성 예측을 생성하고, 제1 및 제2 가중치 세트는 각각 처음부터 훈련된다. 도 23은, 모델이 염기 해상도에서 기준 생물학적 양 출력 시퀀스와 대체 생물학적 양 출력 시퀀스를 비교하는 병원성 예측 로직을 추가로 포함하도록 구성되는 개시된 기술의 일 구현의 흐름도이며, 여기서 제5 가중치 세트는 복수의 생물학적 양 출력 시퀀스로부터 대체 시퀀스 병원성 예측을 생성하고, 제1 및 제2 가중치 세트는 종단 간으로 훈련된다. 도 24는 제1 생물학적 양 출력 시퀀스에 대한 값으로 입력 염기 시퀀스로부터의 생물학적 양 모델로부터 생성될 수 있는 진화 보존 측정의 개략도이다. 도 25는 제2 생물학적 양 출력 시퀀스에 대한 값으로 입력 염기 시퀀스로부터의 생물학적 양 모델로부터 생성될 수 있는 전사 개시 측정의 개략도이다. 도 26은 제3 생물학적 양 출력 시퀀스에 대한 값으로 입력 염기 시퀀스로부터의 생물학적 양 모델로부터 생성될 수 있는 후생적 신호의 개략도이다. 도 27은, 발현 변경 분류기가 유전자 발현에 대한 변이체의 효과를 예측하도록 구성되는, 개시된 기술의 일 구현의 흐름도이다. 도 28은, 변이체가 유전자 발현을 감소시키는지 또는 유전자 발현을 감소시키지 않는지를 예측하기 위해 발현 변경 분류기가 발현 감소 분류기로서 추가로 구성되는, 개시된 기술의 일 구현의 흐름도이다. 도 29는, 변이체가 유전자 발현을 증가시키는지 또는 유전자 발현을 증가시키지 않는지를 예측하기 위해 발현 변경 분류기가 발현 감소 분류기로서 추가로 구성되는, 개시된 기술의 일 구현의 흐름도이다. 도 30은, 발현 변경 분류기는 변이체가 유전자 발현을 보존하는지, 유전자 발현을 감소시키는지, 또는 유전자 발현을 증가시키는지를 예측하는 다중-클래스 발현 분류기로 추가로 구성되는, 개시된 기술의 일 구현의 흐름도이다. 도 31은, 추론된 인과성 점수에 대한 기준 진리 인과성 점수의 비교를 위해 유전자 발현 분류기 훈련이 사용되는, 개시된 기술의 일 구현의 흐름도이다. 도 32은 개시된 기술을 구현하는 데 사용될 수 있는 예시적인 컴퓨터 시스템을 도시한다.The patent or application file contains at least one drawing in color. Copies of this patent or patent application publication with color drawing(s) will be provided by the Office upon request and payment of the required fee. Color drawings are also available from PAIR via the Supplemental Content tab. In drawings, similar drawing symbols generally refer to similar parts throughout different drawings. Additionally, the drawings are not necessarily to scale, and instead generally focus on illustrating the principles of the disclosed technology. In the following description, various implementations of the disclosed technology are described with reference to the following drawings. Figure 1 is a flowchart illustrating the process of a system for determining evolutionary and epigenetic characteristics of a genetic sequence. Figure 2 schematically illustrates an exemplary input base sequence comprising nucleotide bases extracted from a sequence database, wherein the target base sequence is flanked by a left sequence containing upstream context bases and a right sequence containing downstream context bases. Figure 3 illustrates an example of a replacement sequence from an exemplary reference gene sequence having two exemplary replacement sequences, wherein the replacement sequences each have a single nucleotide variant at a single base position, but otherwise have the same composition as the reference sequence. Figure 4 illustrates the genetic composition of sequences belonging to a training dataset developed for one implementation of the disclosed technology. Figure 5 schematically illustrates one implementation of a training procedure in which a model trained with the first training set described in Figure 4 is applied to the system of Figure 1 and then retrained with the second training set described in Figure 4. Figure 6 schematically illustrates another implementation of the training procedure applied to the system of Figure 1 using the first training set described in Figure 4, followed by retraining with a subset of the second training set described in Figure 4, and model validation with the remaining subset of samples from the second training set. Figure 7 is a schematic diagram of an implementation of the system of Figure 1 for variant classification, wherein the system is used to compare a reference sequence and a substitute sequence at base resolution by comparing the respective model outputs for each sequence. FIG. 8 is a flow diagram of one implementation of the disclosed technique in which the comparison of a reference sequence and a substitute sequence is quantified by an average delta value. FIG. 9 is a flow diagram of one implementation of the disclosed technique in which the comparison of a reference sequence and a substitute sequence is quantified by a final sum delta value. FIG. 10 is a flow diagram of one implementation of the disclosed technique for generating two biological quantity output sequences from an input base sequence via a first set of weights and a second set of weights on which a biological quantity model is trained end-to-end. FIG. 11 is a flow diagram of one implementation of the disclosed technique for generating three biological quantity output sequences from an input base sequence via a first set of weights and a second set of weights on which a biological quantity model is trained end-to-end. FIG. 12 is a flow diagram of one implementation of the disclosed technique, wherein a biological quantity model comprises a first set of weights for generating alternative representations of an input base sequence and a second set of weights trained from scratch for generating a plurality of biological quantity output sequences from the alternative representations of the input base sequence. FIG. 13 is a flow diagram of one implementation of the disclosed technique, in which a biological quantity model is trained and retrained end-to-end to generate a single biological quantity output sequence from an input base sequence via a first set of weights and a second set of weights that generate subsequent biological quantity output sequences on a single basis. FIG. 14 is a flow diagram of one implementation of the disclosed technology, wherein the biological quantity model comprises a first set of weights and a second set of weights that are trained end-to-end to generate a plurality of biological quantity output sequences from an input base sequence, and a third and fourth set of weights that are trained end-to-end to generate gene expression output sequences from an input plurality of biological quantity output sequences. FIG. 15 is a flow diagram of one implementation of the disclosed technology, wherein the biological quantity model comprises a first set of weights that generate alternative representations of an input base sequence, a second set of weights that are trained from scratch to generate a plurality of biological quantity output sequences from the alternative representations of the input base sequence, and third and fourth sets of weights that are trained end-to-end to generate gene expression output sequences from the input plurality of biological quantity output sequences. FIG. 16 is a flow diagram of one implementation of the disclosed technology, wherein a biological quantity model comprises a first set of weights that generate alternative representations of input base sequences, a second set of weights that are trained from scratch to generate a plurality of biological quantity output sequences, a third set of weights that are trained from scratch to generate alternative biological quantity representations from the plurality of biological quantity output sequences, and a fourth set of weights that are trained from scratch to generate gene expression output sequences from the alternative biological quantity representations. FIG. 17 is a flow diagram of one implementation of the disclosed technology, wherein the biological quantity model is trained end-to-end to generate a plurality of biological quantity output sequences from input base sequences, a first set of weights and a second set of weights, a third set of weights trained from scratch to generate alternative biological quantity representations from the plurality of biological quantity output sequences, and a fourth set of weights trained from scratch to generate gene expression output sequences from the alternative biological quantity representations. FIG. 18 is a flow diagram of one implementation of the disclosed technology, wherein a biological quantity model is trained to generate a plurality of biological quantity output sequences from an input base sequence, and then retrained with a permutation of a third set of weights to generate alternative biological quantity representations from the plurality of biological quantity output sequences, and a fourth set of weights is trained end-to-end with the permuted first set of weights to generate gene expression output sequences from the alternative biological quantity representations. FIG. 19 is a flow diagram of one implementation of the disclosed technique, wherein a first set of weights is trained to generate alternative sequence representations from input base sequences and is then retrained as a permutation of a third set of weights to generate alternative biological quantity representations from multiple biological quantity output sequences, and a fourth set of weights is trained from scratch to generate gene expression output sequences from the alternative biological quantity representations. FIG. 20 is a flow diagram of one implementation of the disclosed technology, comprising a first set of weights trained to generate alternative sequence representations from input base sequences, a second set of weights trained to generate a plurality of biological quantity output sequences from the alternative representations of the input bases, a retrained first set of weights used as a replacement for the third set of weights to generate alternative biological quantity representations from the plurality of biological quantity output sequences, and a fourth set of weights trained from scratch to generate gene expression output sequences from the alternative biological quantity representations. FIG. 21 is a flow diagram of one implementation of the disclosed technology, comprising a first set of weights on which a biological quantity model is trained to generate alternative sequence representations from input base sequences, a second set of weights on which a biological quantity model is trained to generate a plurality of biological quantity output sequences from the alternative representations of the input bases, a retrained first set of weights used as a permutation of the third set of weights to generate alternative biological quantity representations from the plurality of biological quantity output sequences, and a fourth set of weights that is end-to-end trained with the permuted first set of weights to generate gene expression output sequences from the alternative biological quantity representations. FIG. 22 is a flow diagram of one implementation of the disclosed technology, wherein the model is configured to further include pathogenicity prediction logic for comparing a reference biological quantity output sequence to a surrogate biological quantity output sequence at base resolution, wherein the fifth set of weights generates surrogate sequence pathogenicity predictions from the plurality of biological quantity output sequences, and the first and second sets of weights are each trained from scratch. FIG. 23 is a flow diagram of one implementation of the disclosed technology, wherein the model is configured to further include pathogenicity prediction logic for comparing a reference biological quantity output sequence to a surrogate biological quantity output sequence at base resolution, wherein the fifth set of weights generates surrogate sequence pathogenicity predictions from the plurality of biological quantity output sequences, and the first and second sets of weights are trained end-to-end. Figure 24 is a schematic diagram of an evolutionary conservation measure that can be generated from a biological quantity model from an input base sequence as a value for a first biological quantity output sequence. Figure 25 is a schematic diagram of a transcription initiation measurement that can be generated from a biological quantity model from an input base sequence as a value for a second biological quantity output sequence. Figure 26 is a schematic diagram of an epigenetic signal that can be generated from a biological quantity model from an input base sequence as a value for a third biological quantity output sequence. FIG. 27 is a flow diagram of one implementation of the disclosed technology, wherein an expression change classifier is configured to predict the effect of a variant on gene expression. FIG. 28 is a flow diagram of one implementation of the disclosed technology, wherein the expression alteration classifier is further configured as a decreased expression classifier to predict whether the variant decreases gene expression or does not decrease gene expression. FIG. 29 is a flow diagram of one implementation of the disclosed technology, wherein the expression change classifier is further configured as a decreased expression classifier to predict whether the variant increases gene expression or does not increase gene expression. FIG. 30 is a flow diagram of one implementation of the disclosed technology, wherein the expression change classifier is further configured as a multi-class expression classifier that predicts whether the variant preserves gene expression, decreases gene expression, or increases gene expression. Figure 31 is a flow diagram of one implementation of the disclosed technique, where gene expression classifier training is used to compare ground truth causality scores to inferred causality scores. FIG. 32 illustrates an exemplary computer system that may be used to implement the disclosed technology. 아래의 논의는 어느 당업자라도 개시된 기술을 제조하고 사용할 수 있게 하도록 제시되며, 특정의 적용 및 그의 요건과 관련하여 제공된다. 개시된 구현에 대한 다양한 수정은 당업자에게 용이하게 명백할 것이며, 본원에 정의된 일반 원리는 개시된 기술의 사상 및 범위로부터 벗어나지 않고 다른 구현 및 응용에 적용될 수 있다. 따라서, 개시된 기술은 도시된 구현으로 제한되도록 의도된 것이 아니라, 본원에 개시된 원리 및 특징과 일치하는 가장 넓은 범주에 부합되어야 한다.The discussion below is presented to enable any person skilled in the art to make and use the disclosed technology, and is provided in the context of a particular application and its requirements. Various modifications to the disclosed implementations will be readily apparent to those skilled in the art, and the generic principles defined herein may be applied to other implementations and applications without departing from the spirit and scope of the disclosed technology. Accordingly, the disclosed technology is not intended to be limited to the implementations shown but should be accorded the widest scope consistent with the principles and features disclosed herein. 다양한 구현에 대한 상세한 설명은 첨부된 도면과 함께 읽을 때 더 잘 이해될 것이다. 도면이 다양한 구현의 기능 블록도를 도시하는 범위에서, 기능 블록은 반드시 하드웨어 회로 사이의 분할을 나타내는 것은 아니다. 따라서, 예를 들어, 하나 이상의 기능 블록(예: 모듈, 프로세서 또는 메모리)은 단일 하드웨어(예: 범용 신호 프로세서 또는 랜덤 액세스 메모리의 블록, 하드 디스크 등) 또는 다수의 하드웨어에서 구현될 수 있다. 유사하게, 프로그램은 독립형 프로그램일 수 있고, 운영 체제에 하위 루틴으로서 통합될 수 있고, 설치된 소프트웨어 패키지 내의 기능일 수 있고, 등등이다. 다양한 구현이 도면에 도시된 배열 및 수단으로 제한되지 않는다는 것이 이해될 것이다.A detailed description of the various implementations will be better understood when read in conjunction with the attached drawings. To the extent that the drawings depict functional block diagrams of various implementations, the functional blocks do not necessarily represent partitions between hardware circuits. Thus, for example, one or more functional blocks (e.g., a module, a processor, or a memory) may be implemented in a single piece of hardware (e.g., a general-purpose signal processor or a block of random access memory, a hard disk, etc.) or in multiple pieces of hardware. Similarly, a program may be a standalone program, may be integrated into an operating system as a subroutine, may be a function within an installed software package, etc. It will be understood that the various implementations are not limited to the arrangements and means depicted in the drawings. 모듈로 지정된, 도면의 처리 엔진 및 데이터베이스는 하드웨어 또는 소프트웨어로 구현될 수 있고, 도면에 도시된 바와 같이 정확하게 동일한 블록으로 분할될 필요가 없다. 모듈 중 일부는 또한, 상이한 프로세서, 컴퓨터, 또는 서버에서 구현될 수 있거나, 또는 다수의 상이한 프로세서, 컴퓨터, 또는 서버 사이에 분산될 수 있다. 또한, 모듈 중 일부가, 달성된 기능에 영향을 주지 않고서 도면에 도시된 것과 조합되어, 병렬로 또는 상이한 순서로 작동될 수 있다는 것이 이해될 것이다. 도면에서의 모듈은 또한, 방법에서의 흐름도 단계로서 생각될 수 있다. 모듈은 또한, 그의 코드 전부가 반드시 메모리에 인접하게 배치될 필요가 없고; 코드의 일부는 코드의 다른 부분과는 분리될 수 있으며, 이때 다른 모듈 또는 다른 기능으로부터의 코드가 사이에 배치된다.The processing engine and database of the drawing, designated as modules, may be implemented in hardware or software, and need not be divided into exactly identical blocks as shown in the drawing. Some of the modules may also be implemented on different processors, computers, or servers, or distributed among multiple different processors, computers, or servers. It will also be appreciated that some of the modules may be combined and operated in parallel or in a different order than depicted in the drawings without affecting the functionality achieved. Modules in a drawing can also be thought of as flowchart steps in a method. A module also does not necessarily have all of its code placed contiguously in memory; parts of the code can be separated from other parts of the code, with code from other modules or other functions placed in between. 개시된 기술의 예시적인 응용Exemplary applications of the disclosed technology 생물학적 양 모델(124)의 다양한 구현의 수 많은 비교에 의해 강조되는 바와 같이, 많은 구현은 아키텍처 컴포넌트에서 중복을 공유한다. '생물학적 양 모델'이라는 문구와 관련하여, 생물학적 양 모델은 게놈 시퀀스로부터 복수의 생물학적 양 클래스를 예측한다. 생물학적 양 클래스의 예는 단백질(전사 인자) 결합, 메틸화, 히스톤 수정, DNA 접근성, 보존을 포함한다. 이들 중, 메틸화와 히스톤 수정은 후생적인 것으로 간주된다. 이와는 대조적으로, 염색질은 히스톤 수정 및 DNA 접근성을 지칭한다.As highlighted by the numerous comparisons of different implementations of the biological quantity model (124), many implementations share redundancy in architectural components . With respect to the phrase 'biological quantity model', the biological quantity model predicts multiple biological quantity classes from a genome sequence . Examples of biological quantity classes include protein (transcription factor) binding, methylation, histone modification, DNA accessibility, and conservation . Of these, methylation and histone modification are considered epigenetic . In contrast, chromatin refers to histone modification and DNA accessibility. 따라서, 일부 구현에서, 생물학적 양 모델은 '후생유전학 모델'로 지칭될 수 있다. 다른 구현에서, 생물학적 양 모델은 '염색질 모델'로 지칭될 수 있다. 또 다른 구현에서, 생물학적 양 모델은 '염색질 및 후생유전학 모델' 또는 '후생유전학 및 염색질 모델'로 지칭될 수 있다.Thus, in some implementations, the biological quantity model may be referred to as an 'epigenetic model.' In other implementations, the biological quantity model may be referred to as a 'chromatin model.' In yet other implementations, the biological quantity model may be referred to as a 'chromatin and epigenetic model' or an 'epigenetic and chromatin model.' 생물학적 양 모델(124)의 각각의 요소는 다수의 실시예에서 결합될 수 있는 다수의 구현을 갖는다. 개시된 기술을 위해 구현될 수 있는 다양한 순열은 더 넓은 범위의 유용성, 성능 효율성 및 성능 정확성 모두를 제공한다. 핵산 시퀀스의 관점 및 염색질 구조의 관점으로부터 복수의 추가 시퀀스 포맷을 생성하기 위해 개시된 기술의 많은 구현에서 입력 염기 시퀀스에 적용되는 데이터 변환은 광범위한 유전체학, 단백질 분석, 및 병원성 연구 질문에 폭넓은 적용성을 갖는 풍부한 출력 신호의 출력을 초래하는 혁신적인 전략이다. 이전 버전의 PrimateAI는 높은 성능을 가진 변이체 병원성의 분류를 위해 여러 가지 도구를 사용했다. 이 생물학적 양 모델(124)은 이 방법론에 다른 도구 뿐만 아니라 생물학적 복제 및 전사 프로세스에 영향을 미치는 후생적 신호를 연구하는 추가적인 차원을 도입한다.Each element of the biological quantity model (124) has multiple implementations that can be combined in multiple embodiments . The various permutations that can be implemented for the disclosed technology provide a wider range of usability, performance efficiency, and performance accuracy. In many implementations of the disclosed techniques for generating multiple additional sequence formats from the perspective of nucleic acid sequences and from the perspective of chromatin structure, data transformation applied to the input base sequences is an innovative strategy that results in the output of a rich output signal with broad applicability to a wide range of genomics, protein analysis, and pathogenicity research questions. Previous versions of PrimateAI used several tools for classification of variant pathogenicity with high performance. This biological quantity model (124) introduces an additional dimension to this methodology as well as other tools to study epigenetic signals that influence biological replication and transcription processes. PrimateAI가 제공하는 다양한 도구 외에도 유전자 발현을 연구하기 위해 염색질 구조의 추가적인 관점이 유용하다는 것은 분명하지만, 개시된 기술이 제공하는 진정한 영향은 전체 유전자 발현 예측 로직에 후생적 신호를 추가하는 데에 있다. 염색질의 DNA 시퀀스 및 히스톤 단백질 성분 둘 모두는 다량의 화학적 변형을 겪을 수 있다. 염색질 성분에 직접 결합하고 염색질 성분의 화학적 변형을 촉매하는 효소는 염색질 구조를 변경할 수 있고, 염색질 구조의 변화는 또한 이들의 타깃 리간드 접근하여 기능하는 염색질-상호작용 효소의 능력을 변경할 수 있다. 염색질의 구조 및 구조를 변경하는 효소는 전사 및 발현을 위한 유전자의 접근성에 직접적으로 영향을 미친다. DNA 변이체는 염색질 구조의 변화를 유발할 수 있으며, 이는 후속적으로 유전자 발현 및 유전자 억제의 적절한 조절에 필요한 전사 인자 결합 및 효소 반응과 같은 후생적 효과를 변화시킬 수 있다.While it is clear that additional perspectives on chromatin structure are useful for studying gene expression beyond the various tools provided by PrimateAI, the real impact of the disclosed technology lies in adding epigenetic signals to the overall gene expression prediction logic. Both the DNA sequence and the histone protein components of chromatin can undergo a number of chemical modifications. Enzymes that bind directly to chromatin components and catalyze chemical modifications of chromatin components can alter chromatin structure, and changes in chromatin structure can also alter the ability of chromatin-interacting enzymes to access and function on their target ligands. Enzymes that alter the structure and conformation of chromatin directly affect the accessibility of genes for transcription and expression. DNA mutations can induce changes in chromatin structure, which can subsequently alter epigenetic effects such as transcription factor binding and enzymatic reactions required for proper regulation of gene expression and gene repression. 역으로, 메틸화 및 단백질 결합 현상과 같은 염색체에 대한 후생적 영향은 돌연변이 비율에 영향을 미칠 수 있으며, 잠재적으로 잠복성 또는 병원성일 수 있는 변이체를 도입할 수 있다. 유전자에 대한 진화적 제약 및 해당 유전자의 변이체의 병원성에 대한 연구는 개시된 기술의 많은 구현에서 입증된 바와 같이 후생적 특징에 의해 증강될 때 상당히 더 포괄적이고 정확하다. 전반적으로, 개시된 기술은 타깃 유전자 시퀀스에 대한 유전자 발현 및 유전자 병원성의 예측에 적용될 수 있는 다양한 출력을 생성하기 위한 훈련 및 학습 전략의 범위에 허용되는 다양한 변경을 보유한다. 개시된 염색질-집중 전략은 유전적 및 환경적 노출-관련 질환의 연구, 약물의 개발, 및 핵산 시퀀스의 단백질로의 전사 및 변환에서의 후생유전학의 영향에 유용하다.Conversely, epigenetic influences on chromosomes, such as methylation and protein binding events, may influence mutation rates and potentially introduce variants that may be silent or pathogenic. Studies of evolutionary constraints on genes and the pathogenicity of variants in those genes are significantly more comprehensive and accurate when augmented by epigenetic features, as demonstrated in many implementations of the disclosed techniques. Overall, the disclosed technology possesses a wide range of variations that allow for a range of training and learning strategies to generate a variety of outputs that can be applied to prediction of gene expression and gene pathogenicity for target gene sequences. The disclosed chromatin-focusing strategy is useful for the study of genetic and environmental exposure-related diseases, drug development, and the impact of epigenetics on the transcription and translation of nucleic acid sequences into proteins. 생물학적 양 모델 개요Biological quantity model overview 도 1은 유전자 시퀀스의 진화적, 후생적 특성을 결정하기 위한 시스템의 프로세스(100)을 설명하는 흐름도이다. 입력 염기 시퀀스(122)는 시퀀스 데이터베이스(110)로부터 추출되고, 입력 염기 시퀀스(126)의 대체 표현을 생성하는 생물학적 양 모델(124)에 의해 처리된다. 입력 염기 시퀀스(126)의 대체 표현은 복수의 생물학적 양 출력 시퀀스(136)의 형태로 대체 생물학적 양 표현으로 변환된다.Figure 1 is a flow chart illustrating a process (100) of a system for determining evolutionary and epigenetic characteristics of a genetic sequence. An input base sequence (122) is extracted from a sequence database (110) and processed by a biological quantity model (124) that generates an alternative representation of the input base sequence (126) . The alternative representation of the input base sequence (126) is converted into an alternative biological quantity representation in the form of a plurality of biological quantity output sequences (136). 생물학적 양 모델 입력 데이터Biological quantity model input data 도 2는 시퀀스 데이터베이스(202)로부터 추출된 뉴클레오티드 염기를 포함하는 예시적인 입력 염기 시퀀스(200)를 개략적으로 도시하며, 여기서 타깃 염기 시퀀스(226)는 상류 컨텍스트 염기를 포함하는 좌측 시퀀스(224) 및 하류 컨텍스트 염기를 포함하는 우측 시퀀스(228)에 의해 플랭킹된다. 상류 컨텍스트 염기(224)는 아데닌, 티민, 시토신 또는 구아닌과 동일할 수 있는 뉴클레오티드 염기의 시퀀스 {x1, x2, x3, ..., xn}이다. 타깃 염기 시퀀스(226)는 후속하여 상류 컨텍스트 염기(224)를 따른다. 타깃 염기 시퀀스(226)는 아데닌, 티민, 시토신 또는 구아닌과 같을 수 있는 뉴클레오티드 염기의 시퀀스 {y1, y2, y3, ..., yn}를 포함한다. 하류 컨텍스트 염기(228)는 후속하여 타깃 염기 시퀀스(226)를 따른다. 하류 컨텍스트 염기는 아데닌, 티민, 시토신 또는 구아닌과 같을 수 있는 뉴클레오티드 염기 {z1, z2, z3, ..., zn}의 시퀀스이다. 입력 염기 시퀀스(200)는 또한 상류 컨텍스트 염기(224), 타깃 염기 시퀀스(226), 또는 하류 컨텍스트 염기(228)에서 알려지지 않았거나 누락된 갭 위치를 포함할 수 있다.FIG. 2 schematically illustrates an exemplary input base sequence (200) comprising nucleotide bases extracted from a sequence database (202), wherein a target base sequence (226) is flanked by a left sequence (224) comprising upstream context bases and a right sequence (228) comprising downstream context bases . The upstream context base (224) is a sequence of nucleotide bases {x 1 , x 2 , x 3 , ..., x n }, which can be identical to adenine, thymine, cytosine or guanine. The target base sequence (226) subsequently follows the upstream context base (224) . The target base sequence (226) comprises a sequence of nucleotide bases {y 1 , y 2 , y 3 , ..., y n }, which may be adenine, thymine, cytosine or guanine. The downstream context base (228) subsequently follows the target base sequence (226) . The downstream context base is a sequence of nucleotide bases {z 1 , z 2 , z 3 , ..., z n }, which may be adenine, thymine, cytosine or guanine. The input base sequence (200) may also include unknown or missing gap positions in the upstream context base (224), the target base sequence (226), or the downstream context base (228). 도 3은 2개의 예시적인 대체 시퀀스(322 및 342)를 갖는 예시적인 기준 유전자 시퀀스(302)로부터의 대체 시퀀스(300)의 일례를 도시하며, 여기서, 대체 시퀀스는 단일 염기 위치에서 각각의 단일 뉴클레오티드 변이체를 보유하지만, 그 외에는 기준 시퀀스와 동일한 조성을 보유한다. 예를 들어, 단일 뉴클레오티드 치환은 뉴클레오티드(306)와 비교하여 변이체(326) 및 변이체(336)로 나타난다. 동일한 타깃 염기 시퀀스를 보유하는 것에 더하여, 상류 시퀀스(304, 324, 및 344)는 서로 동일하고, 하류 시퀀스(308, 328, 및 348)는 서로 동일하다.FIG. 3 illustrates an example of a replacement sequence (300) from an exemplary reference gene sequence (302) having two exemplary replacement sequences (322 and 342), wherein the replacement sequences each have a single nucleotide variant at a single base position, but otherwise have the same composition as the reference sequence. For example, a single nucleotide substitution results in variant (326) and variant (336) compared to nucleotide (306) . In addition to having the same target base sequence, the upstream sequences (304, 324, and 344) are identical to each other, and the downstream sequences (308, 328, and 348) are identical to each other. 도 4는 개시된 기술의 일 구현을 위해 개발된 훈련 데이터세트(400)에 속하는 시퀀스의 유전자 구성을 도시한다. 하나의 훈련 데이터세트(422)는 후생적 효과에 의해 교란된 대체 시퀀스(예: 단일 뉴클레오티드 변이체(433)를 보유하는 대체 시퀀스 A(432))를 포함하고 제2 훈련 데이터세트(452)는 후생적 효과에 의해 교란되지 않는 대체 시퀀스(예: 단일 뉴클레오티드 변이체(463)를 보유하는 시퀀스(462))를 포함한다. 단일 뉴클레오티드 변이체(433 및 463)는 기준 염기 위치(403)와 조성이 상이하지만; 기준 시퀀스, 대체 시퀀스 A(432) 및 대체 시퀀스 B(462) 내의 모든 다른 염기 위치는 상이하지 않다.FIG. 4 illustrates the genetic makeup of sequences belonging to training datasets (400) developed for one implementation of the disclosed technology . One training dataset (422) includes a replacement sequence that is perturbed by an epigenetic effect (e.g., a replacement sequence A (432) having a single nucleotide variant (433)) and a second training dataset (452) includes a replacement sequence that is not perturbed by an epigenetic effect (e.g., a sequence (462) having a single nucleotide variant (463)). Single nucleotide variants (433 and 463) differ in composition from the reference base position (403); however, all other base positions within the reference sequence, alternate sequence A (432), and alternate sequence B (462) are not different. 생물학적 양 모델 구조Biological quantity model structure 도 5는 도 4에 설명된 제1 훈련 세트로 훈련된 생물학적 양 모델(124)로 도 1의 시스템(100)에 적용된 다음, 도 4에 설명된 제2 훈련 세트로 재훈련을 받고 있는 훈련 절차의 일 구현(500)을 개략적으로 도시한다.FIG. 5 schematically illustrates an implementation (500) of a training procedure in which a biological quantity model (124) trained with the first training set described in FIG. 4 is applied to the system (100) of FIG. 1 and then retrained with the second training set described in FIG. 4. 시퀀스 데이터베이스(502)로부터 획득된 시퀀스는 먼저 제1 훈련 반복 세트(566)에 대한 제1 훈련 데이터세트로부터 획득되어, 염기 해상도에서 유전자 발현의 변화를 검출하도록 구성된 생물학적 양 모델(124)을 활용하여, 입력 염기 시퀀스(524)로부터 복수의 생물학적 양 출력 시퀀스(548)을 생성하며, 입력 염기 시퀀스(524)를 처리하고 입력 염기 시퀀스(524)의 대체 표현(예: 합성된 표현)을 생성하는 생물학적 양 모델(124), 및 생물학적 양 출력 시퀀스 생성기(528)를 포함한다. 그 다음, 생물학적 양 모델(124)은 생물학적 양 모델(124)의 모델 구성에 대한 임의의 변경 없이 제2 훈련 데이터세트로부터 획득된 시퀀스에 대한 제2 훈련 반복 세트(586)를 거친다.The sequences obtained from the sequence database (502) are first obtained from a first training dataset for a first training iteration set (566), and a biological quantity model (124) configured to detect changes in gene expression at base resolution is utilized to generate a plurality of biological quantity output sequences (548) from the input base sequence (524), including a biological quantity model (124) that processes the input base sequence (524) and generates an alternative representation (e.g., a synthesized representation) of the input base sequence (524), and a biological quantity output sequence generator (528). Next, the biological quantity model (124) is subjected to a second training iteration set (586) on sequences obtained from the second training dataset without any changes to the model configuration of the biological quantity model (124). 일 구현에서, 생물학적 양 모델(124)은 최저부터 최고의 시퀀스로 배열된 잔차 블록의 그룹을 포함한다. 잔차 블록의 각각의 그룹은 잔차 블록에서의 컨볼루션 필터의 수, 잔차 블록의 컨볼루션 윈도우 크기, 및 잔차 블록의 아트러스 컨볼루션 속도에 의해 파라미터화된다. 아트러스 컨볼루션 속도는, 일부 구현에서, 더 낮은 잔차 블록 그룹으로부터 더 높은 잔차 블록 그룹으로 비지수적으로 진행된다. 다른 구현에서, 그것은 기하급수적으로 진행된다. 잔차 블록 그룹 간의 컨볼루션 윈도우 크기는 다르며, 각 잔차 블록은 적어도 하나의 일괄 정규화 계층, 적어도 하나의 정류 선형 유닛(약어 ReLU) 계층, 적어도 하나의 아트러스 컨볼루션 계층, 적어도 하나의 잔차 연결로 구성된다.In one implementation, the biological quantity model (124) comprises a group of residual blocks arranged in sequence from lowest to highest . Each group of residual blocks is parameterized by the number of convolution filters in the residual block, the convolution window size of the residual block, and the atlas convolution rate of the residual block. The artus convolution speedup, in some implementations, proceeds non-exponentially from lower residual block groups to higher residual block groups. In other implementations, it proceeds exponentially. The convolution window sizes are different between residual block groups, and each residual block consists of at least one batch normalization layer, at least one rectified linear unit (ReLU) layer, at least one atomic convolution layer, and at least one residual connection. 일 구현에서, 입력의 차원은 (Cu + L + Cd) x 4이며, 여기서 Cu는 상류 플랭킹 컨텍스트 염기의 수이고, Cd는 하류 플랭킹 컨텍스트 염기의 수이고, L은 입력 프로모터 시퀀스에서 염기의 수이다. 출력의 차원은 4 x L이다.일부 구현에서, 잔차 블록의 각각의 그룹은 선행하는 입력을 처리함으로써 중간 출력을 생성하고, 중간 출력의 차원은 (I-[{(W-1) * D} * A]) x N이고, 여기서, I는 선행하는 입력의 차원이고, W는 잔차 블록의 컨볼루션 윈도우 크기이고, D는 잔차 블록의 아트러스 컨볼루션 속도이고, A는 그룹에서의 아트러스 컨볼루션 계층의 수이고, N은 잔차 블록에서의 컨볼루션 필터의 수이다.In one implementation, the dimension of the input is (Cu + L + Cd) x 4, where Cu is the number of upstream flanking context bases, Cd is the number of downstream flanking context bases, and L is the number of bases in the input promoter sequence. The dimension of the output is 4 x L. In some implementations, each group of residual blocks produces an intermediate output by processing the preceding input, and the dimension of the intermediate output is (I-[{(W-1) * D} * A]) x N, where I is the dimension of the preceding input, W is the convolution window size of the residual block, D is the atlas convolution rate of the residual block, A is the number of atlas convolution layers in the group, and N is the number of convolution filters in the residual block. 일 구현에서, 입력은 입력 시퀀스의 좌측에 200개의 상류 플랭킹 컨텍스트 염기(Cu)를 갖고 입력 시퀀스의 우측에 200개의 하류 플랭킹 컨텍스트 염기(Cd)를 가진다. 입력 시퀀스(L)의 길이는, 3001과 같이, 임의적일 수 있다. 일 구현에서, 제1 그룹에서의 각각의 잔차 블록은 32개의 컨볼루션 필터, 11개의 컨볼루션 윈도우 크기, 및 1개의 아트러스 컨볼루션 속도를 갖고, 제2 그룹 내의 각각의 잔차 블록은 32개의 컨볼루션 필터, 11개의 컨볼루션 윈도우 크기, 및 4개의 아트러스 컨볼루션 속도를 갖는다. 다른 아키텍처에서, 각각의 잔차 블록은 32개의 컨볼루션 필터, 11개의 컨볼루션 윈도우 크기, 및 1개의 아트러스 컨볼루션 속도를 갖는다.In one implementation, the input has 200 upstream flanking context bases (Cu) on the left side of the input sequence and 200 downstream flanking context bases (Cd) on the right side of the input sequence. The length of the input sequence (L) can be arbitrary, such as 3001. In one implementation, each residual block in the first group has 32 convolution filters, 11 convolution window sizes, and 1 atlas convolution rate, and each residual block in the second group has 32 convolution filters, 11 convolution window sizes, and 4 atlas convolution rates . In another architecture, each residual block has 32 convolution filters, 11 convolution window sizes, and 1 atlas convolution rate. 일 구현에서, 입력은 입력 시퀀스의 좌측에 1,000개의 상류 플랭킹 컨텍스트 염기(Cu)를 갖고 입력 시퀀스의 우측에 1,000개의 하류 플랭킹 컨텍스트 염기(Cd)를 가진다. 입력 시퀀스(L)의 길이는, 3001과 같이, 임의적일 수 있다. 일 구현에서, 4개의 잔차 블록의 적어도 3개의 그룹 및 적어도 3개의 스킵 연결이 존재한다. 제1 그룹의 각각의 잔차 블록은 32개의 컨볼루션 필터, 11개의 컨볼루션 윈도우 크기, 및 1개의 아트러스 컨볼루션 속도를 갖고, 제2 그룹의 각각의 잔차 블록은 32개의 컨볼루션 필터, 11개의 컨볼루션 윈도우 크기, 및 4개의 아트러스 컨볼루션 속도를 갖고, 제3 그룹의 각각의 잔차 블록은 32개의 컨볼루션 필터, 21개의 컨볼루션 윈도우 크기, 및 19개의 아트러스 컨볼루션 속도를 갖는다.In one implementation, the input has 1,000 upstream flanking context bases (Cu) on the left side of the input sequence and 1,000 downstream flanking context bases (Cd) on the right side of the input sequence. The length of the input sequence (L) can be arbitrary, such as 3001. In one implementation, there are at least three groups of four residual blocks and at least three skip connections . Each residual block of a first group has 32 convolution filters, 11 convolution window sizes, and 1 atlas convolution rate, each residual block of a second group has 32 convolution filters, 11 convolution window sizes, and 4 atlas convolution rates, and each residual block of a third group has 32 convolution filters, 21 convolution window sizes, and 19 atlas convolution rates. 일 구현에서, 입력은 입력 시퀀스의 좌측에 5,000개의 상류 플랭킹 컨텍스트 염기(Cu)를 갖고 입력 시퀀스의 우측에 5,000개의 하류 플랭킹 컨텍스트 염기(Cd)를 가진다. 입력 시퀀스(L)의 길이는, 3001과 같이, 임의적일 수 있다. 일 구현에서, 4개의 잔차 블록의 적어도 4개의 그룹 및 적어도 4개의 스킵 연결이 존재한다. 제1 그룹의 각각의 잔차 블록은 32개의 컨볼루션 필터, 11개의 컨볼루션 윈도우 크기, 및 1개의 아트러스 컨볼루션 속도를 갖고, 제2 그룹의 각각의 잔차 블록은 32개의 컨볼루션 필터, 11개의 컨볼루션 윈도우 크기, 및 4개의 아트러스 컨볼루션 속도를 갖고, 제3 그룹의 각각의 잔차 블록은 32개의 컨볼루션 필터, 21개의 컨볼루션 윈도우 크기, 및 19개의 아트러스 컨볼루션 속도를 갖고, 제4 그룹의 각각의 잔차 블록은 32개의 컨볼루션 필터, 41개의 컨볼루션 윈도우 크기, 및 25개의 아트러스 컨볼루션 속도를 갖는다.In one implementation, the input has 5,000 upstream flanking context bases (Cu) on the left side of the input sequence and 5,000 downstream flanking context bases (Cd) on the right side of the input sequence. The length of the input sequence (L) can be arbitrary, such as 3001. In one implementation, there are at least four groups of four residual blocks and at least four skip connections . Each of the residual blocks of the first group has 32 convolution filters, 11 convolution window sizes, and 1 atlas convolution rate, each of the residual blocks of the second group has 32 convolution filters, 11 convolution window sizes, and 4 atlas convolution rates, each of the residual blocks of the third group has 32 convolution filters, 21 convolution window sizes, and 19 atlas convolution rates, and each of the residual blocks of the fourth group has 32 convolution filters, 41 convolution window sizes, and 25 atlas convolution rates. 일반적으로 말하면, 생물학적 양 모델(124)은 규칙 기반 모델, 트리 기반 모델, 또는 기계 학습 모델일 수 있다. 예시로는 다층 퍼셉트론(MLP), 피드포워드 신경망, 완전 연결 신경망, 완전 컨볼루션 신경망, ResNet, WaveNet과 같은 시퀀스 대 시퀀스(Seq2Seq) 모델, 의미 분할 신경망, 및 생성적 적대 신경망(GAN)(예: CycleGAN, StyleGAN, pixelRNN, text-2-image, DiscoGAN, IsGAN) 등이 있다.Generally speaking, the biological quantity model (124) can be a rule-based model, a tree-based model, or a machine learning model . Examples include multilayer perceptrons (MLPs), feedforward neural networks, fully-connected neural networks, fully convolutional neural networks, sequence-to-sequence (Seq2Seq) models such as ResNets, WaveNets, semantic segmentation neural networks, and generative adversarial networks (GANs) (e.g., CycleGAN, StyleGAN, pixelRNN, text-2-image, DiscoGAN, IsGAN). 일부 구현에서, 생물학적 양 모델(124)은 Transformer, Vision Transformer (ViT), Bidirectional Transformer (BERT), Detection Transformer (DETR), Deformable DETR, UP-DETR, DeiT, Swin, GPT, iGPT, GPT-2, GPT-3, BERT, SpanBERT, RoBERTa, XLNet, ELECTRA, UniLM, BART, T5, ERNIE (THU), KnowBERT, DeiT-Ti, DeiT-S, DeiT-B, T2T-ViT-14, T2T-ViT-19, T2T-ViT-24, PVT-Small, PVT-Medium, PVT-Large, TNT-S, TNT-B, CPVT-S, CPVT-S-GAP, CPVT-B, Swin-T, Swin-S, Swin-B, Twins-SVT-S, Twins-SVT-B, Twins-SVT-L, Shuffle-T, Shuffle-S, Shuffle-B, XCiT-S12/16, CMT-S, CMT-B, VOLO-D1, VOLO-D2, VOLO-D3, VOLO-D4, MoCo v3, ACT, TSP, Max-DeepLab, VisTR, SETR, Hand-Transformer, HOT-Net, METRO, Image Transformer, Taming transformer, TransGAN, IPT, TTSR, STTN, Masked Transformer, CLIP, DALL-E, Cogview, UniT, ASH, TinyBert, FullyQT, ConvBert, FCOS, Faster R-CNN + FPN, DETR-DC5, TSP-FCOS, TSP-RCNN, ACT+MKDD (L=32), ACT+MKDD (L=16), SMCA, Efficient DETR, UP-DETR, UP-DETR, ViTB/16-FRCNN, ViT-B/16-FRCNN, PVT-Small+RetinaNet, Swin-T+RetinaNet, Swin-T+ATSS, PVT-Small+DETR, TNT-S+DETR, YOLOS-Ti, YOLOS-S, 및 YOLOS-B와 같은 셀프 어텐션 메커니즘을 포함할 수 있다.In some implementations, the biological quantity model (124) comprises a Transformer, a Vision Transformer (ViT), a Bidirectional Transformer (BERT), a Detection Transformer (DETR), a Deformable DETR, an UP-DETR, a DeiT, a Swin, a GPT, an iGPT, a GPT-2, a GPT-3, a BERT, a SpanBERT, a RoBERTa, an XLNet, an ELECTRA, a UniLM, a BART, a T5, an ERNIE (THU), a KnowBERT, a DeiT-Ti, a DeiT-S, a DeiT-B, a T2T-ViT-14, a T2T-ViT-19, a T2T-ViT-24, a PVT-Small, a PVT-Medium, a PVT-Large, a TNT-S, a TNT-B, a CPVT-S, a CPVT-S-GAP, a CPVT-B, a Swin-T, a Swin-S, a Swin-B, a Twins-SVT-S, Twins-SVT-B, Twins-SVT-L, Shuffle-T, Shuffle-S, Shuffle-B, METRO, Image Transformer, Taming transformer, TransGAN, IPT, TTSR, STTN, Masked Transformer, CLIP, DALL-E, Cogview, UniT, ASH, TinyBert, FullyQT, ConvBert, FCOS, Faster R-CNN + FPN, DETR-DC5, TSP-FCOS, TSP-RCNN, ACT+MKDD (L=32), ACT+MKDD (L=16), SMCA, Efficient DETR, It may include self-attention mechanisms such as UP-DETR, UP-DETR, ViTB/16-FRCNN, ViT-B/16-FRCNN, PVT-Small+RetinaNet, Swin-T+RetinaNet, Swin-T+ATSS, PVT-Small+DETR, TNT-S+DETR, YOLOS-Ti, YOLOS-S, and YOLOS-B. 일부 구현에서, 생물학적 양 모델(124)의 예는 복수의 컨볼루션 계층을 갖는 컨볼루션 신경망(CNN), 장기 단기 메모리 네트워크(LSTM), 양방향 LSTM(Bi-LSTM), 또는 게이트형 순환 유닛과 같은 순환 신경망(RNN), 및 CNN과 RNN 양자의 조합을 포함한다.In some implementations, examples of biological quantity models (124) include convolutional neural networks (CNNs) having multiple convolutional layers, long short-term memory networks (LSTMs), bidirectional LSTMs (Bi-LSTMs), or recurrent neural networks (RNNs) such as gated recurrent units, and combinations of both CNNs and RNNs. 일부 구현에서, 생물학적 양 모델(124)은 1D 컨볼루션, 2D 컨볼루션, 3D 컨볼루션, 4D 컨볼루션, 5D 컨볼루션, 확장형 또는 아트러스 컨볼루션, 전치 컨볼루션, 깊이별 분리가능 컨볼루션, 포인트별 컨볼루션, 1x1 컨볼루션, 그룹 컨볼루션, 편평형 컨볼루션, 공간 및 교차 채널 컨볼루션, 셔플 그룹형 컨볼루션, 공간 분리가능 컨볼루션, 및 디컨볼루션을 사용할 수 있다. 생물학적 양 모델(124)은 하나 이상의 손실 함수, 예컨대 로지스틱 회귀/로그 손실, 다중클래스 교차-엔트로피/소프트맥스 손실, 이진 교차-엔트로피 손실, 평균 제곱 오류 손실, L1 손실, L2 손실, 평활한 L1 손실, 및 Huber 손실을 사용할 수 있다. 생물학적 양 모델(124)은 임의의 병렬성, 효율성, 및 압축 체계, 예컨대 TFRecord, 압축 인코딩(예: PNG), 샤딩, 맵 변환을 위한 병렬 검출, 배칭, 프리페칭, 모델 병렬성, 데이터 병렬성, 및 동기식/비동기식 확률적 기울기 하강법(SGD)을 사용할 수 있다. 유전자 발현 모델(124)은 업샘플링 계층, 다운샘플링 계층, 순환 연결, 게이트 및 게이트형 메모리 유닛(예: LSTM 또는 GRU), 잔차 블록, 잔차 연결, 하이웨이 연결, 스킵 연결, 핍홀(peephole) 연결, 활성화 함수(예: ReLU(rectifying linear unit), 리키 ReLU(leaky ReLU), ELU(exponential liner unit), 시그모이드 및 tanh(hyperbolic tangent)와 같은 비선형 변환 함수), 일괄 정규화 계층, 규칙화 계층, 드롭아웃, 풀링 계층(예컨대, 최대 또는 평균 풀링), 글로벌 평균 풀링 계층, 및 주의 메커니즘을 포함할 수 있다.In some implementations, the biological sheep model (124) can use 1D convolution, 2D convolution, 3D convolution, 4D convolution, 5D convolution, dilated or atrus convolution, transpose convolution, depth-wise separable convolution, point-wise convolution, 1x1 convolution, group convolution, flat convolution, spatial and cross-channel convolution, shuffle grouped convolution, spatial separable convolution, and deconvolution . The biological sheep model (124) can use one or more loss functions, such as logistic regression/log loss, multiclass cross-entropy/softmax loss, binary cross-entropy loss, mean squared error loss, L1 loss, L2 loss, smoothed L1 loss, and Huber loss . The biological quantity model (124) can use arbitrary parallelism, efficiency, and compression schemes, such as TFRecord, compressed encoding (e.g., PNG), sharding, parallel detection for map transformation, batching, prefetching, model parallelism, data parallelism, and synchronous/asynchronous stochastic gradient descent (SGD). The gene expression model (124) may include upsampling layers, downsampling layers, recurrent connections, gates and gated memory units (e.g., LSTMs or GRUs), residual blocks, residual connections, highway connections, skip connections, peephole connections, activation functions (e.g., nonlinear transformation functions such as rectifying linear units (ReLU), leaky ReLU, exponential linear units (ELU), sigmoid, and hyperbolic tangent (tanh)), batch normalization layers, regularization layers, dropout, pooling layers (e.g., max or average pooling), global average pooling layers, and attention mechanisms. 일부 구현에서, 생물학적 양 모델(124)은 선형 회귀 모델, 로지스틱 회귀 모델, Elastic Net 모델, 서포트 벡터 머신(SVM), 랜덤 포레스트(RF), 결정 트리, 또는 향상된 결정 트리(예: XGBoost), 또는 일부 다른 트리 기반 로직(예: 메트릭 트리, kd-트리, R-트리, 범용 B-트리, X-트리, 볼 트리, 로컬리티 센서티브 해시, 및 역 인덱스)일 수 있다. 생물학적 양 모델(124)은, 일부 구현에서, 다수의 모델의 총체일 수 있다.In some implementations, the biological quantity model (124) can be a linear regression model, a logistic regression model, an Elastic Net model, a support vector machine (SVM), a random forest (RF), a decision tree, or an enhanced decision tree (e.g., XGBoost), or some other tree-based logic (e.g., metric trees, kd-trees, R-trees, universal B-trees, X-trees, ball trees, locality sensitive hashes, and inverted indexes) . The biological quantity model (124), in some implementations, can be an aggregation of multiple models. 일부 구현에서, 생물학적 양 모델(124)은 역전파 기반 기울기 업데이트 기법을 사용하여 훈련될 수 있다. 상기 모델을 훈련하기 위해 사용될 수 있는 예시적인 기울기 하강 기법은 확률적 기울기 하강법, 일괄 기울기 하강법, 및 미니-일괄 기울기 하강법을 포함한다. 상기 모델을 훈련하는 데 사용될 수 있는 기울기 하강 최적화 알고리즘의 일부 예는 Momentum, Nesterov 가속화된 기울기, Adagrad, Adadelta, RMSprop, Adam, AdaMax, Nadam, 및 AMSGrad이다.In some implementations, the biological quantity model (124) can be trained using a backpropagation-based gradient update technique. Exemplary gradient descent techniques that can be used to train the above model include stochastic gradient descent, batch gradient descent, and mini-batch gradient descent. Some examples of gradient descent optimization algorithms that can be used to train the above models are Momentum, Nesterov accelerated gradient, Adagrad, Adadelta, RMSprop, Adam, AdaMax, Nadam, and AMSGrad. 도 6은 도 4에 설명된 제1 훈련 세트를 사용하여 도 1의 시스템(100)에 적용된 다음, 도 4에 설명된 제2 훈련 세트의 하위 세트와의 재훈련을 진행하고, 제2 훈련 세트로부터의 샘플의 나머지 하위 세트와의 생물학적 양 모델(124)의 모델 검증이 뒤따르는 훈련 절차의 다른 구현(600)을 개략적으로 도시한다. 시퀀스 데이터베이스(602)로부터 획득된 시퀀스는 먼저 제1 훈련 반복 세트(666)에 대한 제1 훈련 데이터세트로부터 획득되어, 염기 해상도에서 유전자 발현의 변화를 검출하도록 구성된 생물학적 양 모델(124)을 활용하여, 입력 염기 시퀀스(624)로부터 복수의 생물학적 양 출력 시퀀스(648)를 생성하며, 입력 염기 시퀀스(624)를 처리하고 입력 염기 시퀀스(624)의 대체 표현(예: 합성 표현)을 생성하는 생물학적 양 모델(124), 및 생물학적 양 출력 시퀀스 생성기(628)를 포함한다. 그 다음, 생물학적 양 모델(124)은 생물학적 양 모델(124)의 모델 구성에 대한 임의의 변경 없이 제2 훈련 데이터세트로부터 획득된 시퀀스의 하위 세트에 대한 제2 훈련 반복 세트(667)를 거친다. 훈련 반복(666) 및 훈련 반복(667)에 이어서, 생물학적 양 모델(124)은 제2 훈련 데이터세트의 제2 하위 세트와의 검증 프로세스(686)를 거치며, 여기서 훈련 데이터세트의 제1 및 제2 하위 세트는 2개가 중복되지 않고(668), 이는 생물학적 양 모델(124)이 더 다양하고 보이지 않는 훈련 예제에 대해 훈련될 수 있게 한다.FIG. 6 schematically illustrates another implementation (600) of a training procedure applied to the system (100) of FIG. 1 using the first training set described in FIG. 4, followed by retraining with a subset of the second training set described in FIG. 4, followed by model validation of the biological quantity model (124) with the remaining subset of samples from the second training set. The sequences obtained from the sequence database (602) are first obtained from a first training dataset for a first training iteration set (666), and a biological quantity model (124) configured to detect changes in gene expression at base resolution is utilized to generate a plurality of biological quantity output sequences (648) from the input base sequence (624), including a biological quantity model (124) that processes the input base sequence (624) and generates an alternative representation (e.g., a synthetic representation) of the input base sequence (624), and a biological quantity output sequence generator (628). Next, the biological sheep model (124) is subjected to a second training iteration set (667) on a subset of sequences obtained from the second training dataset without any change to the model configuration of the biological sheep model (124) . Following the training iterations (666) and the training iterations (667), the biological sheep model (124) is subjected to a validation process (686) with a second subset of the second training dataset, wherein the first and second subsets of the training dataset are non-overlapping (668), which allows the biological sheep model (124) to be trained on more diverse and unseen training examples. 도 7은 변이체 분류를 위한 도 1로부터의 시스템(100)의 구현(700)의 개략도이며, 여기서 상기 시스템을 사용하여 783에서 표현된 각각의 시퀀스에 대한 생물학적 양 모델(124)의 각각의 모델 출력을 비교함으로써 염기 해상도에서 기준 시퀀스(702) 및 대체 시퀀스(704)를 비교한다. 기준 시퀀스(702)는 생물학적 양 모델(124)에 의해 개별적으로 처리된다. 대체 표현 생성기(722)는 기준 입력 염기 시퀀스(702)를 처리하여 대체 표현을 생성하고(예: 합성된 표현 시퀀스), 생물학적 양 출력 시퀀스 생성기(742)는 대체 표현을 처리하여 복수의 생물학적 양 출력 시퀀스(762)를 생성한다. 대체 시퀀스(704)는 생물학적 양 모델(124)에 의해 개별적으로 처리된다. 대체 표현 생성기(724)는 기준 입력 염기 시퀀스(704)를 처리하여 대체 표현을 생성하고(예: 합성된 표현 시퀀스), 생물학적 양 출력 시퀀스 생성기(744)는 대체 표현을 처리하여 복수의 생물학적 양 출력 시퀀스(764)을 생성한다. 흐름도(783)를 종료시키는 것으로 나타낸 프로세스에서 입증된 바와 같이, 복수의 생물학적 양 출력 시퀀스(762) 및 복수의 생물학적 양 출력 시퀀스(764)는 염기 해상도에서 비교된다.FIG. 7 is a schematic diagram of an implementation (700) of the system (100) from FIG. 1 for variant classification, wherein the system is used to compare a reference sequence (702) and a substitute sequence (704) at base resolution by comparing the respective model outputs of the biological quantity models (124) for each sequence represented at 783. The reference sequence (702) is individually processed by the biological quantity model (124). The alternative representation generator (722) processes the reference input base sequence (702) to generate an alternative representation (e.g., a synthesized representation sequence), and the biological quantity output sequence generator (742) processes the alternative representation to generate a plurality of biological quantity output sequences (762). The alternative sequences (704) are individually processed by the biological quantity model (124) . The alternative representation generator (724) processes the reference input base sequence (704) to generate an alternative representation (e.g., a synthesized representation sequence), and the biological quantity output sequence generator (744) processes the alternative representation to generate a plurality of biological quantity output sequences (764). As evidenced by the process shown in terminating the flowchart (783), the plurality of biological quantity output sequences (762) and the plurality of biological quantity output sequences (764) are compared at base resolution. 도 8은 기준 시퀀스와 대체 시퀀스의 비교가 최종 평균 델타 값(833)에 의해 정량화되는 개시된 기술의 일 구현(800)의 흐름도이다. 최종 평균 델타 값(833)을 결정하기 위해, 염기 해상도 병원성 분류 로직(826)은 기준 시퀀스에 대해 예측된 복수의 생물학적 양 출력 시퀀스(810)와 대체 시퀀스 사이의 차이를 처리하도록 구성된다. 최종 평균 델타 값(833)은 기준 시퀀스와 대체 시퀀스를 비교하는 제1 누적 평균 델타 값(822) 및 기준 시퀀스와 대체 시퀀스를 비교하는 제2 평균 델타 값(824)으로부터의 평균으로 취해진다. 제1 델타 시퀀스(812)는 기준 시퀀스로부터 예측되는 제1 기준 생물학적 양 출력 시퀀스(802)와 대체 시퀀스로부터 예측되는 제1 대체 생물학적 양 출력 시퀀스(804) 사이의 염기 당 차이로 생성된다. 제2 델타 시퀀스(814)는 제2 기준 생물학적 양 출력 시퀀스(806)와 제2 대체 생물학적 양 출력 시퀀스(808) 사이의 염기 당 차이로 생성된다. 제1 누적 평균 델타 값(822)은 제1 델타 시퀀스(812) 내의 각각의 염기 위치에 대해 획득된 델타 값의 평균으로 취해진다. 제2 누적 평균 델타 값(824)은 제2 델타 시퀀스(814) 내의 각각의 염기 위치에 대해 획득된 델타 값의 평균으로 취해진다. 최종 평균 델타 값(833)에 기초하여, 대체 시퀀스에 의해 표현되는 변이체는 보존 상태(846)로 분류될 수 있으며, 여기서 변이체는 보존된 상태(842) 또는 비보존된 상태(844)에 속하는 것으로 분류될 수 있다.FIG. 8 is a flow diagram of one implementation (800) of the disclosed technology in which a comparison of a reference sequence to a surrogate sequence is quantified by a final mean delta value (833) . To determine the final mean delta value (833), base resolution pathogenicity classification logic (826) is configured to process differences between a plurality of biological positive output sequences (810) predicted for the reference sequence and the surrogate sequences . The final mean delta value (833) is taken as an average from a first cumulative mean delta value (822) comparing the reference sequence to the surrogate sequences and a second mean delta value (824) comparing the reference sequence to the surrogate sequences . A first delta sequence (812) is generated as the per-base difference between a first reference biological positive output sequence (802) predicted from the reference sequence and a first surrogate biological positive output sequence (804) predicted from the surrogate sequences . A second delta sequence (814) is generated as the per-base difference between the second reference biological quantity output sequence (806) and the second alternative biological quantity output sequence (808) . A first cumulative average delta value (822) is taken as the average of the delta values obtained for each base position within the first delta sequence (812). The second cumulative average delta value (824) is taken as the average of the delta values obtained for each base position within the second delta sequence (814). Based on the final average delta value (833), the variant represented by the replacement sequence can be classified into a conserved state (846), where the variant can be classified as belonging to a conserved state (842) or a non-conserved state (844). 도 9은 기준 시퀀스와 대체 시퀀스의 비교가 최종 합 델타 값(933)에 의해 정량화되는 개시된 기술의 일 구현(900)의 흐름도이다. 최종 합 델타 값(933)을 결정하기 위해, 염기 해상도 병원성 분류 로직(926)은 기준 시퀀스에 대해 예측된 복수의 생물학적 양 출력 시퀀스(910)와 대체 시퀀스 사이의 차이를 처리하도록 구성된다. 최종 합 델타 값(933)은 기준 시퀀스와 대체 시퀀스를 비교하는 제1 누적 합 델타 값(922) 및 기준 시퀀스와 대체 시퀀스를 비교하는 제2 합 델타 값(924)으로부터의 합으로 취해진다. 제1 델타 시퀀스(912)는 기준 시퀀스로부터 예측되는 제1 기준 생물학적 양 출력 시퀀스(902)와 대체 시퀀스로부터 예측되는 제1 대체 생물학적 양 출력 시퀀스(904) 사이의 염기 당 차이로 생성된다. 제2 델타 시퀀스(914)는 제2 기준 생물학적 양 출력 시퀀스(906)와 제2 대체 생물학적 양 출력 시퀀스(909) 사이의 염기 당 차이로 생성된다. 제1 누적 합 델타 값(922)은 제1 델타 시퀀스(912) 내의 각각의 염기 위치에 대해 획득된 델타 값의 합으로 취해진다. 제2 누적 합 델타 값(924)은 제2 델타 시퀀스 (914) 내의 각각의 염기 위치에 대해 획득된 델타 값의 합으로 취해진다. 최종 합 델타 값(933)에 기초하여, 대체 시퀀스에 의해 표현되는 변이체는 보존 상태(946)로 분류될 수 있으며, 여기서 변이체는 보존된 상태(942) 또는 비보존된 상태(944)에 속하는 것으로 분류될 수 있다.FIG. 9 is a flow diagram of one implementation (900) of the disclosed technique in which a comparison of a reference sequence to a surrogate sequence is quantified by a final sum delta value (933) . To determine the final sum delta value (933), base resolution pathogenicity classification logic (926) is configured to process differences between a plurality of biological positive output sequences (910) predicted for the reference sequence and the surrogate sequences . The final sum delta value (933) is taken as the sum from a first cumulative sum delta value (922) comparing the reference sequence to the surrogate sequences and a second sum delta value (924) comparing the reference sequence to the surrogate sequences . A first delta sequence (912) is generated as the per-base difference between a first reference biological positive output sequence (902) predicted from the reference sequence and a first surrogate biological positive output sequence (904) predicted from the surrogate sequences . A second delta sequence (914) is generated as the per-base difference between the second reference biological quantity output sequence (906) and the second alternative biological quantity output sequence (909) . A first cumulative sum delta value (922) is taken as the sum of the delta values obtained for each base position in the first delta sequence (912) . A second cumulative sum delta value (924) is taken as the sum of the delta values obtained for each base position in the second delta sequence (914) . Based on the final sum delta value (933), a variant represented by the alternative sequence can be classified into a conserved state (946), wherein the variant can be classified as belonging to a conserved state (942) or a non-conserved state (944). 도 10은, 생물학적 양 모델(124)이 종단 간으로 훈련되는 제1 가중치 세트(1042) 및 제2 가중치 세트(1062)를 통해 입력 염기 시퀀스(1022)로부터 2개의 생물학적 양 출력 시퀀스(1084)를 생성하는, 개시된 기술의 일 구현(1000)의 흐름도이다. 제1 가중치 세트(1042)는 입력 염기 시퀀스(1022)의 대체 표현을 생성하기 위해 입력 염기 시퀀스(1022)를 처리하는 대체 표현 생성기(1044)를 포함한다. 제2 가중치 세트(1062)는 입력 염기 시퀀스(1022)의 대체 표현을 처리하고 복수의 생물학적 양 출력 시퀀스(1084)을 생성하는 생물학적 양 출력 시퀀스 생성기(1064)를 포함한다. 개시된 기술의 구현(1000)에서, 2개의 출력 시퀀스(1081 및 1083)가 생물학적 양 출력 시퀀스 생성기(1064)로부터 생성된다.FIG. 10 is a flow diagram of an implementation (1000) of the disclosed technique for generating two biological quantity output sequences (1084) from an input base sequence (1022) via a first set of weights (1042) and a second set of weights (1062) over which a biological quantity model (124) is trained end-to-end . The first set of weights (1042) includes a surrogate representation generator (1044) that processes an input base sequence (1022) to generate an surrogate representation of the input base sequence (1022) . The second set of weights (1062) includes a biological quantity output sequence generator (1064) that processes the surrogate representation of the input base sequence (1022) and generates a plurality of biological quantity output sequences (1084) . In the implementation (1000) of the disclosed technique, two output sequences (1081 and 1083) are generated from the biological quantity output sequence generator (1064). 도 11은, 생물학적 양 모델(124)이 종단 간으로 훈련되는 제1 가중치 세트(1122) 및 제2 가중치 세트(1142)를 통해 입력 염기 시퀀스(1102)로부터 3개의 생물학적 양 출력 시퀀스(1168)를 생성하는, 개시된 기술의 일 구현(1100)의 흐름도이다. 제1 가중치 세트(1122)는 입력 염기 시퀀스(1102)의 대체 표현을 생성하기 위해 입력 염기 시퀀스(1102)를 처리하는 대체 표현 생성기(1124)를 포함한다. 제2 가중치 세트(1142)는 입력 염기 시퀀스(1102)의 대체 표현을 처리하고 복수의 생물학적 양 출력 시퀀스(1168)를 생성하는 생물학적 양 출력 시퀀스 생성기(1144)를 포함한다.FIG. 11 is a flow diagram of an implementation (1100) of the disclosed technique for generating three biological quantity output sequences (1168) from an input base sequence (1102) via a first set of weights (1122) and a second set of weights (1142) over which a biological quantity model (124) is trained end-to-end . The first set of weights (1122) includes a surrogate representation generator (1124) that processes an input base sequence (1102) to generate an surrogate representation of the input base sequence (1102) . The second set of weights (1142) includes a biological quantity output sequence generator (1144) that processes the surrogate representation of the input base sequence (1102) and generates a plurality of biological quantity output sequences (1168). 도 11의 구현(1100)은 3개의 출력 시퀀스(1162, 1164, 및 1166)가 생물학적 양 출력 시퀀스 생성기(1064)로부터 생성된 2개의 출력 시퀀스(1081 및 1083)와 비교하여 생물학적 양 출력 시퀀스 생성기(1144)로부터 생성된다는 점에서 도 10의 구현(1000)과 상이하다. 도 10에서, 제1 출력 시퀀스(1081)는 진화적 보존의 염기 당 측정일 수 있다. 제2 출력 시퀀스(1083)는 전사 시작의 염기 당 측정일 수 있다. 도 11에서, 제1 출력 시퀀스(1162)는 진화적 보존의 염기 당 측정일 수 있다. 제2 출력 측정(1164)은 전사 개시의 염기 당 측정일 수 있다.The implementation (1100) of FIG. 11 differs from the implementation (1000) of FIG. 10 in that the three output sequences (1162, 1164, and 1166) are generated from the biological quantity output sequence generator (1144) as compared to the two output sequences (1081 and 1083) generated from the biological quantity output sequence generator (1064) . In FIG. 10, the first output sequence (1081) can be a per-base measure of evolutionary conservation . The second output sequence (1083) can be a per-base measure of transcription start . In FIG. 11, the first output sequence (1162) can be a per-base measure of evolutionary conservation . The second output measurement (1164) can be a per-base measure of transcription start. 당업자는 다중 출력 시퀀스가 동시에 예측될 수 있고, 진화 보존의 측정, 나타낸 전사 개시의 측정, 및 염기 당 후생적 신호와 같은 정보의 상이한 순열 및 조합을 나타낼 수 있음을 이해할 것이다.Those skilled in the art will appreciate that multiple output sequences can be predicted simultaneously, representing different permutations and combinations of information such as measures of evolutionary conservation, measures of indicated transcription initiation, and epigenetic signals per base. 도 12는 생물학적 양 모델(124)이 입력 염기 시퀀스(1242)의 대체 표현을 생성하는 제1 가중치 세트(1222) 및 입력 염기 시퀀스(1206)의 대체 표현으로부터 복수의 생물학적 양 출력 시퀀스(1246)를 생성하기 위해 처음부터 훈련되는 제2 가중치 세트(1226)를 포함하는 개시된 기술의 일 구현(1200)의 흐름도이다. 도 10의 구현(1000) 및 도 11의 구현(1100)을 비교하여, 제1 가중치 세트 및 제2 가중치 세트는 구현(1200)에서 종단 간으로 훈련되지 않는다. 도 10의 구현(1000) 및 도 11의 구현(1100)에서 유사하게 수행되는 바와 같이, 구현(1200)을 위한 제1 가중치 세트(1222)는 입력 염기 시퀀스(1242)의 대체 표현을 생성하기 위해 입력 염기 시퀀스(1202)를 처리하는 대체 표현 생성기(1224)를 포함한다. 그러나, 가중치가 종단 간으로 훈련되는 구현과는 대조적으로, 대체 표현 생성기(1224)로부터의 대체 시퀀스 표현 출력(1242)은 복수의 생물학적 양 출력 시퀀스(1246)를 생성하기 위해 제2 가중치 세트(1226)를 훈련하는데 사용되는 생물학적 양 출력 시퀀스 생성기(1228)로서 구성된 후속 모델에 대한 입력(1206)에 매핑된다.FIG. 12 is a flow diagram of an implementation (1200) of the disclosed technique including a first set of weights (1222) for generating alternative representations of an input base sequence (1242) and a second set of weights (1226) trained from scratch to generate a plurality of biological quantity output sequences (1246) from alternative representations of the input base sequence (1206) . Comparing the implementation (1000) of FIG. 10 and the implementation (1100) of FIG. 11 , the first set of weights and the second set of weights are not trained end-to-end in the implementation (1200) . As performed similarly in the implementation (1000) of FIG. 10 and the implementation (1100) of FIG. 11, the first set of weights (1222) for the implementation (1200) includes a surrogate representation generator (1224) that processes the input base sequence (1202) to generate surrogate representations of the input base sequence (1242) . However, in contrast to the implementation where the weights are trained end-to-end, the surrogate sequence representation output (1242) from the surrogate representation generator (1224) is mapped to an input (1206) for a subsequent model configured as a biological quantity output sequence generator (1228) that is used to train a second set of weights (1226) to generate a plurality of biological quantity output sequences (1246). 도 13은, 생물학적 양 모델(124)이 단일 기반으로 후속 생물학적 양 출력 시퀀스를 생성하기 위해 종단 간으로 훈련되고 재훈련되는 제1 가중치 세트(1322) 및 제2 가중치 세트(1342)을 통해 입력 염기 시퀀스(1302)로부터 하나의 생물학적 양 출력 시퀀스(1362)를 생성하는, 개시된 기술의 일 구현(1300)의 흐름도이다. 도 13에 도시된 주어진 예에서, 생물학적 양 모델(124)은 제1 가중치 세트(1322) 및 제2 가중치 세트(1342)에 대해 훈련되어 복수의 생물학적 양 출력 시퀀스(1366)에서 제1 출력 시퀀스(1362)를 생성한다. 제1 출력 시퀀스가 생성된 후, 생물학적 양 모델(124)은 제1 가중치 세트(1324) 및 제2 가중치 세트(1344)에 대해 종단 간으로 재훈련되어 입력 염기 시퀀스(1304)를 처리하고 제2 출력 시퀀스(1364)를 생성한다. 재훈련 프로세스에서, 입력 염기 시퀀스(1304)는 입력 염기 시퀀스(1302)와 동일하고, 제1 가중치 세트(1324)는 제1 가중치 세트(1322)와 동일하며, 제2 가중치 세트(1344)는 제2 가중치 세트(1362)와 동일하다. 그러나, 제2 출력 시퀀스(1364)는 제1 생물학적 양 출력 시퀀스와 상이한 생물학적 양 출력 시퀀스이다. 복수의 생물학적 양 출력 시퀀스(1366) 전체에 대해, 제1 및 제2 가중치를 종단 간으로 재훈련시켜 각각의 후속 생물학적 양 출력 시퀀스를 생성한다.FIG. 13 is a flow diagram of an implementation (1300) of the disclosed technique, wherein a biological quantity model (124) generates a single biological quantity output sequence (1362) from an input base sequence (1302) via a first set of weights (1322) and a second set of weights (1342) that are trained and retrained end-to-end to generate subsequent biological quantity output sequences on a single basis . In the given example illustrated in FIG. 13 , the biological quantity model (124) is trained on the first set of weights (1322) and the second set of weights (1342) to generate a first output sequence (1362) from a plurality of biological quantity output sequences (1366) . After the first output sequence is generated, the biological quantity model (124) is retrained end-to-end for the first weight set (1324) and the second weight set (1344) to process the input base sequence (1304) and generate the second output sequence (1364) . In the retraining process, the input base sequence (1304) is identical to the input base sequence (1302), the first weight set (1324) is identical to the first weight set (1322), and the second weight set (1344) is identical to the second weight set (1362) . However, the second output sequence (1364) is a different biological quantity output sequence than the first biological quantity output sequence . For the entire plurality of biological quantity output sequences (1366), the first and second weights are retrained end-to-end to generate each subsequent biological quantity output sequence. 도 14는, 생물학적 양 모델(124)이 입력 염기 시퀀스로부터 복수의 생물학적 양 출력 시퀀스(1462)을 생성하도록 종단 간으로 훈련되는 제1 가중치 세트(1422) 및 제2 가중치 세트(1442), 및 입력 복수의 생물학적 양 출력 시퀀스(1406)로부터 유전자 발현 출력 시퀀스(1466)를 생성하도록 종단 간으로 훈련되는 제3 및 제4 가중치 세트(1426 및 1446)을 포함하는, 개시된 기술의 일 구현(1400)의 흐름도이다. 입력 염기 시퀀스(1402)로부터의 생물학적 양 출력 시퀀스(1426)의 생성은 도 10의 구현(1000) 및 도 11의 구현(1100)과 유사하며, 여기서 입력 염기 시퀀스(1402)는 대체 표현 생성기(1404)로서 구성된 제1 가중치 세트(1422) 및 생물학적 양 출력 시퀀스 생성기(1444)로서 구성된 제2 가중치 세트(1442)에 의해 처리되어 복수의 생물학적 양 출력 시퀀스(1462)를 생성한다. 이어서, 제2 가중치 세트(1442)의 출력으로서 복수의 출력 시퀀스(1462)는 유전자 발현 출력 시퀀스(1466)를 생성하도록 구성된 후속 모델을 위한 입력(1406)에 매핑된다. 입력 생물학적 양 출력 시퀀스(1406)는 종단 간으로 훈련되는 제3 가중치 세트(1426) 및 제4 가중치 세트(1446)에 의해 처리된다. 제3 가중치 세트(1426)는 생물학적 양 대체 표현 생성기(1428)로서 구성되고, 제4 가중치 세트는 유전자 발현 출력 생성기(1448)로서 구성된다. 생성된 유전자 발현 출력 시퀀스(1466)는 염기 해상도 유전자 발현(1468)의 측정이다.FIG. 14 is a flow diagram of an implementation (1400) of the disclosed technology, comprising a first set of weights (1422) and a second set of weights (1442) that are trained end-to-end to generate a plurality of biological quantity output sequences (1462) from an input base sequence, and a third and fourth sets of weights (1426 and 1446) that are trained end-to-end to generate gene expression output sequences (1466) from an input plurality of biological quantity output sequences (1406) . The generation of biological quantity output sequences (1426) from the input base sequences (1402) is similar to the implementation (1000) of FIG. 10 and the implementation (1100) of FIG. 11, wherein the input base sequences (1402) are processed by a first set of weights (1422) configured as an alternative expression generator (1404) and a second set of weights (1442) configured as a biological quantity output sequence generator (1444) to generate a plurality of biological quantity output sequences (1462) . Subsequently, the plurality of output sequences (1462) as outputs of the second set of weights (1442) are mapped to inputs (1406) for a subsequent model configured to generate gene expression output sequences (1466) . The input biological quantity output sequences (1406) are processed by a third set of weights (1426) and a fourth set of weights (1446) that are trained end-to-end . The third weight set (1426) is configured as a biological quantity surrogate expression generator (1428), and the fourth weight set is configured as a gene expression output generator (1448) . The generated gene expression output sequence (1466) is a measure of base-resolution gene expression (1468). 도 15는, 생물학적 양 모델(124)이 입력 염기 시퀀스(1542)의 대체 표현을 생성하는 제1 가중치 세트(1522), 처음부터 훈련되어 입력 염기 시퀀스(1562)의 대체 표현으로부터 복수의 생물학적 양 출력 시퀀스(1512)을 생성하는 제2 가중치 세트(1582), 및 종단 간으로 훈련되어 입력 복수의 생물학적 양 출력 시퀀스(1506)로부터 유전자 발현 출력 시퀀스(1566)를 생성하는 제3 및 제4 가중치 세트(1124 및 1546)를 포함하는 개시된 기술의 일 구현(1500)의 흐름도이다. 입력 염기 시퀀스(1502)로부터의 생물학적 양 출력 시퀀스(1512)의 생성은 도 12의 구현(1200)과 유사하며, 여기서 생물학적 양 모델(124)은 입력 염기 시퀀스(1542)의 대체 표현을 생성하는 제1 가중치 세트(1522) 및 처음부터 훈련되어 입력 염기 시퀀스(1562)의 대체 표현으로부터 복수의 생물학적 양 출력 시퀀스(1512)를 생성하는 제2 가중치 세트(1582)를 포함한다.FIG. 15 is a flow diagram of an implementation (1500) of the disclosed technology, wherein the biological quantity model (124) includes a first set of weights (1522) that generates alternative representations of an input base sequence (1542), a second set of weights (1582) that are trained from scratch to generate a plurality of biological quantity output sequences (1512) from alternative representations of an input base sequence (1562), and third and fourth sets of weights (1124 and 1546) that are trained end-to-end to generate gene expression output sequences (1566) from an input plurality of biological quantity output sequences (1506) . The generation of biological quantity output sequences (1512) from input base sequences (1502) is similar to the implementation (1200) of FIG. 12, where the biological quantity model (124) includes a first set of weights (1522) that generates alternative representations of the input base sequences (1542) and a second set of weights (1582) that are trained from scratch to generate multiple biological quantity output sequences (1512) from alternative representations of the input base sequences (1562). 구현(도 10의 1000, 도 11의 1100, 도 12의 1200, 도 13의 1300, 및 도 14의 1400)에서와 같이, 제1 가중치 세트(1522)는 대체 표현 생성기(1524)로서 구성되고, 제2 가중치 세트(1582)는 생물학적 양 출력 시퀀스 생성기(1584)로서 구성된다. 이어서, 제2 가중치 세트(1582)의 출력으로서 복수의 출력 시퀀스(1512)는 유전자 발현 출력 시퀀스(1566)를 생성하도록 구성된 후속 모델을 위한 입력(1506)에 매핑된다. 입력 생물학적 양 출력 시퀀스(1506)는 종단 간으로 훈련되는 제3 가중치 세트(1124) 및 제4 가중치 세트(1546)에 의해 처리된다. 제3 가중치 세트(1124)는 생물학적 양 대체 표현 생성기(1528)로서 구성되고, 제4 가중치 세트는 유전자 발현 출력 생성기(1548)로서 구성된다. 생성된 유전자 발현 출력 시퀀스(1566)는 염기 해상도 유전자 발현(1568)의 측정이다.As in the implementations (1000 of FIG. 10, 1100 of FIG. 11, 1200 of FIG. 12, 1300 of FIG. 13, and 1400 of FIG. 14), the first set of weights (1522) is configured as an alternative expression generator (1524), and the second set of weights (1582) is configured as a biological quantity output sequence generator (1584) . Then, the plurality of output sequences (1512) as outputs of the second set of weights (1582) are mapped to inputs (1506) for a subsequent model configured to generate gene expression output sequences (1566) . The input biological quantity output sequences (1506) are processed by the third set of weights (1124) and the fourth set of weights (1546), which are trained end-to-end . The third weight set (1124) is configured as a biological quantity surrogate expression generator (1528), and the fourth weight set is configured as a gene expression output generator (1548) . The generated gene expression output sequence (1566) is a measure of base-resolution gene expression (1568). 도 16은, 생물학적 양 모델(124)이 입력 염기 시퀀스(1642)의 대체 표현을 생성하는 제1 가중치 세트(1622), 처음부터 훈련되어 복수의 생물학적 양 출력 시퀀스(1612)를 생성하는 제2 가중치 세트(1682), 처음부터 훈련되어 복수의 생물학적 양 출력 시퀀스(1606)로부터 대체 생물학적 양 표현(1646)을 생성하는 제3 가중치 세트(1124), 및 처음부터 훈련되어 대체 생물학적 양 표현(1666)으로부터 유전자 발현 출력 시퀀스(1616)를 생성하는 제4 가중치 세트(1686)를 포함하는 개시된 기술의 일 구현(1600)의 흐름도이다. 입력 염기 시퀀스(1602)로부터의 생물학적 양 출력 시퀀스(1612)의 생성은 도 16의 구현(1600)과 유사하며, 여기서 생물학적 양 모델(124)은 입력 염기 시퀀스(1642)의 대체 표현을 생성하는 제1 가중치 세트(1622) 및 처음부터 훈련되어 입력 염기 시퀀스(1662)의 대체 표현으로부터 복수의 생물학적 양 출력 시퀀스(1612)를 생성하는 제2 가중치 세트(1682)를 포함한다.FIG. 16 is a flow diagram of an implementation (1600) of the disclosed technology, wherein a biological quantity model (124) includes a first set of weights (1622) that generates alternative representations of an input base sequence (1642), a second set of weights (1682) that is trained from scratch to generate a plurality of biological quantity output sequences (1612), a third set of weights (1124) that is trained from scratch to generate alternative biological quantity representations (1646) from the plurality of biological quantity output sequences (1606), and a fourth set of weights (1686) that is trained from scratch to generate gene expression output sequences (1616) from the alternative biological quantity representations (1666) . The generation of biological quantity output sequences (1612) from input base sequences (1602) is similar to the implementation (1600) of FIG. 16, where the biological quantity model (124) includes a first set of weights (1622) that generates alternative representations of the input base sequences (1642) and a second set of weights (1682) that are trained from scratch to generate multiple biological quantity output sequences (1612) from alternative representations of the input base sequences (1662). 구현(도 10의 1000, 도 11의 1100, 도 12의 1200, 도 13의 1300, 도 14의 1400, 및 도 15의 1500)에서와 같이, 제1 가중치 세트(1622)는 대체 표현 생성기(1624)로서 구성되고, 제2 가중치 세트(1682)는 생물학적 양 출력 시퀀스 생성기(1684)로서 구성된다. 이어서, 제2 가중치 세트(1682)의 출력으로서 복수의 생물학적 양 출력 시퀀스(1612)는 대체 생물학적 양 표현(1646)을 생성하도록 구성된 후속 모델에 대한 입력(1606)에 매핑된다. 후속 모델은 처음부터 훈련되어 대체 생물학적 양 표현(1646)을 생성하는 제3 가중치 세트(1124)를 포함하는 생물학적 양 대체 표현 생성기(1628)로서 구성된다. 이어서, 제3 가중치 세트(1124)의 출력으로서 대체 생물학적 양 표현 출력(1646)은 유전자 발현 출력 시퀀스(1616)를 생성하기 위해 제2 후속 모델에 대한 입력(1666)에 매핑된다. 제2 후속 모델은 처음부터 훈련되어 유전자 발현 출력 시퀀스(1616)를 생성하는 제4 가중치 세트(1686)를 포함하는 유전자 발현 출력 생성기(1688)로서 구성된다. 생성된 유전자 발현 출력 시퀀스(1616)는 염기 해상도 유전자 발현(1618)의 측정이다.As in the implementations (1000 of FIG. 10, 1100 of FIG. 11, 1200 of FIG. 12, 1300 of FIG. 13, 1400 of FIG. 14, and 1500 of FIG. 15), the first set of weights (1622) is configured as a substitute representation generator (1624), and the second set of weights (1682) is configured as a biological quantity output sequence generator (1684) . Subsequently, the plurality of biological quantity output sequences (1612) as outputs of the second set of weights (1682) are mapped to inputs (1606) to a subsequent model configured to generate substitute biological quantity representations (1646) . The subsequent model is configured as a biological quantity substitute representation generator (1628) including a third set of weights (1124) that is trained from scratch to generate the substitute biological quantity representations (1646) . Next, the alternative biological quantity expression output (1646) as the output of the third set of weights (1124) is mapped to the input (1666) for the second subsequent model to generate the gene expression output sequence (1616) . The second subsequent model is configured as a gene expression output generator (1688) including the fourth set of weights (1686) that is trained from scratch to generate the gene expression output sequence (1616) . The generated gene expression output sequence (1616) is a measure of base-resolution gene expression (1618). 도 17은, 생물학적 양 모델(124)이 종단 간으로 훈련되어 입력 염기 시퀀스(1702)로부터 복수의 생물학적 양 출력 시퀀스(1762)를 생성할 수 있는 제1 가중치 세트(1722) 및 제2 가중치 세트(1742)를 포함하는, 개시된 기술의 일 구현(1700)의 흐름도이다. 제3 가중치 세트(1726)는 처음부터 훈련되어 복수의 생물학적 양 출력 시퀀스(1706)로부터 대체 생물학적 양 표현(1746)을 생성할 수 있다. 제4 가중치 세트(1786)는 처음부터 훈련되어 대체 생물학적 양 표현(1766)으로부터 유전자 발현 출력 시퀀스(1716)를 생성할 수 있다. 주어진 위치에서 주어진 타깃 염기에 대한 유전자 발현 출력 시퀀스(1716)에서 주어진 염기 당 유전자 발현 출력은 주어진 위치에서 주어진 타깃 염기의 유전자 발현 수준의 척도를 특정한다. 일 구현에서, 유전자 발현 수준은 CAGE 전사 시작 부위(CTSS)와 같은 염기 당 메트릭으로 측정된다. 다른 구현에서, 유전자 발현 수준은 유전자 당 메트릭, 예컨대 TPM(transcripts per million) 또는 RPKM(reads per kilobase of transcript)으로 측정된다. 또 다른 구현에서, 유전자 발현 수준은 FPKM(fragments per kilobase million)과 같은 유전자 당 메트릭으로 측정된다.FIG. 17 is a flow diagram of an implementation (1700) of the disclosed technology, wherein a biological quantity model (124) is trained end-to-end to generate a plurality of biological quantity output sequences (1762) from an input base sequence (1702), including a first set of weights (1722) and a second set of weights (1742) . A third set of weights (1726) is trained from scratch to generate alternative biological quantity representations (1746) from the plurality of biological quantity output sequences (1706) . A fourth set of weights (1786) is trained from scratch to generate gene expression output sequences (1716) from the alternative biological quantity representations (1766) . A gene expression output per base in a gene expression output sequence (1716) for a given target base at a given position specifies a measure of the gene expression level of the given target base at the given position. In one implementation, gene expression levels are measured by per-base metrics such as CAGE transcription start sites (CTSS). In other implementations, gene expression levels are measured as per-gene metrics, such as transcripts per million (TPM) or reads per kilobase of transcript (RPKM). In another implementation, gene expression levels are measured as a per-gene metric, such as fragments per kilobase million (FPKM). 입력 염기 시퀀스(1702)로부터의 생물학적 양 출력 시퀀스(1762)의 생성은 도 10의 구현(1000) 및 도 11의 구현(1100)과 유사하며, 여기서 입력 염기 시퀀스(1702)는 대체 표현 생성기(1724)에 의해 적용된 제1 가중치 세트(1722) 및 생물학적 양 출력 시퀀스 생성기(1744)에 의해 적용된 제2 가중치 세트(1742)에 의해 처리되어 복수의 생물학적 양 출력 시퀀스(1762)를 생성할 수 있다.The generation of a biological quantity output sequence (1762) from an input base sequence (1702) is similar to the implementation (1000) of FIG. 10 and the implementation (1100) of FIG. 11, wherein the input base sequence (1702) is processed by a first set of weights (1722) applied by an alternative representation generator (1724) and a second set of weights (1742) applied by a biological quantity output sequence generator (1744) to generate a plurality of biological quantity output sequences (1762). 이어서, 제2 가중치 세트(1742)의 출력으로서 복수의 생물학적 양 출력 시퀀스(1762)는 대체 생물학적 양 표현(1746)을 생성하도록 구성된 후속 모델에 대한 입력(1706)에 매핑될 수 있다. 후속 모델은 처음부터 훈련되어 대체 생물학적 양 표현(1746)을 생성하는 제3 가중치 세트(1726)를 포함하는 생물학적 양 대체 표현 생성기(1728)로서 구성될 수 있다. 이어서, 제3 가중치 세트(1726)의 출력으로서 대체 생물학적 양 표현 출력(1746)은 유전자 발현 출력 시퀀스(1716)를 생성하기 위해 제2 후속 모델에 대한 입력(1766)에 매핑될 수 있다. 제2 후속 모델은 처음부터 훈련되어 유전자 발현 출력 시퀀스(1716)를 생성하는 제4 가중치 세트(1786)를 포함하는 유전자 발현 출력 생성기(1788)로서 구성될 수 있다. 생성된 유전자 발현 출력 시퀀스(1716)는 염기 해상도 유전자 발현(1718)의 측정이다.Subsequently, the plurality of biological quantity output sequences (1762) as outputs of the second set of weights (1742) can be mapped to inputs (1706) to a subsequent model configured to generate alternative biological quantity representations (1746) . The subsequent model can be configured as a biological quantity alternative representation generator (1728) that includes a third set of weights (1726) that is trained from scratch to generate the alternative biological quantity representations (1746) . Subsequently, the alternative biological quantity representation outputs (1746) as outputs of the third set of weights (1726) can be mapped to inputs (1766) to a second subsequent model to generate gene expression output sequences (1716) . The second subsequent model can be configured as a gene expression output generator (1788) that includes a fourth set of weights (1786) that is trained from scratch to generate gene expression output sequences (1716) . The generated gene expression output sequence (1716) is a measure of base-resolution gene expression (1718). 생물학적 양 모델에 대한 전송 학습의 적용Application of transfer learning to biological quantity models 도 18은, 생물학적 양 모델(124)이 입력 염기 시퀀스(1802)로부터 대체 시퀀스 표현(1822)을 생성하도록 훈련된 후, 제3 가중치 세트(1842)의 치환으로 재훈련되어 복수의 생물학적 양 출력 시퀀스(1832)로부터 대체 생물학적 양 표현을 생성하는 제1 가중치 세트(1812), 및 치환된 제1 가중치 세트로 종단 간에 훈련되어 생물학적 양 출력 시퀀스(1832)로부터 유전자 발현 출력 시퀀스(1862)를 생성하는 제4 가중치 세트(1852)를 포함하는, 개시된 기술의 일 구현(1800)의 흐름도이다. 대체 표현 생성기(1813)로부터 학습되는 가중치 세트 1(1812)에 대한 최적화된 가중치 스칼라 값은 제3 가중치 세트(1842) 내의 각각의 가중치에 대한 치환 스칼라 값으로서 전송될 수 있다. 도 14의 구현(1400) 및 도 15의 구현(1500)에서와 같이, 생물학적 양 대체 표현 생성기(1843)는 유전자 발현 출력 생성기(1863)를 포함하는 제4 가중치 세트(1852)로 종단 간으로 훈련되는 제3 가중치 세트(1842)를 포함한다. 생성된 유전자 발현 출력 시퀀스(1862)는 염기 해상도 유전자 발현(1863)의 측정이다.FIG. 18 is a flow diagram of an implementation (1800) of the disclosed technology, including a first set of weights (1812) that is trained to generate alternative sequence representations (1822) from input base sequences (1802), and then retrained with permutations of a third set of weights (1842) to generate alternative biological quantity representations from multiple biological quantity output sequences (1832), and a fourth set of weights (1852) that is trained end-to-end with the permuted first set of weights to generate gene expression output sequences (1862) from the biological quantity output sequences (1832) . The optimized weight scalar values for weight set 1 (1812) learned from the alternative representation generator (1813) can be transmitted as permutation scalar values for each weight in the third set of weights (1842) . As in the implementation (1400) of FIG. 14 and the implementation (1500) of FIG. 15, the biological quantity surrogate expression generator (1843) includes a third set of weights (1842) that is trained end-to-end with a fourth set of weights (1852) that includes a gene expression output generator (1863) . The generated gene expression output sequence (1862) is a measure of base-resolution gene expression (1863). 도 19는, 생물학적 양 모델(124)이 입력 염기 시퀀스(1902)로부터 대체 시퀀스 표현(1922)을 생성하도록 훈련된 다음, 제3 가중치 세트(1932)의 치환으로서 재훈련되어 대체 시퀀스 표현(1922)으로부터 대체 생물학적 양 표현(1942)을 생성하는 제1 가중치 세트(1912), 및 처음부터 훈련되어 대체 생물학적 양 표현(1952)으로부터 유전자 발현 출력 시퀀스(1972)를 생성하는 제4 가중치 세트(1962)를 포함하는, 개시된 기술의 일 구현(1900)의 흐름도이다. 대체 표현 생성기(1913)로부터 학습되는 가중치 세트 1(1912)에 대한 최적화된 가중치 스칼라 값은 제3 가중치 세트(1932) 내의 각각의 가중치에 대한 치환 스칼라 값으로서 전송될 수 있다.FIG. 19 is a flow diagram of an implementation (1900) of the disclosed technology, wherein a biological quantity model (124) is trained to generate alternative sequence representations (1922) from input base sequences (1902), and is then retrained as a permutation of a third set of weights (1932) to generate alternative biological quantity representations (1942) from the alternative sequence representations (1922), and a fourth set of weights (1962) that is trained from scratch to generate gene expression output sequences (1972) from the alternative biological quantity representations (1952). The optimized weight scalar values for the weight set 1 (1912) learned from the alternative representation generator (1913) can be transmitted as alternative scalar values for each weight in the third weight set (1932). 이어서, 제3 가중치 세트(1932)의 출력으로서 대체 생물학적 양 표현 출력(1942)은 유전자 발현 출력 시퀀스(1716)를 생성하기 위해 후속 모델에 대한 입력(1952)에 매핑된다. 도 16의 구현(1600) 및 도 17의 구현(1700)에서와 같이, 후속 모델은 처음부터 훈련되어 유전자 발현 출력 시퀀스(1972)을 생성하는 제4 가중치 세트(1962)를 포함하는 유전자 발현 출력 생성기(1963)로서 구성된다. 생성된 유전자 발현 출력 시퀀스(1972)는 염기 해상도 유전자 발현(1973)의 측정이다.Next, the alternative biological quantity expression output (1942) as the output of the third set of weights (1932) is mapped to the input (1952) for the subsequent model to generate the gene expression output sequence (1716) . As in the implementation (1600) of FIG. 16 and the implementation (1700) of FIG. 17, the subsequent model is trained from scratch and consists of a gene expression output generator (1963) including a fourth set of weights (1962) that generates the gene expression output sequence (1972) . The generated gene expression output sequence (1972) is a measure of base-resolution gene expression (1973). 도 20은, 생물학적 양 모델(124)이 입력 염기 시퀀스(2042)로부터 대체 시퀀스 표현(2062)을 생성하도록 훈련된 제1 가중치 세트(2052), 복수의 생물학적 양 출력 시퀀스(2014)를 생성하도록 훈련된 제2 가중치 세트(2004), 복수의 생물학적 양 출력 시퀀스로부터 대체 생물학적 양 표현(2044)을 생성하기 위해 제3 가중치 세트(2034)의 치환으로서 사용되는 재훈련된 제1 가중치 세트(2052), 및 처음부터 훈련되어 대체 생물학적 양 표현(2054)으로부터 유전자 발현 출력 시퀀스(2074)를 생성하는 제4 가중치 세트(2064)를 포함하는, 개시된 기술의 일 구현(2000)의 흐름도이다. 도 19의 구현(1900)에서와 같이, 대체 표현 생성기(2053)로부터 학습되는 가중치 세트 1(2052)에 대한 최적화된 가중치 스칼라 값은 제3 가중치 세트(2034) 내의 각각의 가중치에 대한 치환 스칼라 값으로서 전송될 수 있다.FIG. 20 is a flow diagram of an implementation (2000) of the disclosed technology, wherein a biological quantity model (124) comprises a first set of weights (2052) trained to generate alternative sequence representations (2062) from input base sequences (2042), a second set of weights (2004) trained to generate a plurality of biological quantity output sequences (2014), a retrained first set of weights (2052) used as a replacement for a third set of weights (2034) to generate alternative biological quantity representations (2044) from the plurality of biological quantity output sequences, and a fourth set of weights (2064) trained from scratch to generate gene expression output sequences (2074) from the alternative biological quantity representations (2054) . As in the implementation (1900) of Fig. 19, the optimized weight scalar values for the weight set 1 (2052) learned from the alternative representation generator (2053) can be transmitted as alternative scalar values for each weight in the third weight set (2034). 제2 가중치 세트(2004)는 복수의 생물학적 양 출력 시퀀스(2014)를 생성하도록 구성된다. 도 14의 구현(1400)에서와 같이, 생물학적 양 출력 시퀀스(2014)의 도 15의 구현(1500), 도 16의 구현(1600), 및 도 17의 구현(1700)은 제3 가중치 세트(2034)를 포함하는 생물학적 양 대체 표현 생성기(2035)로서 구성된 후속 모델의 입력에 매핑된다. 구현(2000)에서, 제3 가중치 세트(2034)에 대한 스칼라 값은 훈련된 제1 가중치 세트(2052)로부터의 최적화된 스칼라 값으로부터 치환된다. 도 16의 구현(1600) 및 도 17의 구현(1700)에서와 같이, 제3 가중치 세트(2034)의 출력으로서 대체 생물학적 양 표현 출력(2044)은 유전자 발현 출력 시퀀스(2074)를 생성하기 위해 후속 모델에 대한 입력(2044)에 매핑된다. 후속 모델은 처음부터 훈련되어 유전자 발현 출력 시퀀스(2074)를 생성하는 제4 가중치 세트(2064)를 포함하는 유전자 발현 출력 생성기(2065)로서 구성된다. 생성된 유전자 발현 출력 시퀀스(2074)는 염기 해상도 유전자 발현(2075)의 측정이다.The second set of weights (2004) is configured to generate a plurality of biological quantity output sequences (2014) . As in the implementation (1400) of FIG. 14, the biological quantity output sequences (2014) of FIG. 15, FIG. 16, FIG. 17, and FIG. 18 are mapped to inputs of a subsequent model configured as a biological quantity surrogate representation generator (2035) including the third set of weights (2034) . In the implementation (2000), the scalar values for the third set of weights (2034) are substituted from the optimized scalar values from the trained first set of weights (2052) . As in the implementation (1600) of FIG. 16 and the implementation (1700) of FIG. 17, the alternative biological quantity expression output (2044) as the output of the third set of weights (2034) is mapped to the input (2044) for the subsequent model to generate the gene expression output sequence (2074) . The subsequent model is configured as a gene expression output generator (2065) including the fourth set of weights (2064) trained from scratch to generate the gene expression output sequence (2074) . The generated gene expression output sequence (2074) is a measure of base-resolution gene expression (2075). 도 21은, 생물학적 양 모델(124)이 입력 염기 시퀀스(2132)로부터 대체 시퀀스 표현(2152)을 생성하도록 훈련되는 제1 가중치 세트(2142), 입력 염기의 대체 표현으로부터 복수의 생물학적 양 출력 시퀀스(2114)를 생성하도록 훈련되는 제2 가중치 세트(2104), 복수의 생물학적 양 출력 시퀀스(2124)로부터 대체 생물학적 양 표현을 생성하기 위해 제3 가중치 세트(2134)의 치환으로서 사용되는 재훈련된 제1 가중치 세트(2142), 및 치환된 제1 가중치 세트(2142)로 종단 간으로 훈련되어 대체 생물학적 양 표현으로부터 유전자 발현 출력 시퀀스(2154)를 생성하는 제4 가중치 세트(2144)를 포함하는, 개시된 기술의 일 구현(2100)의 흐름도이다. 도 19로부터의 구현(1900) 및 도 20으로부터의 구현(2000)에서와 같이, 대체 표현 생성기(2152)로부터 학습되는 가중치 세트 1(2142)에 대한 최적화된 가중치 스칼라 값은 제3 가중치 세트(2134) 내의 각각의 가중치에 대한 대체 스칼라 값으로서 전송될 수 있다.FIG. 21 is a flow diagram of an implementation (2100) of the disclosed technology, wherein a biological quantity model (124) comprises a first set of weights (2142) trained to generate alternative sequence representations (2152) from input base sequences (2132), a second set of weights (2104) trained to generate a plurality of biological quantity output sequences (2114) from the alternative representations of the input bases, a retrained first set of weights (2142) used as a permutation of a third set of weights (2134) to generate alternative biological quantity representations from the plurality of biological quantity output sequences (2124), and a fourth set of weights (2144) trained end-to-end with the permuted first set of weights (2142) to generate gene expression output sequences (2154) from the alternative biological quantity representations . As in the implementation (1900) from Fig. 19 and the implementation (2000) from Fig. 20, the optimized weight scalar values for the weight set 1 (2142) learned from the surrogate representation generator (2152) can be transmitted as surrogate scalar values for each weight in the third weight set (2134). 도 20의 구현(2000)에서와 같이, 제2 가중치 세트(2104)는 복수의 생물학적 양 출력 시퀀스(2114)를 생성하도록 구성된다. 도 14의 구현(1400)에서와 같이, 생물학적 양 출력 시퀀스(2114)의 도 15의 구현(1500), 도 16의 구현(1600), 및 도 17의 구현(1700)은 제3 가중치 세트(2134)를 포함하는 생물학적 양 대체 표현 생성기(2135)로서 구성된 후속 모델의 입력에 매핑된다. 도 20으로부터의 구현(2000)에서와 같이, 제3 가중치 세트(2134)에 대한 스칼라 값은 훈련된 제1 가중치 세트(2142)로부터의 최적화된 스칼라 값으로부터 치환된다. 도 14의 구현(1400), 도 15의 구현(1500), 및 도 18의 구현(1800)에서와 같이, 생물학적 양 대체 표현 생성기(2135)는 유전자 발현 출력 생성기(2145)를 포함하는 제4 가중치 세트(2144)로 종단 간으로 훈련되는 제3 가중치 세트(2134)를 포함한다. 생성된 유전자 발현 출력 시퀀스(2154)는 염기 해상도 유전자 발현(2155)의 측정이다.As in the implementation (2000) of FIG. 20, the second set of weights (2104) is configured to generate a plurality of biological quantity output sequences (2114) . As in the implementation (1400) of FIG. 14, the biological quantity output sequences (2114) of the implementations (1500) of FIG. 15, (1600) of FIG. 16, and (1700) of FIG. 17 are mapped to the input of a subsequent model configured as a biological quantity surrogate representation generator (2135) including the third set of weights (2134) . As in the implementation (2000) from FIG. 20, the scalar values for the third set of weights (2134) are substituted from the optimized scalar values from the trained first set of weights (2142) . As in the implementation (1400) of FIG. 14, the implementation (1500) of FIG. 15, and the implementation (1800) of FIG. 18, the biological quantity surrogate expression generator (2135) includes a third set of weights (2134) that is trained end-to-end with a fourth set of weights (2144) that includes a gene expression output generator (2145) . The generated gene expression output sequence (2154) is a measure of base-resolution gene expression (2155). 염기 해상도에서 병원성 예측Pathogenicity prediction at base resolution 도 22는, 생물학적 양 모델(124)이 염기 해상도에서 기준 생물학적 양 출력 시퀀스(2253)와 대체 생물학적 양 출력 시퀀스(2256)를 비교하는 병원성 예측 로직을 추가로 포함하도록 구성된, 개시된 기술의 일 구현(2200)의 흐름도이며, 여기서 제5 가중치 세트(2223)는 복수의 생물학적 양 출력 시퀀스(2253 및 2256)로부터 대체 시퀀스 병원성 예측(2233)을 생성하고, 제1 가중치 세트(2212 및 2214) 및 제2 가중치 세트(2242 및 2244)는 각각 처음부터 훈련된다. 도 12의 구현(1200), 도 15의 구현(1500), 및 도 16의 구현(1600)에서와 같이, 구현(2200)을 위한 제1 가중치 세트(2212)는 기준 입력 염기 시퀀스(2202)의 대체 표현(2222)을 생성하기 위해 기준 입력 염기 시퀀스(2202)를 처리하는 대체 표현 생성기(2213)를 포함한다. 대체 표현 생성기(2213)로부터의 대체 시퀀스 표현 출력(2222)은 복수의 생물학적 양 출력 시퀀스(2253)를 생성하기 위해 제2 가중치 세트(2242)를 훈련하는데 사용되는 생물학적 양 출력 시퀀스 생성기(2243)로서 구성된 후속 모델에 대한 입력(2232)에 매핑된다.FIG. 22 is a flow diagram of an implementation (2200) of the disclosed technology, wherein the biological quantity model (124) is further configured to include pathogenicity prediction logic that compares a reference biological quantity output sequence (2253) to a surrogate biological quantity output sequence (2256) at base resolution, wherein the fifth set of weights (2223) generates surrogate sequence pathogenicity predictions (2233) from the plurality of biological quantity output sequences (2253 and 2256), and the first set of weights (2212 and 2214) and the second set of weights (2242 and 2244) are each trained from scratch . As in the implementation (1200) of FIG. 12, the implementation (1500) of FIG. 15, and the implementation (1600) of FIG. 16, the first set of weights (2212) for the implementation (2200) includes a surrogate representation generator (2213) that processes the reference input base sequence (2202) to generate surrogate representations (2222) of the reference input base sequence . The surrogate sequence representation output (2222) from the surrogate representation generator (2213) is mapped to an input (2232) for a subsequent model configured as a biological quantity output sequence generator (2243) that is used to train a second set of weights (2242) to generate a plurality of biological quantity output sequences (2253). 동시에, 구현(2200)을 위한 제1 가중치 세트(2214)는 대체 입력 염기 시퀀스(2204)를 처리하여 대체 입력 염기 시퀀스(2204)의 대체 표현(2224)을 생성하는 대체 표현 생성기(2216)를 포함한다. 대체 표현 생성기(2216)로부터의 대체 시퀀스 표현 출력(2224)은 복수의 생물학적 양 출력 시퀀스(2256)를 생성하기 위해 제2 가중치 세트(2244)를 훈련하는데 사용되는 생물학적 양 출력 시퀀스 생성기(2246)로서 구성된 후속 모델에 대한 입력(2234)에 매핑된다. 기준 시퀀스(2202) 및 대체 시퀀스(2204)에 대한 각각의 생물학적 양 모델(124)에 대한 가중치의 최적화된 스칼라 값은 염기 해상도 대체 시퀀스 병원성 예측(2233)을 생성하는 제5 가중치 세트(2223)로 전송될 수 있다.Concurrently, the first set of weights (2214) for the implementation (2200) includes a substitute representation generator (2216) that processes the substitute input base sequence (2204) to generate substitute representations (2224) of the substitute input base sequence (2204) . The substitute sequence representation output (2224) from the substitute representation generator (2216) is mapped to an input (2234) for a subsequent model configured as a biological quantity output sequence generator (2246) that is used to train the second set of weights (2244) to generate a plurality of biological quantity output sequences (2256) . The optimized scalar values of the weights for each biological quantity model (124) for the reference sequence (2202) and the substitute sequence (2204) can be transmitted to a fifth set of weights (2223) that generates base-resolution substitute sequence pathogenicity predictions (2233). 일 구현에서, 병원성 예측은 0과 1 사이의 점수일 수 있으며, 여기서 0은 절대 양성을 나타내고 1은 절대 병원성을 나타낸다. 다른 구현에서, 컷오프를 사용할 수 있으며, 예를 들어, 병원성 점수가 5보다 크면 병원성으로 간주하고, 5보다 작으면 양성으로 간주할 수 있다.In one implementation, the pathogenicity prediction can be a score between 0 and 1, where 0 represents absolute positivity and 1 represents absolute pathogenicity. In other implementations, a cutoff could be used, for example, if the pathogenicity score is greater than 5, it is considered pathogenic, and if it is less than 5, it is considered benign. 도 23는, 생물학적 양 모델(124)이 염기 해상도에서 기준 생물학적 양 출력 시퀀스(2362)와 대체 생물학적 양 출력 시퀀스(2364)를 비교하는 병원성 예측 로직을 추가로 포함하도록 구성된, 개시된 기술의 일 구현(2300)의 흐름도이며, 여기서 제5 가중치 세트(2353)는 복수의 생물학적 양 출력 시퀀스(2362 및 2364)로부터 대체 시퀀스 병원성 예측(2363)을 생성하고, 제1 가중치 세트(2322 및 2324) 및 제2 가중치 세트(2342 및 2344)는 종단 간으로 훈련된다. 도 11의 구현(1100), 도 12의 구현(1200), 도 13의 구현(1300), 도 14의 구현(1400), 및 도 17의 구현(1700)에서와 같이, 기준 입력 염기 시퀀스(2302)는 대체 표현 생성기(2323)로서 구성된 제1 가중치 세트(2322) 및 생물학적 양 출력 시퀀스 생성기(2343)로서 구성된 제2 가중치 세트(2342)에 의해 처리되어 복수의 생물학적 양 출력 시퀀스(2362)를 생성한다. 제1 가중치 세트(2322) 및 제2 가중치 세트(2342)는 종단 간으로 훈련된다.FIG. 23 is a flow diagram of an implementation (2300) of the disclosed technology, wherein the biological quantity model (124) is further configured to include pathogenicity prediction logic that compares a reference biological quantity output sequence (2362) to a surrogate biological quantity output sequence (2364) at base resolution, wherein the fifth set of weights (2353) generates surrogate sequence pathogenicity predictions (2363) from the plurality of biological quantity output sequences (2362 and 2364), and the first set of weights (2322 and 2324) and the second set of weights (2342 and 2344) are trained end-to-end . As in the implementation (1100) of FIG. 11, the implementation (1200) of FIG. 12, the implementation (1300) of FIG. 13, the implementation (1400) of FIG. 14, and the implementation (1700) of FIG. 17, the reference input base sequence (2302) is processed by a first set of weights (2322) configured as an alternative representation generator (2323) and a second set of weights (2342) configured as a biological quantity output sequence generator (2343) to generate a plurality of biological quantity output sequences (2362) . The first set of weights (2322) and the second set of weights (2342) are trained end-to-end. 동시에, 제1 가중치 세트(2324)는 대체 입력 염기 시퀀스(2304)를 처리하여 대체 표현 생성기(2326)를 포함하고, 제2 가중치 세트(2344)는 생물학적 양 출력 시퀀스 생성기(2346)를 포함하여, 복수의 생물학적 양 출력 시퀀스(2362)가 기준 입력 염기 시퀀스(2302)로부터 생성되는 것과 동일한 방식으로 복수의 생물학적 양 출력 시퀀스(2364)를 생성한다. 기준 시퀀스(2302) 및 대체 시퀀스(2304)에 대한 각각의 생물학적 양 모델(124)에 대한 가중치의 최적화된 스칼라 값은 염기 해상도 대체 시퀀스 병원성 예측(2363)을 생성하는 제5 가중치 세트(2353)로 전송될 수 있다.Concurrently, the first set of weights (2324) includes a replacement representation generator (2326) that processes the replacement input base sequence (2304), and the second set of weights (2344) includes a biological quantity output sequence generator (2346) that generates a plurality of biological quantity output sequences (2364) in the same manner as the plurality of biological quantity output sequences (2362) are generated from the reference input base sequence (2302) . The optimized scalar values of the weights for each biological quantity model (124) for the reference sequence (2302) and the replacement sequence (2304) can be transmitted to the fifth set of weights (2353) that generates base-resolution replacement sequence pathogenicity predictions (2363). 생물학적 양 출력 시퀀스 데이터Biological quantity output sequence data 도 24는 제1 생물학적 양 출력 시퀀스에 대한 값으로 입력 염기 시퀀스로부터의 생물학적 양 모델(124)로부터 생성될 수 있는 진화 보존의 측정(2400)의 개략도이다. 입력 염기 시퀀스가 대체 표현 생성기(2401)에 의해 처리된 후에, 생물학적 양 출력 시퀀스 생성기(2402)는 phyloP 점수(2403)의 형태로 제1 생물학적 양 출력 시퀀스를 생성할 수 있다. 입력 염기 시퀀스가 대체 표현 생성기(2404)에 의해 처리된 후에, 생물학적 양 출력 시퀀스 생성기(2405)는 phastCons 값(2406)의 형태로 제1 생물학적 양 출력 시퀀스를 생성할 수 있다. 입력 염기 시퀀스가 대체 표현 생성기(2407)에 의해 처리된 후에, 생물학적 양 출력 시퀀스 생성기(2408)는 phastCons 값(2409)의 형태로 제1 생물학적 양 출력 시퀀스를 생성할 수 있다.FIG. 24 is a schematic diagram of a measure of evolutionary conservation (2400) that can be generated from a biological quantity model (124) from an input base sequence as a value for a first biological quantity output sequence . After an input base sequence is processed by an alternative representation generator (2401), a biological quantity output sequence generator (2402) can generate a first biological quantity output sequence in the form of a phyloP score (2403) . After an input base sequence is processed by an alternative representation generator (2404), a biological quantity output sequence generator (2405) can generate a first biological quantity output sequence in the form of a phastCons value (2406) . After an input base sequence is processed by an alternative representation generator (2407), a biological quantity output sequence generator (2408) can generate a first biological quantity output sequence in the form of a phastCons value (2409). 도 25는 제2 생물학적 양 출력 시퀀스에 대한 값으로 입력 염기 시퀀스로부터의 생물학적 양 모델(124)로부터 생성될 수 있는 전사 개시 측정(2500)의 개략도이다. 입력 염기 시퀀스가 대체 표현 생성기(2502)에 의해 처리된 후, 생물학적 양 출력 시퀀스 생성기(2504)는 유전자 발현(CAGE) 값(2506)의 캡 분석의 형태로 제2 생물학적 양 출력 시퀀스를 생성할 수 있다.FIG. 25 is a schematic diagram of a transcription initiation measurement (2500) that can be generated from a biological quantity model (124) from an input base sequence as a value for a second biological quantity output sequence . After the input base sequence is processed by the alternative expression generator (2502), the biological quantity output sequence generator (2504) can generate a second biological quantity output sequence in the form of a cap analysis of gene expression (CAGE) value (2506). 도 26은 제3 생물학적 양 출력 시퀀스에 대한 값으로 입력 염기 시퀀스로부터의 생물학적 양 모델(124)로부터 생성될 수 있는 후생적 신호(2600)의 개략도이다. 입력 염기 시퀀스가 대체 표현 생성기(2601)에 의해 처리된 후, 생물학적 양 출력 시퀀스 생성기(2602)는 DNase I-과민성 부위 예측 출력(2603)의 형태로 제1 생물학적 양 출력 시퀀스를 생성할 수 있다. 입력 염기 시퀀스가 대체 표현 생성기(2604)에 의해 처리된 후에, 생물학적 양 출력 시퀀스 생성기(2605)는 전사 인자 결합 부위 예측 출력(2606)의 형태로 제1 생물학적 양 출력 시퀀스를 생성할 수 있다. 입력 염기 시퀀스가 대체 표현 생성기(2607)에 의해 처리된 후에, 생물학적 양 출력 시퀀스 생성기(2608)는 히스톤 수정 마크 예측 출력(2609)의 형태로 제1 생물학적 양 출력 시퀀스를 생성할 수 있다.FIG. 26 is a schematic diagram of an epigenetic signal (2600) that can be generated from a biological quantity model (124) from an input base sequence as a value for a third biological quantity output sequence . After an input base sequence is processed by an alternative representation generator (2601), a biological quantity output sequence generator (2602) can generate a first biological quantity output sequence in the form of a DNase I-hypersensitive site prediction output (2603) . After an input base sequence is processed by an alternative representation generator (2604), a biological quantity output sequence generator (2605) can generate a first biological quantity output sequence in the form of a transcription factor binding site prediction output (2606) . After an input base sequence is processed by an alternative representation generator (2607), a biological quantity output sequence generator (2608) can generate a first biological quantity output sequence in the form of a histone modification mark prediction output (2609). 유전자 발현 분류 모델Gene expression classification model 도 27은, 발현 변경 분류기(2700)가 유전자 발현에 대한 변이체의 효과를 예측하도록 구성되는, 개시된 기술의 일 구현의 흐름도이다. 변이체 발현-보존 인과성 점수 검증 데이터세트(2702)를 사용하여 지정된 결정 임계 값(예: 0.01, 0.0001 또는 1e-14 미만의 p-값 컷오프)를 갖는 이진 분류 모델(2704)에 대한 변이체 세트의 기준 진리 분기점을 생성하며, 이를 사용하여 변이체를 유전자 발현 변경 클래스(2724)(즉, 유전자 발현을 변경하는 변이체) 또는 유전자 발현 보존 클래스(2744)(즉, 유전자 발현을 변경하지 않는 변이체)로 분류할 수 있다. 변이체의 분류는 유전자 발현을 변경시킬 수 있는 통계적으로 교란되지 않는 가능성을 명시하는 할당된 인과성 점수로부터 학습된다.FIG. 27 is a flow diagram of one implementation of the disclosed technology, wherein an expression alteration classifier (2700) is configured to predict the effect of a variant on gene expression . A ground truth branch point for a set of variants is generated for a binary classification model (2704) having a specified decision threshold (e.g., a p-value cutoff of less than 0.01, 0.0001, or 1e-14) using a variant expression-conserving causality score validation dataset (2702), which can be used to classify the variants into a gene expression altering class (2724) (i.e., variants that alter gene expression) or a gene expression conserving class (2744) (i.e., variants that do not alter gene expression). The classification of a variant is learned from its assigned causality score, which specifies its statistically unconfounded probability of altering gene expression. 도 28은, 발현 변경 분류기는 변이체가 유전자 발현을 감소시키는지 또는 유전자 발현을 감소시키지 않는지를 예측하기 위해 발현 감소 분류기(2800)로서 추가로 구성되는, 개시된 기술의 일 구현의 흐름도이다. 변이체 과소 발현 인과성 점수 검증 세트를 사용하여, 지정된 결정 임계 값(예: 0.01, 0.0001, 또는 1e-14 미만의 p-값 컷오프)을 갖는 이진 분류 모델(2804)에 대한 변이체 세트의 과소 발현 기준 진리 분기점을 생성하며, 이를 사용하여FIG. 28 is a flow diagram of one implementation of the disclosed technology, wherein the expression change classifier is further configured as a decreased expression classifier (2800) to predict whether a variant decreases gene expression or does not decrease gene expression . Using the variant under-expression causality score validation set, a truth branch point for the under-expression criterion of a set of variants for a binary classification model (2804) having a specified decision threshold (e.g., a p-value cutoff of less than 0.01, 0.0001, or 1e-14) is generated, using the 변이체를 유전자 발현 감소 클래스(2824)(즉, 유전자 발현을 줄이는 변이체) 또는 유전자 발현 비감소 클래스(2844)(즉, 유전자 발현을 줄이지 않는 변이체)로 분류할 수 있다. 변이체의 분류는 유전자 발현을 줄일 수 있는 통계적으로 교란되지 않는 가능성을 명시하는 할당된 과소 발현 인과성 점수로부터 학습된다.Variants can be classified into a gene expression reduction class (2824) (i.e., variants that reduce gene expression) or a gene expression non-reduction class (2844) (i.e., variants that do not reduce gene expression). The classification of a variant is learned from its assigned underexpression causality score, which specifies a statistically unconfounded probability that it can reduce gene expression. 도 29는, 변이체가 유전자 발현을 증가시키는지 또는 유전자 발현을 증가시키지 않는지를 예측하기 위해 발현 변경 분류기가 발현 증가 분류기(2900)로서 추가로 구성되는, 개시된 기술의 일 구현의 흐름도이다. 변이체 과다 발현 인과성 점수 검증 데이터세트(2902)를 사용하여, 지정된 결정 임계 값(예: 0.01, 0.0001, 또는 1e-14 미만의 p-값 컷오프)을 갖는 이진 분류 모델(2904)에 대한 변이체 세트의 과다 발현 기준 진리 분기점을 생성하며, 이를 사용하여FIG. 29 is a flow diagram of one implementation of the disclosed technology, wherein the expression change classifier is further configured as an expression increase classifier (2900) to predict whether a variant increases gene expression or does not increase gene expression . Using a variant over-expression causality score validation dataset (2902), a truth breakpoint for the over-expression of a set of variants is generated for a binary classification model (2904) having a specified decision threshold (e.g., a p-value cutoff of less than 0.01, 0.0001, or 1e-14), and using the 변이체를 유전자 발현 감소 클래스(2924) 또는 유전자 발현 비감소 클래스(2944)로 분류할 수 있다. 변이체의 분류는 유전자 발현을 늘릴 수 있는 통계적으로 교란되지 않는 가능성을 명시하는 할당된 과다 발현 인과성 점수로부터 학습된다.Mutants can be classified into a reduced gene expression class (2924) or a non-reduced gene expression class (2944). The classification of a variant is learned from its assigned overexpression causality score, which specifies a statistically unconfounded probability that it can increase gene expression. 도 30은, 발현 변경 분류기가 변이체가 유전자 발현을 보존하고, 유전자 발현을 감소시키거나, 또는 유전자 발현을 증가시킬지를 예측하는 다중-클래스 발현 분류기(3000)로 추가로 구성되는, 개시된 기술의 일 구현의 흐름도이다. 변이체 데이터세트는 시스템(3000)에 의해 처리되며, 여기서 제1 성능 측정치는 유전자 발현 변경 인과성 점수(3024)로부터 유전자 발현 변경 클래스(3028) 또는 유전자 발현 보존 클래스(3038)에 해당하는 출력을 생성하는 결정 임계치를 갖는 이진 분류기(3026)를 통한 기준 진리 분기점에 대한 추론된 분기점의 비교를 위해 생성되고, 제2 성능 측정치는 유전자 발현 감소 인과성 점수(3044)로부터 유전자 발현 감소 클래스(3048) 또는 유전자 발현 비감소 클래스(3058)에 해당하는 출력을 생성하는 결정 임계치를 갖는 이진 분류기(3046)를 통한 기준 진리 분기점에 대한 추론된 분기점의 비교를 위해 생성되고, 제3 성능 측정치는 유전자 발현 증가 인과성 점수(3064)로부터 유전자 발현 증가 클래스(3068) 또는 유전자 발현 비증가 클래스(3078)에 해당하는 출력을 생성하는 결정 임계치를 갖는 이진 분류기(3066)를 통한 기준 진리 분기점에 대한 추론된 분기점의 비교를 위해 생성된다.FIG. 30 is a flow diagram of one implementation of the disclosed technology, wherein the expression change classifier further comprises a multi-class expression classifier (3000) that predicts whether a variant will preserve gene expression, decrease gene expression, or increase gene expression . The variant dataset is processed by the system (3000), wherein a first performance measure is generated for comparison of the inferred branch point to a ground truth branch point through a binary classifier (3026) having a decision threshold that produces output corresponding to a gene expression altered class (3028) or a gene expression preserved class (3038) from a gene expression altered causality score (3024), a second performance measure is generated for comparison of the inferred branch point to a ground truth branch point through a binary classifier (3046) having a decision threshold that produces output corresponding to a gene expression decreased class (3048) or a gene expression non-decreased class (3058) from a gene expression decreased causality score (3044), and a third performance measure is generated for comparison of the inferred branch point to a ground truth branch point through a binary classifier (3066) having a decision threshold that produces output corresponding to a gene expression increased class (3068) or a gene expression non-increased class (3078) from a gene expression increased causality score (3064). Generated for comparison of inferred branch points. 시스템(3000)은 유전자 변경 클래스에서 동일한 수의 변이체를 분류하기 위해 제1, 제2, 및 제3 추론된 분기점을 필요하도록 구성되어 있으므로, 제1, 제2, 및 제3 성능 측정치를 서로 비교할 수 있게 한다. 시스템(3000)은 제1, 제2, 및 제3 성능 측정치의 비교에 기초하여 검증 데이터(3002)에 대한 결정 임계치를 갖는 제1 이진 분류기(3026), 제2 이진 분류기(3046), 및 제3 이진 분류기(3066)의 각각의 성능을 비교하도록 추가로 구성된다. 결정 임계치를 갖는 제1 이진 분류기(3026), 제2 이진 분류기(3046), 및 제3 이진 분류기(3066)에 대한 결정 임계치는 동일할 수 있다. 결정 임계치를 갖는 제1 이진 분류기(3026), 제2 이진 분류기(3046), 및 제3 이진 분류기(3066)에 대한 결정 임계치는 상이할 수 있다.The system (3000) is configured to require the first, second, and third inferred branch points to classify an equal number of variants in a genetic alteration class, thereby allowing the first, second, and third performance measures to be compared with each other . The system (3000) is further configured to compare the performance of each of the first binary classifier (3026), the second binary classifier (3046), and the third binary classifier (3066) having decision thresholds on the validation data (3002) based on the comparison of the first, second, and third performance measures . The decision thresholds for the first binary classifier (3026), the second binary classifier (3046), and the third binary classifier (3066) having decision thresholds can be the same . The decision thresholds for the first binary classifier (3026), the second binary classifier (3046), and the third binary classifier (3066) having decision thresholds may be different. 시스템(3000)은 결정 임계치를 갖는 제1 이진 분류기(3026), 제2 이진 분류기(3046), 및 제3 이진 분류기(3066)로부터의 기준 진리 분기점 및 추론된 분기점을 포함하는 원핫 인코딩된 벡터를 처리하는 다중 클래스 분류기(3080)로부터 유전자 발현 보존 클래스(3082), 유전자 발현 감소 클래스(3084), 및 유전자 발현 증가 클래스(3086)로의 변이체 세트(3002)의 기준 진리 분기점 및 추론된 분기점을 생성하도록 추가로 구성된다.The system (3000) is further configured to generate ground truth branch points and inferred branch points of a set of variants (3002) into a gene expression preservation class (3082), a gene expression decrease class (3084), and a gene expression increase class (3086) from a multi-class classifier (3080) that processes one-hot encoded vectors including ground truth branch points and inferred branch points from a first binary classifier (3026), a second binary classifier (3046), and a third binary classifier (3066) having decision thresholds. 도 31은, 추론된 인과성 점수(3144)에 대한 기준 진리 인과성 점수의 비교를 위해 유전자 발현 분류기 훈련(3100)이 사용되는, 개시된 기술의 일 구현의 흐름도이다. 기준 진리 인과성 점수 세트(3122)가 변이체 훈련 데이터세트(3102)에 대해 생성된다. 결정 임계치를 갖는 이진 분류기(3142)는 변이체 훈련 데이터세트(3102)를 처리하여 추론된 제1 클래스(3161) 및 추론된 제2 클래스(3163)로 분류된 추론된 인과성 점수를 생성한다. 유전자 발현 분류기 훈련 프로토콜(3100)은 다수의 반복을 위한 이진 분류기(3142)의 가중치에 대해 역전파를 수행하여 손실 함수를 최적화한다.FIG. 31 is a flow diagram of one implementation of the disclosed technology, wherein a gene expression classifier training (3100) is used for comparison of ground truth causality scores to inferred causality scores (3144) . A set of ground truth causality scores (3122) is generated for a variant training dataset (3102) . A binary classifier (3142) having a decision threshold processes the variant training dataset (3102) to generate inferred causality scores that are classified into an inferred first class (3161) and an inferred second class (3163). The gene expression classifier training protocol (3100) optimizes a loss function by performing backpropagation over the weights of a binary classifier (3142) for multiple iterations. 컴퓨터 시스템computer system 도 32은 개시된 기술을 구현하는 데 사용될 수 있는 예시적인 컴퓨터 시스템(3200)을 도시한다. 컴퓨터 시스템(3200)은 버스 하위 시스템(3255)을 통해 다수의 주변 디바이스와 통신하는 적어도 하나의 중앙 처리 장치(CPU)(3272)를 포함한다. 이러한 주변 디바이스는, 예를 들어, 메모리 디바이스 및 파일 저장 하위 시스템(3232)을 포함하는 저장 하위 시스템(3210), 사용자 인터페이스 입력 디바이스(3238), 사용자 인터페이스 출력 디바이스(3276), 및 네트워크 인터페이스 하위 시스템(3274)을 포함할 수 있다. 입력 및 출력 디바이스는 컴퓨터 시스템(3200)과의 사용자 상호작용을 하게 한다. 네트워크 인터페이스 하위 시스템(3274)은 다른 컴퓨터 시스템에서의 상응하는 인터페이스 디바이스에 대한 인터페이스를 포함하는 인터페이스를 외부 네트워크에 제공한다.FIG. 32 illustrates an exemplary computer system (3200) that may be used to implement the disclosed technology. The computer system (3200) includes at least one central processing unit (CPU) (3272) that communicates with a number of peripheral devices via a bus subsystem (3255). Such peripheral devices may include, for example, a storage subsystem (3210) including a memory device and a file storage subsystem (3232), a user interface input device (3238), a user interface output device (3276), and a network interface subsystem (3274). Input and output devices enable user interaction with the computer system (3200). The network interface subsystem (3274) provides an interface to external networks, including interfaces to corresponding interface devices in other computer systems. 일 구현에서, 생물학적 양 모델(124)은 저장 하위 시스템(3210) 및 사용자 인터페이스 입력 디바이스(3238)에 통신 가능하게 링크된다.In one implementation, a biological quantity model (124) is communicatively linked to a storage subsystem (3210) and a user interface input device (3238). 사용자 인터페이스 입력 디바이스(3238)는 키보드; 마우스, 트랙볼, 터치패드, 또는 그래픽 태블릿과 같은 포인팅 디바이스; 스캐너; 디스플레이 내에 통합된 터치 스크린; 음성 인식 시스템 및 마이크로폰과 같은 오디오 입력 디바이스; 및 다른 유형의 입력 디바이스를 포함할 수 있다. 일반적으로, 용어 '입력 디바이스'의 사용은 정보를 컴퓨터 시스템(3200)에 입력하기 위한 모든 가능한 유형의 디바이스 및 방식을 포함하도록 의도된다.User interface input devices (3238) may include a keyboard; a pointing device such as a mouse, trackball, touchpad, or graphics tablet; a scanner; a touch screen integrated into the display; an audio input device such as a voice recognition system and a microphone; and other types of input devices. In general, use of the term 'input device' is intended to encompass all possible types of devices and methods for entering information into a computer system (3200). 사용자 인터페이스 출력 장치(3276)는 디스플레이 하위 시스템, 프린터, 팩스기, 또는 오디오 출력 장치와 같은 비시각적 디스플레이를 포함할 수 있다. 디스플레이 하위 시스템은 LED 디스플레이, 음극선관(CRT), 액정 디스플레이(LCD)와 같은 평면 디바이스, 프로젝션 장치, 또는 가시적인 이미지를 생성하기 위한 일부 다른 메커니즘을 포함할 수 있다. 디스플레이 하위 시스템은 또한, 오디오 출력 디바이스와 같은 비시각적 디스플레이를 제공할 수 있다. 일반적으로, '출력 디바이스'라는 용어의 사용은 정보를 컴퓨터 시스템(3200)으로부터 사용자에게 또는 다른 기계 또는 컴퓨터 시스템에 출력하기 위한 모든 가능한 유형의 디바이스 및 방식을 포함하도록 의도된다.A user interface output device (3276) may include a non-visual display, such as a display subsystem, a printer, a fax machine, or an audio output device. The display subsystem may include a flat device such as an LED display, a cathode ray tube (CRT), a liquid crystal display (LCD), a projection device, or some other mechanism for producing a visible image. The display subsystem may also provide non-visual displays, such as audio output devices. In general, the use of the term 'output device' is intended to encompass all possible types of devices and methods for outputting information from the computer system (3200) to a user or to another machine or computer system. 저장 하위 시스템(3210)은 본원에 설명된 모듈 및 방법의 일부 또는 전부의 기능을 제공하는 프로그래밍 및 데이터 구성을 저장한다. 이러한 소프트웨어 모듈은 일반적으로, 프로세서(3278)에 의해 실행된다.The storage subsystem (3210) stores programming and data configurations that provide the functionality of some or all of the modules and methods described herein. These software modules are typically executed by the processor (3278). 프로세서(3278)는 그래픽 처리 장치(GPU), 필드 프로그래밍가능 게이트 어레이(FPGA), 주문형 반도체(ASIC), 및/또는 코어스-그레인드 재구성가능 아키텍처(CGRA)일 수 있다. 프로세서(3278)는 Google Cloud Platform™, Xilinx™, 및 Cirrascale™과 같은 심층 학습 클라우드 플랫폼에 의해 호스팅될 수 있다. 프로세서(3278)의 예는 Google의 Tensor Processing Unit(TPU)™, 랙마운트 솔루션, 예컨대 GX4 Rackmount Series™, GX32 Rackmount Series™, NVIDIA DGX-1™, Microsoft의 Stratix V FPGA™, Graphcore의 Intelligent Processor Unit (IPU)™, Snapdragon processors™을 갖는 Qualcomm의 Zeroth Platform™, NVIDIA의 Volta™, NVIDIA의 DRIVE PX™, NVIDIA의 JETSON TX1/TX2 MODULE™, Intel의 Nirvana™, Movidius VPU™, Fujitsu DPI™, ARM의 DynamicIQ™, IBM TrueNorth™, Testa V100s™을 갖는 Lambda GPU 서버 등을 포함한다.The processor (3278) may be a graphics processing unit (GPU), a field programmable gate array (FPGA), an application-specific integrated circuit (ASIC), and/or a course-grained reconfigurable architecture (CGRA). The processor (3278) may be hosted by a deep learning cloud platform such as Google Cloud Platform™, Xilinx™, and Cirrascale™. Examples of processors (3278) include Google's Tensor Processing Unit (TPU)™, rackmount solutions such as the GX4 Rackmount Series™, the GX32 Rackmount Series™, NVIDIA DGX-1™, Microsoft's Stratix V FPGA™, Graphcore's Intelligent Processor Unit (IPU)™, Qualcomm's Zeroth Platform™ with Snapdragon processors™, NVIDIA's Volta™, NVIDIA's DRIVE PX™, NVIDIA's JETSON TX1/TX2 MODULE™, Intel's Nirvana™, Movidius VPU™, Fujitsu DPI™, ARM's DynamicIQ™, IBM TrueNorth™, Lambda GPU servers with Testa V100s™, and so on. 저장 하위 시스템(3210)에 사용되는 메모리 하위 시스템(3222)은 프로그램 실행 동안 명령어 및 데이터의 저장을 위한 메인 랜덤 액세스 메모리(RAM)(3232) 및 고정된 명령어가 저장되는 판독 전용 메모리(ROM)(3234)를 포함하는 다수의 메모리를 포함할 수 있다. 파일 저장 하위 시스템(3232)은 프로그램 및 데이터 파일을 위한 영구 저장소를 제공할 수 있고, 하드 디스크 드라이브, 연관된 착탈식 매체와 함께 플로피 디스크 드라이브, CD-ROM 드라이브, 광학 드라이브, 또는 착탈식 매체 카트리지를 포함할 수 있다. 특정 구현의 기능을 구현하는 모듈은 저장 하위 시스템(3210) 내의 파일 저장 하위 시스템(3232)에 의해, 또는 프로세서에 의해 접근 가능한 다른 기계에 저장될 수 있다.A memory subsystem (3222) used in the storage subsystem (3210) may include a number of memories, including a main random access memory (RAM) (3232) for storing instructions and data during program execution and a read-only memory (ROM) (3234) in which fixed instructions are stored. The file storage subsystem (3232) may provide persistent storage for program and data files and may include a hard disk drive, a floppy disk drive with associated removable media, a CD-ROM drive, an optical drive, or a removable media cartridge. Modules implementing the functionality of a particular implementation may be stored by a file storage subsystem (3232) within the storage subsystem (3210), or on another machine accessible by the processor. 버스 하위 시스템(3255)은 컴퓨터 시스템(3200)의 다양한 구성요소 및 하위 시스템이 의도된 대로 서로 통신하게 하기 위한 메커니즘을 제공한다. 버스 하위 시스템(3255)이 개략적으로 단일 버스로서 도시되어 있지만, 버스 하위 시스템의 대체 구현은 다수의 버스를 사용할 수 있다.The bus subsystem (3255) provides a mechanism for allowing the various components and subsystems of the computer system (3200) to communicate with each other as intended. Although the bus subsystem (3255) is schematically depicted as a single bus, alternative implementations of the bus subsystem may use multiple buses. 컴퓨터 시스템(3200) 자체는 개인용 컴퓨터, 휴대용 컴퓨터, 워크스테이션, 컴퓨터 단말기, 네트워크 컴퓨터, 텔레비전, 메인프레임, 서버 팜, 광범위하게 분산된 느슨하게 네트워킹된 컴퓨터 세트, 또는 기타 데이터 처리 시스템이나 사용자 장치를 포함하는 다양한 유형일 수 있다. 컴퓨터 및 네트워크의 지속적으로 변화하는(ever-changing) 특성으로 인해, 도 32에 도시된 컴퓨터 시스템(3200)의 설명은 본 발명의 바람직한 구현을 예시하기 위한 특정 예로서만 의도된다. 도 32에 도시된 컴퓨터 시스템보다 더 많은 또는 더 적은 구성 요소를 갖는 컴퓨터 시스템(3200)의 많은 다른 구성이 가능하다.The computer system (3200) itself may be of various types, including a personal computer, a portable computer, a workstation, a computer terminal, a network computer, a television, a mainframe, a server farm, a widely distributed, loosely networked set of computers, or other data processing system or user device. Due to the ever-changing nature of computers and networks, the description of the computer system (3200) illustrated in FIG. 32 is intended only as a specific example to illustrate preferred implementations of the present invention. Many other configurations of the computer system (3200) are possible, having more or fewer components than the computer system illustrated in FIG. 32. 조항clauses 개시된 기술은 시스템, 방법 또는 제조 물품으로서 실시될 수 있다. 구현의 하나 이상의 특징은 기본 구현과 조합될 수 있다. 상호 배타적이지 않은 구현은 조합 가능한 것으로 교시되어 있다. 구현의 하나 이상의 특징은 다른 구현과 조합될 수 있다. 본 발명은 이러한 옵션을 사용자에게 주기적으로 리마인드한다. 이러한 옵션을 반복하는 인용의 일부 구현으로부터의 생략은 전술한 섹션에 교시된 조합을 제한하는 것으로서 간주되어서는 안 된다 - 이들 인용은 이로써 다음의 구현 각각에 참조로 통합된다.The disclosed technology can be implemented as a system, method, or article of manufacture. One or more features of an implementation may be combined with a base implementation. Non-mutually exclusive implementations are taught as composable. One or more features of an implementation may be combined with other implementations. The present invention periodically reminds the user of these options. Omission from some implementations of the citations that repeat these options should not be considered as limiting the combinations taught in the preceding sections - these citations are hereby incorporated by reference into each of the following implementations. 개시된 기술의 하나 이상의 구현 및 조항 또는 이들의 요소는, 나타낸 방법 단계를 수행하기 위한 컴퓨터 사용 가능 프로그램 코드를 갖는 비일시적 컴퓨터 판독 가능 저장 매체를 포함하는 컴퓨터 제품의 형태로 구현될 수 있다. 더욱이, 개시된 기술의 하나 이상의 구현 및 조항 또는 이들의 요소는, 메모리, 및 메모리에 커플링되고 예시적인 방법 단계를 수행하기 위해 연산하는 적어도 하나의 프로세서를 포함하는 장치의 형태로 구현될 수 있다. 또한, 추가로, 다른 양태에서, 개시된 기술의 하나 이상의 구현 및 조항 또는 이들의 요소는, 본원에 기술된 방법 단계 중 하나 이상을 수행하기 위한 수단의 형태로 구현될 수 있고; 수단은 (i) 하드웨어 모듈(들), (ii) 하나 이상의 하드웨어 프로세서 상에서 실행되는 소프트웨어 모듈(들), 또는 (iii) 하드웨어와 소프트웨어 모듈의 조합을 포함할 수 있고; (i) 내지 (iii) 중 임의의 것은 본원에 제시된 특정 기법을 구현하고, 소프트웨어 모듈은 컴퓨터 판독가능 저장 매체(또는 다수의 그러한 매체)에 저장된다.One or more implementations and provisions or elements thereof of the disclosed technology can be implemented in the form of a computer product comprising a non-transitory computer-readable storage medium having computer-usable program code for performing the disclosed method steps. Furthermore, one or more implementations and provisions or elements thereof of the disclosed technology may be implemented in the form of a device comprising a memory and at least one processor coupled to the memory and operable to perform the exemplary method steps. In addition, in another aspect, one or more implementations and provisions or elements thereof of the disclosed technology may be implemented in the form of a means for performing one or more of the method steps described herein; wherein the means may comprise (i) hardware module(s), (ii) software module(s) executing on one or more hardware processors, or (iii) a combination of hardware and software modules; wherein any of (i) to (iii) implements a particular technique presented herein, and wherein the software modules are stored on a computer-readable storage medium (or multiple such media). 이 섹션에 기술된 조항은 특징으로 조합될 수 있다. 간결함을 위해 특징의 조합은 개별적으로 열거되지 않으며 각 기본 특징 세트에서 반복되지 않는다. 독자는 이 섹션에 기술된 조항에서 식별된 특징이 이 출원의 다른 섹션에서 구현으로 식별된 기본 특징 세트와 어떻게 쉽게 조합될 수 있는지 이해할 것이다. 이러한 조항은 상호 배타적이거나 포괄적이거나 제한적이라는 의미가 아니며; 개시된 기술은 이러한 조항에 제한되지 않으며 오히려 청구된 기술 및 그 등가물의 범위 내에서 가능한 모든 조합, 수정 및 변형을 포함한다.The provisions described in this section may be combined into features. For brevity, combinations of features are not listed individually and are not repeated in each base feature set. The reader will appreciate how the features identified in the provisions described in this section can be readily combined with the basic feature sets identified for implementation in other sections of this application. These terms are not intended to be mutually exclusive, exhaustive, or restrictive; the disclosed technology is not limited to these terms, but rather encompasses all possible combinations, modifications, and variations within the scope of the claimed technology and its equivalents. 이 섹션에 설명된 조항의 다른 구현은 이 섹션에 설명된 조항 중 임의의 조항을 수행하기 위해 프로세서에 의해 실행 가능한 명령어를 저장하는 비일시적 컴퓨터 판독 가능 저장 매체를 포함할 수 있다. 이 섹션에 기술된 조항의 또 다른 구현은 메모리 및 이 섹션에 기술된 조항 중 임의의 항목을 수행하기 위해 메모리에 저장된 명령어를 실행하도록 연산 가능한 하나 이상의 프로세서를 포함하는 시스템을 포함할 수 있다.Another implementation of the provisions described in this section may include a non-transitory computer-readable storage medium storing instructions executable by a processor to perform any of the provisions described in this section. Another implementation of the provisions described in this section may include a system including memory and one or more processors operable to execute instructions stored in the memory to perform any of the provisions described in this section. 다음의 조항이 개시된다:The following provisions are disclosed: 조항 세트 1Set of clauses 1 1. 염기 해상도에서 유전자 발현의 변화를 검출하는 인공 지능 기반 시스템으로서,1. An artificial intelligence-based system for detecting changes in gene expression at base resolution, 시퀀스 데이터베이스에 액세스하고 입력 염기 시퀀스를 생성하는 입력 생성 로직으로서, 상기 입력 염기 시퀀스는 타깃 염기 시퀀스를 포함하고, 상기 타깃 염기 시퀀스는 하류 컨텍스트 염기를 갖는 우측 염기 시퀀스 및 상류 컨텍스트 염기를 갖는 좌측 염기 시퀀스에 의해 플랭킹되는, 상기 입력 생성 로직;An input generating logic for accessing a sequence database and generating an input base sequence, wherein the input base sequence includes a target base sequence, and the target base sequence is flanked by a right base sequence having a downstream context base and a left base sequence having an upstream context base; 상기 입력 염기 시퀀스를 처리하고 상기 입력 염기 시퀀스의 대체 표현을 생성하는 생물학적 양 모델; 및A biological quantity model that processes the input base sequence and generates an alternative representation of the input base sequence; and 상기 입력 염기 시퀀스의 대체 표현을 처리하고 복수의 생물학적 양 출력 시퀀스를 생성하는 생물학적 양 출력 생성 로직을 포함하되,Comprising biological quantity output generation logic for processing alternative representations of the input base sequence and generating a plurality of biological quantity output sequences, 상기 복수의 생물학적 양 출력 시퀀스에서 제1 생물학적 양 출력 시퀀스는 상기 타깃 염기 시퀀스에서의 각각의 타깃 염기에 대한 제1 각각의 염기 당 생물학적 양 출력을 포함하고,In the above plurality of biological quantity output sequences, the first biological quantity output sequence comprises a first respective base-wise biological quantity output for each target base in the target base sequence, 상기 제1 각각의 염기 당 생물학적 양 출력은 복수의 종에 걸쳐 상기 각각의 타깃 염기의 각각의 진화 보존 측정치를 특정하고,The biological quantity output per each of the first bases specifies a measure of evolutionary conservation of each of the target bases across multiple species, 상기 복수의 생물학적 양 출력 시퀀스에서 제2 생물학적 양 출력 시퀀스는 상기 타깃 염기 시퀀스에서의 각각의 타깃 염기에 대한 제2 각각의 염기 당 생물학적 양 출력을 포함하고,In the above plurality of biological quantity output sequences, the second biological quantity output sequence comprises a second respective base-wise biological quantity output for each target base in the target base sequence, 상기 제2 각각의 염기 당 생물학적 양 출력은 상기 타깃 염기 시퀀스에서의 각각의 위치에서 상기 각각의 타깃 염기의 각각의 전사 개시 측정치를 특정하는, 인공 지능 기반 시스템.An artificial intelligence-based system wherein each of the second respective base-specific biological quantity outputs specifies a measurement of each transcription initiation of each of the target bases at each position in the target base sequence. 2. 제1항에 있어서, 상기 각각의 진화 보존 측정치는 신경 치환의 널 모델로부터의 편차를 특정하여 보존으로서 상기 타깃 염기 시퀀스에서의 주어진 위치에서 주어진 타깃 염기의 치환율의 감소를 검출하고, 가속으로서 상기 주어진 위치에서 상기 주어진 타깃 염기의 치환율의 증가를 검출하는 계통 P-값(phyloP) 점수인, 인공 지능 기반 시스템.2. In the first paragraph, the artificial intelligence-based system, wherein each of the evolutionary conservation measures is a phyloP score that detects a decrease in the substitution rate of a given target base at a given position in the target base sequence as conservation by specifying a deviation from the null model of neural substitution, and an increase in the substitution rate of a given target base at a given position as acceleration. 3. 제2항에 있어서, 상기 각각의 진화 보존 측정치는 보존된 상태 또는 비보존된 상태를 갖는 주어진 위치에서 주어진 목표 염기의 사후 확률을 특정하는 phastCons 점수인, 인공 지능 기반 시스템.3. An artificial intelligence-based system, wherein each of the evolutionary conservation measures in the second paragraph is a phastCons score that specifies the posterior probability of a given target base at a given position having a conserved state or a non-conserved state. 4. 제2항에 있어서, 상기 각각의 진화 보존 측정치는 상기 복수의 종에 걸쳐 상기 주어진 위치에서 상기 주어진 타깃 염기의 치환의 수 감소를 특정하는 게놈 진화 속도 프로파일링(GERP) 점수인, 인공 지능 기반 시스템.4. An artificial intelligence-based system, wherein each of the evolutionary conservation measures in the second paragraph is a genome evolution rate profiling (GERP) score that specifies a decrease in the number of substitutions of the given target base at the given position across the plurality of species. 5. 제1항에 있어서, 상기 각각의 전사 개시 측정치는 상기 주어진 위치에서 상기 주어진 타깃 염기의 전사 개시 빈도를 특정하는 유전자 발현(CAGE) 점수의 캡 분석인, 인공 지능 기반 시스템.5. An artificial intelligence-based system, wherein each of the transcription initiation measurements in the first paragraph is a cap analysis of gene expression (CAGE) score that specifies the transcription initiation frequency of the given target base at the given position. 6. 제1항에 있어서, 상기 복수의 생물학적 양 출력 시퀀스에서의 제3 생물학적 양 출력 시퀀스는 상기 타깃 염기 시퀀스에서의 상기 각각의 타깃 염기에 대한 제3 각각의 염기 당 생물학적 양 출력을 포함하고,6. In the first paragraph, the third biological quantity output sequence in the plurality of biological quantity output sequences includes a biological quantity output per third base for each target base in the target base sequence, 상기 제3 각각의 염기 당 생물학적 양 출력은 상기 타깃 염기 시퀀스에서의 각각의 위치에서 상기 각각의 타깃 염기의 각각의 후생적 신호 수준 측정치를 특정하는, 인공 지능 기반 시스템.An artificial intelligence-based system wherein each of the third base-specific biological quantity outputs specifies a measurement of the level of each epigenetic signal of each target base at each position in the target base sequence. 7. 제6항에 있어서, 상기 후생적 신호 수준은 DNase I-과민성 부위(DHS) 또는 시퀀싱을 이용한 트랜스포사제 접근가능 염색질에 대한 분석(ATAC-Seq)을 특정하는, 인공 지능 기반 시스템.7. In the 6th paragraph, the level of the epigenetic signal is an artificial intelligence-based system that specifies analysis of DNase I-hypersensitive sites (DHS) or transposase-accessible chromatin using sequencing (ATAC-Seq). 8. 제6항에 있어서, 상기 후생적 신호 수준은 전사 인자(TF) 결합을 특정하는, 인공 지능 기반 시스템.8. In the 6th paragraph, the level of the epigenetic signal is an artificial intelligence-based system that specifies transcription factor (TF) binding. 9. 제6항에 있어서, 후생적 신호 수준은 히스톤 수정(HM) 마크를 특정하는, 인공 지능 기반 시스템.9. In paragraph 6, an artificial intelligence-based system, wherein the epigenetic signal level specifies a histone modification (HM) mark. 10. 제1항에 있어서, 추가로:10. In paragraph 1, additionally: 상기 복수의 생물학적 양 출력 시퀀스를 처리하고 상기 복수의 생물학적 양 출력 시퀀스의 대체 표현을 생성하는 유전자 발현 모델; 및A gene expression model that processes the plurality of biological quantity output sequences and generates alternative representations of the plurality of biological quantity output sequences; and 상기 복수의 생물학적 양 출력 시퀀스의 대체 표현을 처리하고, 상기 타깃 염기 시퀀스에서의 상기 각각의 타깃 염기에 대한 각각의 염기 당 유전자 발현 출력의 유전자 발현 출력 시퀀스를 생성하는 유전자 발현 출력 생성 로직을 포함하도록 구성되되;A gene expression output generating logic configured to process alternative representations of the above plural biological quantity output sequences and generate a gene expression output sequence of gene expression outputs per base for each of the target bases in the target base sequence; 상기 주어진 위치에서 상기 주어진 타깃 염기에 대한 상기 유전자 발현 출력 시퀀스에서 주어진 염기 당 유전자 발현 출력은 상기 주어진 위치에서 상기 주어진 타깃 염기의 유전자 발현 수준의 척도를 특정하는, 인공 지능 기반 시스템.An artificial intelligence-based system, wherein the gene expression output per base in the gene expression output sequence for the given target base at the given position specifies a measure of the gene expression level of the given target base at the given position. 11. 제10항에 있어서, 상기 유전자 발현 수준은 CAGE 전사 시작 부위(CTSS)와 같은 염기 당 메트릭으로 측정되는, 인공 지능 기반 시스템.11. An artificial intelligence-based system in clause 10, wherein the gene expression level is measured by a per-base metric such as CAGE transcription start site (CTSS). 12. 제10항에 있어서, 상기 유전자 발현 수준은 TPM(transcripts per million) 또는 RPKM(reads per kilobase of transcript)과 같은 유전자 당 메트릭으로 측정되는, 인공 지능 기반 시스템.12. An artificial intelligence-based system in clause 10, wherein the gene expression level is measured by a metric per gene, such as transcripts per million (TPM) or reads per kilobase of transcript (RPKM). 13. 제10항에 있어서, 상기 유전자 발현 수준은 FPKM(fragments per kilobase million)과 같은 유전자 당 메트릭으로 측정되는, 인공 지능 기반 시스템.13. An artificial intelligence-based system, wherein the gene expression level in clause 10 is measured by a per-gene metric such as fragments per kilobase million (FPKM). 14. 제1항에 있어서, 변이체 분류 로직을 포함하도록 추가로 구성되는, 인공 지능 기반 시스템.14. An artificial intelligence-based system, further configured to include variant classification logic, in the first paragraph. 15. 제14항에 있어서, 상기 변이체 분류 로직은 상기 시퀀스 데이터베이스에 액세스하고 기준 염기 시퀀스를 생성하는 기준 입력 생성 로직을 포함하도록 추가로 구성되고, 상기 기준 염기 시퀀스는 기준 타깃 염기 시퀀스를 포함하고, 상기 기준 타깃 염기 시퀀스는 분석 중인 위치에서의 기준 염기를 포함하고, 상기 기준 염기는 하류 컨텍스트 염기를 갖는 우측 염기 시퀀스 및 상류 컨텍스트 염기를 갖는 좌측 염기 시퀀스에 의해 플랭킹되는, 인공 지능 기반 시스템.15. In the 14th paragraph, the variant classification logic is further configured to include a reference input generation logic that accesses the sequence database and generates a reference base sequence, wherein the reference base sequence includes a reference target base sequence, wherein the reference target base sequence includes a reference base at a position being analyzed, and the reference base is flanked by a right base sequence having a downstream context base and a left base sequence having an upstream context base. An artificial intelligence-based system. 16. 제15항에 있어서, 상기 변이체 분류 로직은 상기 시퀀스 데이터베이스에 액세스하고 대체 염기 시퀀스를 생성하는 대체 입력 생성 로직을 포함하도록 추가로 구성되고, 상기 대체 염기 시퀀스는 대체 타깃 염기 시퀀스를 포함하고, 상기 대체 타깃 염기 시퀀스는 상기 분석 중 위치에서 대체 염기를 포함하고, 상기 대체 염기는 상기 하류 컨텍스트 염기를 갖는 상기 우측 염기 시퀀스 및 상기 상류 컨텍스트 염기를 갖는 상기 좌측 염기 시퀀스에 의해 플랭킹되는, 인공 지능 기반 시스템.16. In the 15th paragraph, the variant classification logic is further configured to include a replacement input generation logic that accesses the sequence database and generates a replacement base sequence, wherein the replacement base sequence includes a replacement target base sequence, wherein the replacement target base sequence includes a replacement base at a position during the analysis, and the replacement base is flanked by the right base sequence having the downstream context base and the left base sequence having the upstream context base. An artificial intelligence-based system. 17. 제15항에 있어서, 상기 변이체 분류 로직은, 상기 생물학적 양 모델이 상기 기준 염기 시퀀스를 처리하고 상기 기준 염기 시퀀스의 대체 표현을 생성하게 하고, 추가로 상기 생물학적 양 출력 생성 로직이 상기 기준 염기 시퀀스의 대체 표현을 처리하고 복수의 기준 생물학적 양 출력 시퀀스를 생성하게 하는 기준 처리 로직을 포함하도록 추가로 구성되고,17. In the 15th paragraph, the variant classification logic is further configured to include reference processing logic that causes the biological quantity model to process the reference base sequence and generate an alternative representation of the reference base sequence, and further causes the biological quantity output generation logic to process the alternative representation of the reference base sequence and generate a plurality of reference biological quantity output sequences. 상기 복수의 기준 생물학적 양 출력 시퀀스에서 각각의 기준 생물학적 양 출력 시퀀스는 상기 기준 타깃 염기 시퀀스에서의 각각의 기준 타깃 염기에 대한 각각의 염기 당 기준 생물학적 양 출력을 포함하는, 인공 지능 기반 시스템.An artificial intelligence-based system, wherein each of the plurality of reference biological quantity output sequences includes a reference biological quantity output per base for each reference target base in the reference target base sequence. 18. 제17항에 있어서, 상기 복수의 기준 생물학적 양 출력 시퀀스에서의 제1 기준 생물학적 양 출력 시퀀스는 상기 기준 타깃 염기 시퀀스에서의 각각의 기준 타깃 염기에 대한 제1 각각의 염기 당 기준 생물학적 양 출력을 포함하고,18. In the 17th paragraph, the first reference biological quantity output sequence in the plurality of reference biological quantity output sequences includes a first respective base-specific reference biological quantity output for each reference target base in the reference target base sequence, 상기 제1 각각의 염기 당 기준 생물학적 양 출력은 상기 복수의 종에 걸쳐 상기 각각의 기준 타깃 염기의 각각의 진화 보존 측정치를 특정하는, 인공 지능 기반 시스템.An artificial intelligence-based system wherein each of the first base-specific reference biological quantity outputs specifies a measure of evolutionary conservation of each of the reference target bases across the plurality of species. 19. 제17항에 있어서, 상기 복수의 기준 생물학적 양 출력 시퀀스에서의 제2 기준 생물학적 양 출력 시퀀스는 상기 기준 타깃 염기 시퀀스에서의 상기 각각의 기준 타깃 염기에 대한 제2 각각의 염기 당 기준 생물학적 양 출력을 포함하고,19. In the 17th paragraph, the second reference biological quantity output sequence in the plurality of reference biological quantity output sequences includes a second respective base-specific reference biological quantity output for each of the reference target bases in the reference target base sequence, 상기 제2 각각의 염기 당 기준 생물학적 양 출력은 상기 기준 타깃 염기 시퀀스에서의 각각의 위치에서 상기 각각의 기준 타깃 염기의 각각의 전사 개시 측정치를 특정하는, 인공 지능 기반 시스템.An artificial intelligence-based system wherein each of the second respective base-specific biological quantity outputs specifies a respective transcription initiation measurement of each of the respective reference target bases at each position in the reference target base sequence. 20. 제16항에 있어서, 상기 변이체 분류 로직은, 상기 생물학적 양 모델이 상기 대체 염기 시퀀스를 처리하고 상기 대체 염기 시퀀스의 대체 표현을 생성하게 하고, 추가로 상기 생물학적 양 출력 생성 로직이 상기 대체 염기 시퀀스의 대체 표현을 처리하고 복수의 대체 생물학적 양 출력 시퀀스를 생성하게 하는 대체 처리 로직을 포함하도록 추가로 구성되고,20. In the 16th paragraph, the variant classification logic is further configured to include replacement processing logic that causes the biological quantity model to process the replacement base sequence and generate a replacement representation of the replacement base sequence, and further causes the biological quantity output generation logic to process the replacement representation of the replacement base sequence and generate a plurality of replacement biological quantity output sequences. 상기 복수의 대체 생물학적 양 출력 시퀀스에서 각각의 대체 생물학적 양 출력 시퀀스는 상기 대체 타깃 염기 시퀀스에서의 각각의 대체 타깃 염기에 대한 각각의 염기 당 대체 생물학적 양 출력을 포함하는, 인공 지능 기반 시스템.An artificial intelligence-based system, wherein each alternative biological quantity output sequence in said plurality of alternative biological quantity output sequences comprises an alternative biological quantity output per base for each alternative target base in said alternative target base sequence. 21. 제20항에 있어서, 상기 복수의 대체 생물학적 양 출력 시퀀스에서의 제1 대체 생물학적 양 출력 시퀀스는 상기 대체 타깃 염기 시퀀스에서의 각각의 대체 타깃 염기에 대한 제1 각각의 염기 당 대체 생물학적 양 출력을 포함하고,21. In the 20th paragraph, the first alternative biological quantity output sequence in the plurality of alternative biological quantity output sequences comprises a first respective base-specific alternative biological quantity output for each alternative target base in the alternative target base sequence, 상기 제1 각각의 염기 당 대체 생물학적 양 출력은 상기 복수의 종에 걸쳐 상기 각각의 대체 타깃 염기의 각각의 진화 보존 측정치를 특정하는, 인공 지능 기반 시스템.An artificial intelligence-based system wherein each of the first alternative biological quantity outputs specifies a measure of evolutionary conservation of each of the alternative target bases across the plurality of species. 22. 제20항에 있어서, 상기 복수의 대체 생물학적 양 출력 시퀀스에서의 제2 대체 생물학적 양 출력 시퀀스는 상기 대체 타깃 염기 시퀀스에서의 상기 각각의 대체 타깃 염기에 대한 제2 각각의 염기 당 대체 생물학적 양 출력을 포함하고,22. In the 20th paragraph, the second alternative biological quantity output sequence in the plurality of alternative biological quantity output sequences comprises a second respective base-wise alternative biological quantity output for each of the alternative target bases in the alternative target base sequence, 상기 제2 각각의 염기 당 대체 생물학적 양 출력은 상기 대체 타깃 염기 시퀀스에서의 각각의 위치에서 상기 각각의 대체 타깃 염기의 각각의 전사 개시 측정치를 특정하는, 인공 지능 기반 시스템.An artificial intelligence-based system wherein each of the second respective base-specific alternative biological quantity outputs specifies a respective transcription initiation measurement of each of the respective alternative target bases at each position in the alternative target base sequence. 23. 제20항에 있어서, 상기 변이체 분류 로직은 상기 제1 기준 생물학적 양 출력 시퀀스 및 상기 제1 대체 생물학적 양 출력 시퀀스를 위치별로 비교하고, 상기 제1 기준 생물학적 양 출력 시퀀스 및 상기 제1 대체 생물학적 양 출력 시퀀스에서의 위치에 대한 제1 위치별 시퀀스 차이를 갖는 제1 델타 시퀀스를 생성하는 병원성 예측 로직을 포함하도록 추가로 구성되는, 인공 지능 기반 시스템.23. An artificial intelligence-based system in claim 20, wherein the variant classification logic is further configured to include pathogenicity prediction logic that compares the first reference biological quantity output sequence and the first alternative biological quantity output sequence by position, and generates a first delta sequence having a first position-by-position sequence difference for a position in the first reference biological quantity output sequence and the first alternative biological quantity output sequence. 24. 제23항에 있어서, 상기 병원성 예측 로직은 상기 제2 기준 생물학적 양 출력 시퀀스 및 상기 제2 대체 생물학적 양 출력 시퀀스를 위치별로 비교하고, 상기 제2 기준 생물학적 양 출력 시퀀스 및 상기 제2 대체 생물학적 양 출력 시퀀스에서의 위치에 대한 제2 위치별 시퀀스 차이를 갖는 제2 델타 시퀀스를 생성하도록 추가로 구성되는, 인공 지능 기반 시스템.24. In the 23rd paragraph, the pathogenicity prediction logic is further configured to compare the second reference biological quantity output sequence and the second alternative biological quantity output sequence by position, and generate a second delta sequence having a second position-wise sequence difference for the position in the second reference biological quantity output sequence and the second alternative biological quantity output sequence. An artificial intelligence-based system. 25. 제24항에 있어서, 상기 병원성 예측 로직은 상기 제1 델타 시퀀스 및 상기 제2 델타 시퀀스에 의존하여 상기 대체 염기에 대한 병원성 예측을 생성하도록 추가로 구성되는, 인공 지능 기반 시스템.25. An artificial intelligence-based system in claim 24, wherein the pathogenicity prediction logic is further configured to generate a pathogenicity prediction for the substituted base based on the first delta sequence and the second delta sequence. 26. 제24항에 있어서, 상기 병원성 예측 로직은 상기 제1 위치별 시퀀스 차이를 제1 누적 시퀀스 값으로 누적하고, 상기 제2 위치별 시퀀스 차이를 제2 누적 시퀀스 값으로 누적하도록 추가로 구성되는, 인공 지능 기반 시스템.26. In the 24th paragraph, the artificial intelligence-based system, wherein the pathogenicity prediction logic is further configured to accumulate the first location-specific sequence difference into a first accumulated sequence value, and to accumulate the second location-specific sequence difference into a second accumulated sequence value. 27. 제26항에 있어서, 상기 제1 누적 시퀀스 값은 상기 제1 위치별 시퀀스 차이의 평균이고, 상기 제2 누적 시퀀스 값은 상기 제2 위치별 시퀀스 차이의 평균인, 인공 지능 기반 시스템.27. An artificial intelligence-based system in claim 26, wherein the first accumulated sequence value is an average of the sequence differences for each first location, and the second accumulated sequence value is an average of the sequence differences for each second location. 28. 제26항에 있어서, 상기 제1 누적 시퀀스 값은 상기 제1 위치별 시퀀스 차이의 합이고, 상기 제2 누적 시퀀스 값은 상기 제2 위치별 시퀀스 차이의 합인, 인공 지능 기반 시스템. 28. An artificial intelligence-based system in claim 26, wherein the first accumulated sequence value is a sum of the sequence differences for each first location, and the second accumulated sequence value is a sum of the sequence differences for each second location. 29. 제26항에 있어서, 상기 병원성 예측 로직은 상기 제1 누적 시퀀스 값 및 상기 제2 누적 시퀀스 값에 의존하여 상기 대체 염기에 대한 상기 병원성 예측을 생성하도록 추가로 구성되는, 인공 지능 기반 시스템.29. In the 26th paragraph, the artificial intelligence-based system, wherein the pathogenicity prediction logic is further configured to generate the pathogenicity prediction for the substituted base based on the first cumulative sequence value and the second cumulative sequence value. 30. 제29항에 있어서, 상기 병원성 예측 로직은 상기 제1 누적 시퀀스 값 및 상기 제2 누적 시퀀스 값의 평균에 의존하여 상기 대체 염기에 대한 상기 병원성 예측을 생성하도록 추가로 구성되는, 인공 지능 기반 시스템.30. An artificial intelligence-based system in claim 29, wherein the pathogenicity prediction logic is further configured to generate the pathogenicity prediction for the substituted base based on an average of the first cumulative sequence value and the second cumulative sequence value. 31. 제29항에 있어서, 상기 병원성 예측 로직은 상기 제1 누적 시퀀스 값 및 상기 제2 누적 시퀀스 값의 합에 의존하여 상기 대체 염기에 대한 상기 병원성 예측을 생성하도록 추가로 구성되는, 인공 지능 기반 시스템.31. An artificial intelligence-based system in claim 29, wherein the pathogenicity prediction logic is further configured to generate the pathogenicity prediction for the substituted base based on the sum of the first cumulative sequence value and the second cumulative sequence value. 32. 제23항에 있어서, 상기 병원성 예측 로직은 상기 제1 위치별 시퀀스 차이에 기초하여 상기 제1 델타 시퀀스에서의 위치를 보존된 상태 또는 비보존된 상태에 속하는 것으로 분류하도록 추가로 구성되는, 인공 지능 기반 시스템.32. In the 23rd paragraph, the artificial intelligence-based system, wherein the pathogenicity prediction logic is further configured to classify a position in the first delta sequence as belonging to a conserved state or a non-conserved state based on the sequence difference at the first position. 33. 제32항에 있어서, 상기 병원성 예측 로직은 상기 제2 델타 시퀀스에서의 해당 위치를 상기 보존 상태에 속하는 것으로 분류된 상기 제1 델타 시퀀스에서의 해당 위치와 일치하는 신호 상태에 속하는 것으로 분류하고, 상기 제2 델타 시퀀스에서의 해당 위치를 상기 비보존 상태에 속하는 것으로 분류된 상기 제1 델타 시퀀스에서의 해당 위치와 일치하는 노이즈 상태에 속하는 것으로 분류하도록 추가로 구성되는, 인공 지능 기반 시스템.33. In the 32nd paragraph, the artificial intelligence-based system is further configured to classify the corresponding position in the second delta sequence as belonging to a signal state matching the corresponding position in the first delta sequence classified as belonging to the preserved state, and to classify the corresponding position in the second delta sequence as belonging to a noise state matching the corresponding position in the first delta sequence classified as belonging to the non-preserved state. 34. 제33항에 있어서, 상기 병원성 예측 로직은 상기 제2 위치별 시퀀스 차이의 하위 세트를 변조된 누적 시퀀스 값으로 누적하도록 추가로 구성되고,34. In paragraph 33, the pathogenicity prediction logic is further configured to accumulate a subset of the second location-specific sequence differences into a modulated cumulative sequence value, 상기 제2 위치별 시퀀스 차이의 하위 세트에서의 제2 위치별 시퀀스 차이는 상기 신호 상태에 속하는 것으로 분류되는 상기 제2 델타 시퀀스에서의 해당 위치에 위치하는, 인공 지능 기반 시스템.An artificial intelligence-based system wherein a second positional sequence difference in a subset of the second positional sequence differences is located at a corresponding position in the second delta sequence that is classified as belonging to the signal state. 35. 제34항에 있어서, 상기 병원성 예측 로직은 상기 변조된 누적 시퀀스 값에 의존하여 상기 대체 염기에 대한 상기 병원성 예측을 생성하도록 추가로 구성되는, 인공 지능 기반 시스템.35. An artificial intelligence-based system in claim 34, wherein the pathogenicity prediction logic is further configured to generate the pathogenicity prediction for the substituted base based on the modified cumulative sequence value. 36. 제34항에 있어서, 상기 변조된 누적 시퀀스 값은 상기 제2 위치별 시퀀스 차이의 하위 세트에서의 상기 제2 위치별 시퀀스 차이의 평균인, 인공 지능 기반 시스템.36. An artificial intelligence-based system in claim 34, wherein the modulated accumulated sequence value is an average of the second position-specific sequence differences in a subset of the second position-specific sequence differences. 37. 제34항에 있어서, 상기 변조된 누적 시퀀스 값은 상기 제2 위치별 시퀀스 차이의 하위 세트에서의 상기 제2 위치별 시퀀스 차이의 합인, 인공 지능 기반 시스템.37. An artificial intelligence-based system in claim 34, wherein the modulated accumulated sequence value is a sum of the second position-specific sequence differences in a subset of the second position-specific sequence differences. 38. 제24항에 있어서, 상기 병원성 예측 로직은 상기 제1 기준 생물학적 양 출력 시퀀스 및 상기 제1 대체 생물학적 양 출력 시퀀스의 각각의 부분을 위치별로 비교하고, 상기 각각의 부분에서의 위치에 대해 제1 위치별 하위 시퀀스 차이를 갖는 제1 델타 하위 시퀀스를 생성하도록 추가로 구성되는, 인공 지능 기반 시스템.38. In the 24th paragraph, the pathogenicity prediction logic is further configured to compare each portion of the first reference biological quantity output sequence and the first alternative biological quantity output sequence by position, and generate a first delta subsequence having a first position-by-position subsequence difference for each portion. 39. 제38항에 있어서, 상기 병원성 예측 로직은 상기 제2 기준 생물학적 양 출력 시퀀스 및 상기 제2 대체 생물학적 양 출력 시퀀스의 각각의 부분을 위치별로 비교하고, 상기 각각의 부분에서의 위치에 대해 제2 위치별 하위 시퀀스 차이를 갖는 제2 델타 하위 시퀀스를 생성하도록 추가로 구성되는, 인공 지능 기반 시스템.39. In the 38th paragraph, the pathogenicity prediction logic is further configured to compare each portion of the second reference biological quantity output sequence and the second alternative biological quantity output sequence by position, and generate a second delta subsequence having a second position-by-position subsequence difference for each portion. 40. 제39항에 있어서, 상기 각각의 부분은 상기 분석 중 위치 주위의 우측 및 좌측 플랭킹 위치에 걸쳐 있는, 인공 지능 기반 시스템.40. An artificial intelligence-based system in clause 39, wherein each of said parts spans the right and left flanking positions around the position during said analysis. 41. 제40항에 있어서, 상기 병원성 예측 로직은 상기 제1 델타 하위 시퀀스 및 상기 제2 델타 하위 시퀀스에 의존하여 상기 대체 염기에 대한 병원성 예측을 생성하도록 추가로 구성되는, 인공 지능 기반 시스템.41. An artificial intelligence-based system in claim 40, wherein the pathogenicity prediction logic is further configured to generate a pathogenicity prediction for the substituted base based on the first delta subsequence and the second delta subsequence. 42. 제40항에 있어서, 상기 병원성 예측 로직은 상기 제1 위치별 하위 시퀀스 차이를 제1 누적된 하위 시퀀스 값으로 누적하고, 상기 제2 위치별 하위 시퀀스 차이를 제2 누적된 하위 시퀀스 값으로 누적하도록 추가로 구성되는, 인공 지능 기반 시스템.42. In clause 40, the artificial intelligence-based system is further configured to accumulate the first position-specific sub-sequence difference into a first accumulated sub-sequence value, and to accumulate the second position-specific sub-sequence difference into a second accumulated sub-sequence value. 43. 제42항에 있어서, 상기 제1 누적 하위 시퀀스 값은 상기 제1 위치별 하위 시퀀스 차이의 평균이고, 상기 제2 누적 하위 시퀀스 값은 상기 제2 위치별 하위 시퀀스 차이의 평균인, 인공 지능 기반 시스템.43. An artificial intelligence-based system in claim 42, wherein the first cumulative subsequence value is an average of the subsequence differences by the first location, and the second cumulative subsequence value is an average of the subsequence differences by the second location. 44. 제42항에 있어서, 상기 제1 누적 하위 시퀀스 값은 상기 제1 위치별 하위 시퀀스 차이의 합이고, 상기 제2 누적 하위 시퀀스 값은 상기 제2 위치별 하위 시퀀스 차이의 합인, 인공 지능 기반 시스템.44. An artificial intelligence-based system in clause 42, wherein the first cumulative sub-sequence value is the sum of the sub-sequence differences for each first location, and the second cumulative sub-sequence value is the sum of the sub-sequence differences for each second location. 45. 제42항에 있어서, 상기 병원성 예측 로직은 상기 제1 축적된 하위 시퀀스 값 및 상기 제2 축적된 하위 시퀀스 값에 의존하여 상기 대체 염기에 대한 상기 병원성 예측을 생성하도록 추가로 구성되는, 인공 지능 기반 시스템. 45. An artificial intelligence-based system according to claim 42, wherein the pathogenicity prediction logic is further configured to generate the pathogenicity prediction for the substituted base based on the first accumulated sub-sequence value and the second accumulated sub-sequence value. 46. 제45항에 있어서, 상기 병원성 예측 로직은 상기 제1 누적 하위 시퀀스 값 및 상기 제2 누적 하위 시퀀스 값의 평균에 의존하여 상기 대체 염기에 대한 상기 병원성 예측을 생성하도록 추가로 구성되는, 인공 지능 기반 시스템. 46. An artificial intelligence-based system according to claim 45, wherein the pathogenicity prediction logic is further configured to generate the pathogenicity prediction for the replacement base by relying on an average of the first cumulative subsequence value and the second cumulative subsequence value. 47. 제45항에 있어서, 상기 병원성 예측 로직은 상기 제1 누적 하위 시퀀스 값 및 상기 제2 누적 하위 시퀀스 값의 합에 의존하여 상기 대체 염기에 대한 상기 병원성 예측을 생성하도록 추가로 구성되는, 인공 지능 기반 시스템. 47. An artificial intelligence-based system according to claim 45, wherein the pathogenicity prediction logic is further configured to generate the pathogenicity prediction for the replacement base based on a sum of the first cumulative subsequence value and the second cumulative subsequence value. 48. 제38항에 있어서, 상기 병원성 예측 로직은 상기 제1 위치별 하위 시퀀스 차이에 기초하여 상기 제1 델타 하위 시퀀스에서의 위치를 보존된 상태 또는 비보존된 상태에 속하는 것으로 분류하도록 추가로 구성되는, 인공 지능 기반 시스템. 48. An artificial intelligence-based system according to claim 38, wherein the pathogenicity prediction logic is further configured to classify a position in the first delta subsequence as belonging to a conserved state or a non-conserved state based on the subsequence difference by the first position. 49. 제48항에 있어서, 상기 병원성 예측 로직은 상기 제2 델타 하위 시퀀스에서의 해당 위치를 상기 보존 상태에 속하는 것으로 분류된 상기 제1 델타 하위 시퀀스에서의 해당 위치와 일치하는 신호 상태에 속하는 것으로 분류하고, 상기 제2 델타 하위 시퀀스에서의 해당 위치를 상기 비보존 상태에 속하는 것으로 분류된 상기 제1 델타 하위 시퀀스에서의 해당 위치와 일치하는 노이즈 상태에 속하는 것으로 분류하도록 추가로 구성되는, 인공 지능 기반 시스템. 49. The artificial intelligence-based system of claim 48, wherein the pathogenicity prediction logic is further configured to classify a corresponding position in the second delta subsequence as belonging to a signal state matching a corresponding position in the first delta subsequence classified as belonging to the preserved state, and to classify a corresponding position in the second delta subsequence as belonging to a noise state matching a corresponding position in the first delta subsequence classified as belonging to the non-preserved state. 50. 제49항에 있어서, 상기 병원성 예측 로직은 상기 제2 위치별 하위 시퀀스 차이의 하위 세트를 변조된 누적 하위 시퀀스 값으로 누적하도록 추가로 구성되고, 50. In the 49th paragraph, the pathogenicity prediction logic is further configured to accumulate a subset of the second position-specific subsequence differences into a modulated cumulative subsequence value, 상기 제2 위치별 하위 시퀀스 차이의 하위 세트에서의 제2 위치별 하위 시퀀스 차이는 상기 신호 상태에 속하는 것으로 분류되는 상기 제2 델타 하위 시퀀스에서의 해당 위치에 위치하는, 인공 지능 기반 시스템.An artificial intelligence-based system, wherein the second positional sub-sequence difference in a subset of the second positional sub-sequence differences is located at a corresponding position in the second delta sub-sequence that is classified as belonging to the signal state. 51. 제50항에 있어서, 상기 병원성 예측 로직은 상기 변조된 누적 하위 시퀀스 값에 의존하여 상기 대체 염기에 대한 상기 병원성 예측을 생성하도록 추가로 구성되는, 인공 지능 기반 시스템.51. An artificial intelligence-based system in claim 50, wherein the pathogenicity prediction logic is further configured to generate the pathogenicity prediction for the substituted base based on the modified cumulative subsequence value. 52. 제50항에 있어서, 상기 변조된 누적 하위 시퀀스 값은 상기 제2 위치별 하위 시퀀스 차이의 하위 세트에서의 상기 제2 위치별 하위 시퀀스 차이의 평균인, 인공 지능 기반 시스템. 52. An artificial intelligence-based system in claim 50, wherein the modulated accumulated sub-sequence value is an average of the second position-specific sub-sequence differences in a subset of the second position-specific sub-sequence differences. 53. 제50항에 있어서, 상기 변조된 누적 하위 시퀀스 값은 상기 제2 위치별 하위 시퀀스 차이의 하위 세트에서의 상기 제2 위치별 하위 시퀀스 차이의 합인, 인공 지능 기반 시스템. 53. An artificial intelligence-based system in claim 50, wherein the modulated accumulated sub-sequence value is a sum of the second position-wise sub-sequence differences in a subset of the second position-wise sub-sequence differences. 54. 제1항에 있어서, 상기 타깃 염기 시퀀스는 유전자의 코딩 영역인, 인공 지능 기반 시스템. 54. An artificial intelligence-based system according to claim 1, wherein the target base sequence is a coding region of a gene. 55. 제1항에 있어서, 상기 타깃 염기 시퀀스는 유전자의 비-코딩 영역인, 인공 지능 기반 시스템. 55. An artificial intelligence-based system according to claim 1, wherein the target base sequence is a non-coding region of a gene. 56. 제55항에 있어서, 상기 비-코딩 영역은 전사 시작 부위, 5개의 프라임 비변환 영역(UTR), 3개의 프라임 UTR, 인핸서 및 프로모터에 걸쳐 있는, 인공 지능 기반 시스템. 56. An artificial intelligence-based system according to claim 55, wherein the non-coding region spans a transcription start site, five prime untranslated regions (UTRs), three prime UTRs, an enhancer, and a promoter. 56. 제16항에 있어서, 상기 대체 염기는 이상치 개체의 코호트 중 하나의 이상치 개체에서만 발생하는 단일 변이체이고, 56. In paragraph 16, the replacement base is a single variant occurring only in one outlier individual among the cohort of outlier individuals, 이상치 개체의 코호트에서 이상치 개체는 극한 수준의 유전자 발현을 나타내는, 인공 지능 기반 시스템.An artificial intelligence-based system that identifies outliers in a cohort of outlier individuals, where the outlier individuals exhibit extreme levels of gene expression. 57. 제56항에 있어서, 극한 수준의 유전자 발현은 정규화된 유전자 발현 수준의 꼬리 사분위로부터 결정되는, 인공 지능 기반 시스템.57. An artificial intelligence-based system, wherein in clause 56, the extreme level of gene expression is determined from the tail quartile of the normalized gene expression level. 58. 제57항에 있어서, 극한 수준의 유전자 발현은 과다 유전자 발현 및 과소 유전자 발현을 포함하는, 인공 지능 기반 시스템. 58. An artificial intelligence-based system according to claim 57, wherein the extreme levels of gene expression include over-expression and under-expression of the gene. 59. 제56항에 있어서, 상기 단일 변이체는 코딩 변이체인, 인공 지능 기반 시스템. 59. An artificial intelligence-based system according to claim 56, wherein the single variant is a coding variant. 60. 제56항에 있어서, 상기 단일 변이체는 비-코딩 변이체인, 인공 지능 기반 시스템. 60. An artificial intelligence-based system according to claim 56, wherein the single variant is a non-coding variant. 61. 제60항에 있어서, 상기 비-코딩 변이체는 프로모터 변이체인, 인공 지능 기반 시스템.61. An artificial intelligence-based system according to claim 60, wherein the non-coding variant is a promoter variant. 62. 제60항에 있어서, 상기 비-코딩 변이체는 인핸서 변이체인, 인공 지능 기반 시스템.62. An artificial intelligence-based system according to claim 60, wherein the non-coding variant is an enhancer variant. 62. 제1항에 있어서, 상기 생물학적 양 모델은 제1 가중치 세트를 갖고, 62. In the first paragraph, the biological quantity model has a first weight set, 상기 생물학적 양 출력 생성 로직은 제2 가중치 세트를 갖는, 인공 지능 기반 시스템.An artificial intelligence-based system wherein the above biological quantity output generating logic has a second set of weights. 63. 제62항에 있어서, 훈련 동안, 상기 생물학적 양 모델의 제1 가중치 세트를 처음부터 훈련시켜 상기 입력 염기 시퀀스를 처리하고 상기 입력 염기 시퀀스의 대체 표현을 생성하고, 63. In the 62nd paragraph, during training, the first weight set of the biological quantity model is trained from scratch to process the input base sequence and generate an alternative representation of the input base sequence, 상기 생물학적 양 출력 생성 로직의 제2 가중치 세트를 상기 생물학적 양 모델의 제1 가중치 세트로 처음부터 그리고 종단 간으로 훈련시켜 상기 입력 염기 시퀀스의 대체 표현을 처리하고 상기 복수의 생물학적 양 출력 시퀀스를 생성하는, 인공 지능 기반 시스템.An artificial intelligence-based system, wherein the second set of weights of the biological quantity output generating logic is trained from the beginning and end-to-end with the first set of weights of the biological quantity model to process alternative representations of the input base sequence and generate the plurality of biological quantity output sequences. 64. 제63항에 있어서, 추론 동안, 상기 생물학적 양 모델은 상기 훈련된 제1 가중치 세트를 사용하고, 64. In paragraph 63, during inference, the biological quantity model uses the trained first weight set, 상기 추론 동안, 상기 생물학적 양 출력 생성 로직은 상기 훈련된 제2 가중치 세트를 사용하는, 인공 지능 기반 시스템.During said inference, said biological quantity output generating logic is an artificial intelligence-based system using said trained second weight set. 65. 제1항에 있어서, 상기 유전자 발현 모델은 제3 가중치 세트를 갖고, 65. In the first paragraph, the gene expression model has a third weight set, 상기 유전자 발현 출력 생성 로직은 제4 가중치 세트를 갖는, 인공 지능 기반 시스템.The above gene expression output generation logic is an artificial intelligence-based system having a fourth weight set. 66. 제65항에 있어서, 상기 유전자 발현 모델의 제3 가중치 세트를 처음부터 훈련시켜 상기 복수의 생물학적 양 출력 시퀀스를 처리하고 상기 복수의 생물학적 양 출력 시퀀스의 대체 표현을 생성하고, 66. In the 65th paragraph, the third weight set of the gene expression model is trained from scratch to process the plurality of biological quantity output sequences and generate alternative representations of the plurality of biological quantity output sequences, 상기 유전자 발현 출력 생성 로직의 제4 가중치 세트를 상기 유전자 발현 모델의 제3 가중치 세트로 처음부터 그리고 종단 간으로 훈련시켜 상기 복수의 생물학적 양 출력 시퀀스의 대체 표현을 처리하고 상기 유전자 발현 출력 시퀀스를 생성하는, 인공 지능 기반 시스템.An artificial intelligence-based system, wherein the fourth weight set of the gene expression output generation logic is trained from the beginning and end-to-end with the third weight set of the gene expression model to process alternative representations of the plurality of biological quantity output sequences and generate the gene expression output sequences. 67. 제66항에 있어서, 추론 동안, 상기 유전자 발현 모델은 상기 훈련된 제3 가중치 세트를 사용하고, 67. In paragraph 66, during inference, the gene expression model uses the trained third weight set, 상기 추론 동안, 상기 유전자 발현 출력 생성 로직은 상기 훈련된 제4 가중치 세트를 사용하는, 인공 지능 기반 시스템.During the above inference, the gene expression output generation logic is an artificial intelligence-based system that uses the trained fourth weight set. 68. 제65항에 있어서, 훈련 동안, 상기 생물학적 양 모델의 제1 가중치 세트를 먼저 처음부터 훈련시켜 상기 입력 염기 시퀀스를 처리하고 상기 입력 염기 시퀀스의 대체 표현을 생성한 다음, 상기 유전자 발현 모델의 제3 가중치 세트의 치환으로 재훈련시켜 상기 복수의 생물학적 양 출력 시퀀스를 처리하고 상기 복수의 생물학적 양 출력 시퀀스의 대체 표현을 생성하고, 68. In the 65th paragraph, during training, the first weight set of the biological quantity model is first trained from scratch to process the input base sequence and generate an alternative representation of the input base sequence, and then retrained with the substitution of the third weight set of the gene expression model to process the plurality of biological quantity output sequences and generate an alternative representation of the plurality of biological quantity output sequences. 상기 유전자 발현 출력 생성 로직의 제4 가중치 세트를 상기 유전자 발현 모델에서 치환된 상기 훈련된 제1 가중치 세트로 처음부터 그리고 종단 간으로 훈련시켜 상기 유전자 발현 모델에서 치환된 상기 훈련된 제1 가중치 세트에 의해 생성된 상기 복수의 생물학적 양 출력 시퀀스의 대체 표현을 처리하고 상기 유전자 발현 출력 시퀀스를 생성하는, 인공 지능 기반 시스템.An artificial intelligence-based system for processing alternative representations of the plurality of biological quantity output sequences generated by the trained first weight set substituted in the gene expression model and generating the gene expression output sequences by training the fourth weight set of the gene expression output generating logic from the beginning and end-to-end with the trained first weight set substituted in the gene expression model. 69. 제68항에 있어서, 추론 동안, 상기 생물학적 양 모델은 상기 재훈련된 제1 가중치 세트를 사용하고, 69. In paragraph 68, during inference, the biological quantity model uses the retrained first weight set, 상기 추론 동안, 상기 생물학적 양 출력 생성 로직은 상기 훈련된 제2 가중치 세트를 사용하고,During the above inference, the biological quantity output generation logic uses the second set of trained weights, 추론 동안, 상기 유전자 발현 모델은 상기 재훈련된 제1 가중치 세트를 사용하고,During inference, the gene expression model uses the retrained first set of weights, 상기 추론 동안, 상기 유전자 발현 출력 생성 로직은 상기 훈련된 제4 가중치 세트를 사용하는, 인공 지능 기반 시스템.During the above inference, the gene expression output generation logic is an artificial intelligence-based system that uses the trained fourth weight set. 70. 제17항에 있어서, 훈련 동안, 상기 생물학적 양 모델의 제1 가중치 세트를 먼저 처음부터 훈련시켜 상기 입력 염기 시퀀스를 처리하고 상기 입력 염기 시퀀스의 대체 표현을 생성하고, 70. In the 17th paragraph, during training, the first weight set of the biological quantity model is first trained from scratch to process the input base sequence and generate an alternative representation of the input base sequence, 상기 훈련 동안, 상기 생물학적 양 출력 생성 로직의 제2 가중치 세트를 먼저 상기 생물학적 양 모델의 제1 가중치 세트로 처음부터 그리고 종단 간으로 훈련시켜 상기 입력 염기 시퀀스의 대체 표현을 처리하고 상기 복수의 생물학적 양 출력 시퀀스를 생성하고,During the training, the second weight set of the biological quantity output generation logic is first trained from the beginning and end-to-end with the first weight set of the biological quantity model to process alternative representations of the input base sequence and generate the plurality of biological quantity output sequences. 상기 훈련 동안, 그 다음, 상기 생물학적 양 모델의 훈련된 제1 가중치 세트를 재훈련시켜 상기 기준 염기 시퀀스를 처리하고 상기 기준 염기 시퀀스의 대체 표현을 생성하고, 상기 대체 염기 시퀀스를 처리하고 상기 대체 염기 시퀀스의 대체 표현을 생성하고,During said training, then, the trained first weight set of said biological quantity model is retrained to process said reference base sequence and generate an alternative representation of said reference base sequence, and to process said alternative base sequence and generate an alternative representation of said alternative base sequence. 상기 훈련 동안, 그 다음, 상기 생물학적 양 출력 생성 로직의 훈련된 제2 가중치 세트를 상기 생물학적 양 모델의 훈련된 제1 가중치 세트로 종단 간으로 재훈련시켜 상기 기준 염기 시퀀스의 대체 표현을 처리하고 상기 복수의 기준 생물학적 양 출력 시퀀스를 생성하고, 상기 대체 염기 시퀀스의 대체 표현을 처리하고 상기 복수의 대체 생물학적 양 출력 시퀀스를 생성하는, 인공 지능 기반 시스템.An artificial intelligence-based system, wherein during said training, the trained second weight set of the biological quantity output generating logic is then end-to-end retrained with the trained first weight set of the biological quantity model to process alternative representations of the reference base sequence and generate the plurality of reference biological quantity output sequences, and to process alternative representations of the alternative base sequences and generate the plurality of alternative biological quantity output sequences. 71. 제70항에 있어서, 추론 동안, 상기 생물학적 양 모델은 상기 재훈련된 제1 가중치 세트를 사용하고, 71. In clause 70, during inference, the biological quantity model uses the retrained first weight set, 상기 추론 동안, 상기 생물학적 양 출력 생성 로직은 상기 재훈련된 제2 가중치 세트를 사용하는, 인공 지능 기반 시스템.An artificial intelligence-based system wherein during said inference, said biological quantity output generating logic uses said retrained second weight set. 72. 제23항에 있어서, 상기 병원성 예측 로직은 제5 가중치 세트를 갖는, 인공 지능 기반 시스템. 72. An artificial intelligence-based system according to claim 23, wherein the pathogenicity prediction logic has a fifth weight set. 73. 제72항에 있어서, 훈련 동안, 상기 생물학적 양 모델의 제1 가중치 세트를 먼저 처음부터 훈련시켜 상기 입력 염기 시퀀스를 처리하고 상기 입력 염기 시퀀스의 대체 표현을 생성하고, 73. In the 72nd paragraph, during training, the first weight set of the biological quantity model is first trained from scratch to process the input base sequence and generate an alternative representation of the input base sequence, 상기 훈련 동안, 상기 생물학적 양 출력 생성 로직의 제2 가중치 세트를 먼저 상기 생물학적 양 모델의 제1 가중치 세트로 처음부터 그리고 종단 간으로 훈련시켜 상기 입력 염기 시퀀스의 대체 표현을 처리하고 상기 복수의 생물학적 양 출력 시퀀스를 생성하고,During the training, the second weight set of the biological quantity output generation logic is first trained from the beginning and end-to-end with the first weight set of the biological quantity model to process alternative representations of the input base sequence and generate the plurality of biological quantity output sequences. 상기 훈련 동안, 그 다음, 상기 생물학적 양 모델의 훈련된 제1 가중치 세트 및 상기 생물학적 양 출력 생성 로직의 훈련된 제2 가중치 세트를 종단 간으로 재훈련시켜 대체 염기에 대한 병원성 예측을 생성하는, 인공 지능 기반 시스템.An artificial intelligence-based system, wherein during said training, the trained first set of weights of said biological quantity model and the trained second set of weights of said biological quantity output generating logic are then retrained end-to-end to generate pathogenicity predictions for alternative bases. 74. 제73항에 있어서, 추론 동안, 상기 생물학적 양 모델은 상기 재훈련된 제1 가중치 세트를 사용하고, 74. In paragraph 73, during inference, the biological quantity model uses the retrained first weight set, 상기 추론 동안, 상기 생물학적 양 출력 생성 로직은 상기 재훈련된 제2 가중치 세트를 사용하고,During the above inference, the biological quantity output generation logic uses the second set of retrained weights, 상기 추론 동안, 상기 병원성 예측 로직은 상기 훈련된 제5 가중치 세트를 사용하는, 인공 지능 기반 시스템.During the above inference, the pathogenicity prediction logic is an artificial intelligence-based system using the trained fifth weight set. 75. 제18항에 있어서, 상기 제1 각각의 염기 당 기준 생물학적 양 출력은 상기 기준 타깃 염기 시퀀스에서의 상기 각각의 위치에서 상기 각각의 기준 타깃 염기의 각각의 제1 기준 후생적 신호 수준 측정치를 특정하는, 인공 지능 기반 시스템. 75. An artificial intelligence-based system in accordance with claim 18, wherein the first respective base-specific reference biological quantity output specifies a respective first reference epigenetic signal level measurement of each respective reference target base at each respective position in the reference target base sequence. 76. 제29항에 있어서, 상기 제2 각각의 염기 당 기준 생물학적 양 출력은 상기 기준 타깃 염기 시퀀스에서의 상기 각각의 위치에서 상기 각각의 기준 타깃 염기의 각각의 제2 기준 후생적 신호 수준 측정치를 특정하는, 인공 지능 기반 시스템. 76. An artificial intelligence-based system in accordance with claim 29, wherein the second respective base-specific biological quantity output specifies a respective second reference epigenetic signal level measurement of each of the respective reference target bases at each respective position in the reference target base sequence. 77. 제21항에 있어서, 상기 제1 각각의 염기 당 대체 생물학적 양 출력은 상기 대체 타깃 염기 시퀀스에서의 상기 각각의 위치에서 상기 각각의 대체 타깃 염기의 각각의 제1 대체 후생적 신호 수준 측정치를 특정하는, 인공 지능 기반 시스템. 77. An artificial intelligence-based system in accordance with claim 21, wherein each of the first alternative biological quantity outputs specifies a first alternative epigenetic signal level measurement of each of the alternative target bases at each of the positions in the alternative target base sequence. 78. 제22항에 있어서, 상기 제2 각각의 염기 당 대체 생물학적 양 출력은 상기 대체 타깃 염기 시퀀스에서의 상기 각각의 위치에서 상기 각각의 대체 타깃 염기의 각각의 제2 대체 후생적 신호 수준 측정치를 특정하는, 인공 지능 기반 시스템. 78. An artificial intelligence-based system in claim 22, wherein the second respective base-specific alternative biological quantity output specifies a respective second alternative epigenetic signal level measurement of each respective alternative target base at each respective position in the alternative target base sequence. 79. 제1항에 있어서, 훈련 동안, 상기 생물학적 양 모델 및 상기 생물학적 양 출력 생성 로직을 먼저 처음부터 그리고 종단 간으로 훈련시켜 입력 염기 시퀀스의 분석을 염기별 진화 보존 염색질 시퀀스로 변환한 다음, 종단 간으로 재훈련시켜 입력 염기 시퀀스의 분석을 염기별 전사 개시 빈도 염색질 시퀀스로 변환하는, 인공 지능 기반 시스템. 79. An artificial intelligence-based system in accordance with claim 1, wherein during training, the biological quantity model and the biological quantity output generation logic are first trained from scratch and end-to-end to convert the analysis of the input base sequence into a base-wise evolutionary conserved chromatin sequence, and then retrained end-to-end to convert the analysis of the input base sequence into a base-wise transcription initiation frequency chromatin sequence. 80. 제1항에 있어서, 훈련 동안, 상기 생물학적 양 모델 및 상기 생물학적 양 출력 생성 로직을 먼저 처음부터 그리고 종단 간으로 훈련시켜 입력 염기 시퀀스의 분석을 염기별 후생적 신호 수준 염색질 시퀀스로 변환한 다음, 종단 간으로 재훈련시켜 입력 염기 시퀀스의 분석을 염기별 진화 보존 염색질 시퀀스로 변환하는, 인공 지능 기반 시스템. 80. An artificial intelligence-based system, wherein, during training in the first paragraph, the biological quantity model and the biological quantity output generation logic are first trained from scratch and end-to-end to convert the analysis of the input base sequence into a base-wise epigenetic signal level chromatin sequence, and then retrained end-to-end to convert the analysis of the input base sequence into a base-wise evolutionary conserved chromatin sequence. 81. 제1항에 있어서, 훈련 동안, 상기 생물학적 양 모델 및 상기 생물학적 양 출력 생성 로직을 먼저 처음부터 그리고 종단 간으로 훈련시켜 입력 염기 시퀀스의 분석을 염기별 후생적 신호 수준 염색질 시퀀스로 변환한 다음, 종단 간으로 재훈련시켜 입력 염기 시퀀스의 분석을 염기별 전사 개시 빈도 염색질 시퀀스로 변환하는, 인공 지능 기반 시스템. 81. An artificial intelligence-based system in accordance with claim 1, wherein during training, the biological quantity model and the biological quantity output generation logic are first trained from scratch and end-to-end to convert the analysis of the input base sequence into a base-wise epigenetic signal level chromatin sequence, and then retrained end-to-end to convert the analysis of the input base sequence into a base-wise transcription initiation frequency chromatin sequence. 82. 제1항에 있어서, 훈련 동안, 상기 생물학적 양 모델 및 상기 생물학적 양 출력 생성 로직을 먼저 처음부터 그리고 종단 간으로 훈련시켜 입력 염기 시퀀스의 분석을 염기별 후생적 신호 수준 염색질 시퀀스로 변환한 다음, 종단 간으로 재훈련시켜 입력 염기 시퀀스의 분석을 염기별 진화 보존 염색질 시퀀스 및 염기별 전사 개시 빈도 염색질 시퀀스로 변환하는, 인공 지능 기반 시스템. 82. An artificial intelligence-based system in accordance with claim 1, wherein during training, the biological quantity model and the biological quantity output generation logic are first trained from scratch and end-to-end to convert analysis of an input base sequence into a base-wise epigenetic signal level chromatin sequence, and then retrained end-to-end to convert analysis of an input base sequence into a base-wise evolutionary conserved chromatin sequence and a base-wise transcription initiation frequency chromatin sequence. 83. 제1항에 있어서, 복수의 후생적 효과에 의해 교란된 변이체를 포함하는 훈련 입력 기본 시퀀스의 제1 훈련 세트를 포함하도록 추가로 구성되는, 인공 지능 기반 시스템. 83. An artificial intelligence-based system, further configured to include a first training set of training input base sequences comprising variants perturbed by multiple epigenetic effects, in the first aspect. 84. 제83항에 있어서, 상기 복수의 후생적 효과에서의 후생적 효과는 염색체 간 효과, 유전자 내 효과, 집단 구조 및 조상 효과, 발현 잔차의 확률적 추정(PEER) 효과, 환경 효과, 성별 효과, 일괄 효과, 유전자형 플랫폼 효과, 및/또는 라이브러리 구축 프로토콜 효과를 포함하는, 인공 지능 기반 시스템. 84. An artificial intelligence-based system according to claim 83, wherein the epigenetic effects in the plurality of epigenetic effects include interchromosomal effects, intragenic effects, population structure and ancestral effects, probabilistic estimation of expression residuals (PEER) effects, environmental effects, gender effects, batch effects, genotype platform effects, and/or library construction protocol effects. 85. 제83항에 있어서, 상기 복수의 후생적 효과에 의해 교란되지 않은 변이체를 포함하는 훈련 입력 염기 시퀀스의 제2 훈련 세트를 포함하도록 추가로 구성되는, 인공 지능 기반 시스템. 85. An artificial intelligence-based system, further configured to include a second training set of training input base sequences comprising variants that are not perturbed by the plurality of epigenetic effects, in claim 83. 86. 제85항에 있어서, 상기 제2 훈련 세트에서의 상기 변이체는 유전자 발현을 변경하고 극한 수준의 유전자 발현을 유발하도록 신뢰성 있게 결정되는, 인공 지능 기반 시스템. 86. An artificial intelligence-based system according to claim 85, wherein the variant in the second training set is reliably determined to alter gene expression and cause an extreme level of gene expression. 87. 제86항에 있어서, 상기 제2 훈련 세트에서의 상기 변이체는 유전자 발현 수준을 증가시키는 과다 발현 유발 변이체를 포함하는, 인공 지능 기반 시스템. 87. An artificial intelligence-based system according to claim 86, wherein the variants in the second training set include overexpression-inducing variants that increase gene expression levels. 88. 제86항에 있어서, 상기 제2 훈련 세트에서의 상기 변이체는 유전자 발현 수준을 감소시키는 과소 발현 유발 변이체를 포함하는, 인공 지능 기반 시스템. 88. An artificial intelligence-based system according to claim 86, wherein the variants in the second training set include underexpression-causing variants that reduce gene expression levels. 89. 제87항에 있어서, 상기 제2 훈련 세트는 상기 유발 유전자 과다 발현 가능성을 특정하는 상기 변이체에 대한 과다 발현 확률을 특정하는, 인공 지능 기반 시스템. 89. An artificial intelligence-based system in claim 87, wherein the second training set specifies an overexpression probability for the variant that specifies the possibility of overexpression of the causative gene. 90. 제88항에 있어서, 상기 제2 훈련 세트는 상기 변이체 유발 유전자 과소 발현 가능성을 특정하는 상기 변이체에 대한 과소 발현 확률을 특정하는, 인공 지능 기반 시스템. 90. An artificial intelligence-based system according to claim 88, wherein the second training set specifies an underexpression probability for the variant that specifies the possibility of underexpression of the gene causing the variant. 91. 제86항에 있어서, 상기 제2 훈련 세트에서의 각각의 변이체는 이상치 개체의 코호트 중 단지 하나의 이상치 개체에서 발생하는 단독 변이체이고,91. In clause 86, each variant in the second training set is a single variant occurring in only one outlier individual among the cohort of outlier individuals, 이상치 개체의 코호트에서 이상치 개체는 극한 수준의 유전자 발현을 나타내는, 인공 지능 기반 시스템.An artificial intelligence-based system that identifies outliers in a cohort of outlier individuals, where the outlier individuals exhibit extreme levels of gene expression. 92. 제91항에 있어서, 극한 수준의 유전자 발현은 정규화된 유전자 발현 수준의 꼬리 사분위로부터 결정되는, 인공 지능 기반 시스템.92. An artificial intelligence-based system in clause 91, wherein the extreme level of gene expression is determined from the tail quartile of normalized gene expression levels. 93. 제91항에 있어서, 극한 수준의 유전자 발현은 과다 유전자 발현 및 과소 유전자 발현을 포함하는, 인공 지능 기반 시스템. 93. An artificial intelligence-based system according to claim 91, wherein extreme levels of gene expression include over-expression and under-expression of the gene. 94. 제91항에 있어서, 상기 단일 변이체는 코딩 변이체인, 인공 지능 기반 시스템. 94. An artificial intelligence-based system according to claim 91, wherein the single variant is a coding variant. 95. 제91항에 있어서, 상기 단일 변이체는 비-코딩 변이체인, 인공 지능 기반 시스템. 95. An artificial intelligence-based system according to claim 91, wherein the single variant is a non-coding variant. 96. 제95항에 있어서, 상기 비-코딩 변이체는 5개의 프라임 비변환 영역(UTR) 변이체, 3개의 프라임 UTR 변이체, 인핸서 변이체, 또는 프로모터 변이체인, 인공 지능 기반 시스템. 96. An artificial intelligence-based system according to claim 95, wherein the non-coding variants are five prime untranslated region (UTR) variants, three prime UTR variants, an enhancer variant, or a promoter variant. 97. 제85항에 있어서, 상기 제2 훈련 세트에서의 상기 변이체는 복수의 조직 유형에 걸쳐 있는, 인공 지능 기반 시스템. 97. An artificial intelligence-based system according to claim 85, wherein the variants in the second training set span multiple tissue types. 98. 제85항에 있어서, 상기 제2 훈련 세트에서의 상기 변이체는 복수의 셀 유형에 걸쳐 있는, 인공 지능 기반 시스템. 98. An artificial intelligence-based system according to claim 85, wherein the variants in the second training set span multiple cell types. 99. 제1항에 있어서, 상기 입력 염기 시퀀스 및 상기 복수의 생물학적 양 출력 시퀀스는 상기 복수의 조직 유형에 걸쳐 있는, 인공 지능 기반 시스템. 99. An artificial intelligence-based system according to claim 1, wherein the input base sequence and the plurality of biological quantity output sequences span the plurality of tissue types. 100. 제1항에 있어서, 상기 입력 염기 시퀀스 및 상기 복수의 생물학적 양 출력 시퀀스는 상기 복수의 셀 유형에 걸쳐 있는, 인공 지능 기반 시스템. 100. An artificial intelligence-based system in accordance with claim 1, wherein the input base sequence and the plurality of biological quantity output sequences span the plurality of cell types. 101. 제10항에 있어서, 상기 유전자 발현 출력 시퀀스는 상기 복수의 조직 유형에 걸쳐 있는, 인공 지능 기반 시스템. 101. An artificial intelligence-based system according to claim 10, wherein the gene expression output sequence spans the plurality of tissue types. 102. 제10항에 있어서, 상기 유전자 발현 출력 시퀀스는 상기 복수의 셀 유형에 걸쳐 있는, 인공 지능 기반 시스템. 102. An artificial intelligence-based system according to claim 10, wherein the gene expression output sequence spans the plurality of cell types. 103. 제1항에 있어서, 상기 생물학적 양 모델 및 상기 생물학적 양 출력 생성 로직을 먼저 상기 제1 훈련 세트에서 종단 간으로 훈련시킨 다음, 상기 제2 훈련 세트에서 재훈련시키는, 인공 지능 기반 시스템. 103. An artificial intelligence-based system, wherein the biological quantity model and the biological quantity output generation logic in the first paragraph are first trained end-to-end in the first training set and then retrained in the second training set. 104. 제1항에 있어서, 상기 제2 훈련 세트에서의 상기 변이체는 유전자 발현 변동을 나타내는 제1 기준 진리 라벨로 라벨링된 병원성 세트로서 사용되고, 공통 변이체는 유전자 발현 비변동을 나타내는 제2 기준 진리 라벨로 라벨링된 양성 세트로서 사용되는, 인공 지능 기반 시스템. 104. An artificial intelligence-based system in the first aspect, wherein the variants in the second training set are used as a pathogenic set labeled with a first reference truth label indicating gene expression variation, and the common variants are used as a benign set labeled with a second reference truth label indicating gene expression non-variation. 105. 제104항에 있어서, 상기 양성 세트는 트리뉴클레오티드 컨텍스트, 동종 중합체, k-mer, 이웃 GC 빈도 및 시퀀싱 깊이에 대해 균형을 이루는, 인공 지능 기반 시스템. 105. An artificial intelligence-based system in claim 104, wherein the positive set is balanced for trinucleotide context, homopolymer, k-mer, neighboring GC frequency and sequencing depth. 106. 제104항에 있어서, 상기 과다 발현 확률 및 상기 과소 발현 확률에 적용된 컷오프 확률에 기초하여, 상기 제2 훈련 세트에서의 상기 변이체는 유전자 발현 증가를 나타내는 제1 기준 진리 라벨을 갖는 과다 발현 변이체 훈련 세트, 유전자 발현 감소를 나타내는 제2 기준 진리 라벨을 갖는 과다 발현 변이체 훈련 세트, 및 유전자 발현 유지를 나타내는 신경 발현 변이체 훈련 세트로 분할되는, 인공 지능 기반 시스템. 106. An artificial intelligence-based system according to claim 104, wherein, based on the cutoff probabilities applied to the over-expression probability and the under-expression probability, the variants in the second training set are divided into a training set of over-expressed variants having a first reference truth label indicating increased gene expression, a training set of over-expressed variants having a second reference truth label indicating decreased gene expression, and a training set of neural expression variants indicating maintained gene expression. 107. 제10항에 있어서, 상기 유전자 발현 모델 및 상기 유전자 발현 출력 생성 로직은 먼저 상기 제1 훈련 세트에서 종단 간으로 훈련시킨 다음, 상기 제2 훈련 세트에서 재훈련되는, 인공 지능 기반 시스템. 107. An artificial intelligence-based system in claim 10, wherein the gene expression model and the gene expression output generation logic are first trained end-to-end in the first training set and then retrained in the second training set. 108. 제1항에 있어서, 상기 생물학적 양 모델 및 상기 생물학적 양 출력 생성 로직을 먼저 상기 제1 훈련 세트에서 종단 간으로 훈련시킨 다음, 홀수 염색체에서 발생하는 상기 제2 훈련 세트에서의 해당 변이체에 대해 재훈련시키는, 인공 지능 기반 시스템. 108. An artificial intelligence-based system, wherein in the first paragraph, the biological quantity model and the biological quantity output generation logic are first trained end-to-end in the first training set, and then retrained for the corresponding variants in the second training set occurring in odd chromosomes. 109. 제10항에 있어서, 상기 유전자 발현 모델 및 상기 유전자 발현 출력 생성 로직을 먼저 상기 제1 훈련 세트에서 종단 간으로 훈련시킨 다음, 홀수 염색체에서 발생하는 상기 제2 훈련 세트에서의 해당 변이체에 대해 재훈련시키는, 인공 지능 기반 시스템. 109. An artificial intelligence-based system in claim 10, wherein the gene expression model and the gene expression output generation logic are first trained end-to-end in the first training set, and then retrained for the corresponding variants occurring in odd chromosomes in the second training set. 110. 제85항에 있어서, 상기 제2 훈련 세트에서의 상기 변이체를 훈련을 위해 사용하지 않고 대신에 상기 훈련된 생물학적 양 모델(124), 상기 훈련된 생물학적 양 출력 생성 로직, 상기 훈련된 유전자 발현 모델, 및 상기 훈련된 유전자 발현 출력 생성 로직의 성능을 평가하기 위한 검증 세트로서 사용하는, 인공 지능 기반 시스템. 110. An artificial intelligence-based system, wherein the variants in the second training set are not used for training, but instead are used as a validation set to evaluate the performance of the trained biological quantity model (124), the trained biological quantity output generating logic, the trained gene expression model, and the trained gene expression output generating logic. 111. 제110항에 있어서, 짝수 염색체에서 발생하는 상기 제2 훈련 세트에서의 해당 변이체를 상기 검증 세트로 사용하는, 인공 지능 기반 시스템. 111. An artificial intelligence-based system, wherein the variants in the second training set occurring in even chromosomes are used as the validation set in clause 110. 112. 제1항에 있어서, 상기 타깃 염기 시퀀스의 크기는 전사 시작 부위(TSS)의 다양한 오프셋 위치를 고려하기 위해 훈련 동안 가변되는, 인공 지능 기반 시스템. 112. An artificial intelligence-based system, wherein the size of the target base sequence in the first aspect is varied during training to take into account various offset positions of the transcription start site (TSS). 113. 염기 해상도에서 유전자 발현의 변화를 검출하는 인공 지능 기반 시스템으로서, 113. An artificial intelligence-based system for detecting changes in gene expression at base resolution. 시퀀스 데이터베이스에 액세스하고 입력 염기 시퀀스를 생성하는 입력 생성 로직으로서, 상기 입력 염기 시퀀스는 타깃 염기 시퀀스를 포함하고, 상기 타깃 염기 시퀀스는 하류 컨텍스트 염기를 갖는 우측 염기 시퀀스 및 상류 컨텍스트 염기를 갖는 좌측 염기 시퀀스에 의해 플랭킹되는, 상기 입력 생성 로직;An input generating logic for accessing a sequence database and generating an input base sequence, wherein the input base sequence includes a target base sequence, and the target base sequence is flanked by a right base sequence having a downstream context base and a left base sequence having an upstream context base; 상기 입력 염기 시퀀스를 처리하고 상기 입력 염기 시퀀스의 대체 표현을 생성하는 생물학적 양 모델; 및A biological quantity model that processes the input base sequence and generates an alternative representation of the input base sequence; and 상기 입력 염기 시퀀스의 대체 표현을 처리하고 복수의 생물학적 양 출력 시퀀스를 생성하는 생물학적 양 출력 생성 로직을 포함하되,Comprising biological quantity output generation logic for processing alternative representations of the input base sequence and generating a plurality of biological quantity output sequences, 상기 복수의 생물학적 양 출력 시퀀스에서 각각의 생물학적 양 출력 시퀀스는 상기 타깃 염기 시퀀스에서의 각각의 타깃 염기에 대한 각각의 염기 당 생물학적 양 출력을 포함하는, 인공 지능 기반 시스템.An artificial intelligence-based system, wherein each biological quantity output sequence in the plurality of biological quantity output sequences comprises a biological quantity output per base for each target base in the target base sequence. 114. 제113항에 있어서, 상기 복수의 생물학적 양 출력 시퀀스에서의 제1 생물학적 양 출력 시퀀스는 상기 타깃 염기 시퀀스에서의 상기 각각의 타깃 염기에 대한 제1 각각의 염기 당 생물학적 양 출력을 포함하고, 114. In the 113th paragraph, the first biological quantity output sequence in the plurality of biological quantity output sequences comprises a first biological quantity output per base for each of the target bases in the target base sequence, 상기 제1 각각의 염기 당 생물학적 양 출력은 복수의 종에 걸쳐 상기 각각의 타깃 염기의 각각의 진화 보존 측정치를 특정하는, 인공 지능 기반 시스템.An artificial intelligence-based system wherein each of the first base-specific biological quantity outputs specifies a measure of evolutionary conservation of each of the target bases across multiple species. 115. 제113항에 있어서, 상기 복수의 생물학적 양 출력 시퀀스에서의 제2 생물학적 양 출력 시퀀스는 상기 타깃 염기 시퀀스에서의 상기 각각의 타깃 염기에 대한 제2 각각의 염기 당 생물학적 양 출력을 포함하고, 115. In the 113th paragraph, the second biological quantity output sequence in the plurality of biological quantity output sequences comprises a second respective base-wise biological quantity output for each target base in the target base sequence, 상기 제2 각각의 염기 당 생물학적 양 출력은 상기 타깃 염기 시퀀스에서의 각각의 위치에서 상기 각각의 타깃 염기의 각각의 전사 개시 측정치를 특정하는, 인공 지능 기반 시스템.An artificial intelligence-based system wherein each of the second respective base-specific biological quantity outputs specifies a measurement of each transcription initiation of each of the target bases at each position in the target base sequence. 조항 세트 2Set of clauses 2 1. 시스템으로서,1. As a system, 인과성 점수 세트를 갖는 변이체 세트를 갖는 검증 데이터로서, 인과성 점수는 유전자 발현을 변경시킬 수 있는 통계적으로 교란되지 않은 가능성을 특정하는, 상기 검증 데이터;Validation data having a set of variants each having a set of causality scores, wherein the causality scores specify a statistically unconfounded probability of altering gene expression; 상기 인과성 점수 세트에서의 각각의 인과성 점수를 상기 인과성 점수 세트에 대한 컷오프의 적용에 기초하여 유전자 발현 변경 클래스 또는 유전자 발현 보존 클래스로 분류하여, 상기 변이체 세트의 기준 진리 분기점을 상기 유전자 발현 변경 클래스 및 상기 유전자 발현 보존 클래스로 생성하도록 구성되는 검증 세트 이산화 로직;A validation set discretization logic configured to classify each causality score in the above causality score set into a gene-expression altered class or a gene-expression preserved class based on application of a cutoff to the above causality score set, and to generate a ground truth branch point of the variant set into the gene-expression altered class and the gene-expression preserved class; 모델이 상기 변이체 세트에 대한 예측 점수 세트를 생성하게 하도록 구성된 추론 로직으로서, 상기 예측 점수 세트에서의 예측 점수는 유전자 발현을 변경시킬 수 있는 추론된 가능성을 특정하고, 상기 모델을 훈련시켜 변이체의 유전자 발현 변경가능성을 결정하는, 상기 추론 로직;An inference logic configured to cause a model to generate a set of predicted scores for said set of variants, wherein a predicted score in said set of predicted scores specifies an inferred likelihood that gene expression can be altered, and wherein the inference logic trains the model to determine the likelihood that gene expression of the variant can be altered; 상기 예측 점수 세트에서의 각각의 예측 점수를 상기 예측 점수 세트에 대한 임계치의 적용에 기초하여 상기 유전자 발현 변경 클래스 또는 상기 유전자 발현 보존 클래스로의 분류하여, 상기 변이체 세트의 추론된 분기점을 상기 유전자 발현 변경 클래스 및 상기 유전자 발현 보존 클래스로 생성하도록 구성된 모델 점수 이산화 로직; 및A model score discretization logic configured to classify each prediction score in the set of prediction scores into the gene expression altered class or the gene expression preserved class based on application of a threshold to the set of prediction scores, thereby generating an inferred branch point of the set of variants into the gene expression altered class and the gene expression preserved class; and 상기 기준 진리 분기점에 대한 상기 추론된 분기점의 비교에 기초하여 상기 모델의 성능 측정을 결정하도록 구성된 검증 로직을 포함하는, 시스템.A system comprising verification logic configured to determine a performance measure of the model based on a comparison of the inferred branch point to the reference truth branch point. 2. 제1항에 있어서, 상기 기준 진리 분기점은 상기 유전자 발현 변경 클래스로 분류된 해당 변이체에 제1 라벨(예: 0)을 할당하고, 상기 유전자 발현 보존 클래스로 분류된 해당 변이체에 제2 라벨(예: 1)을 할당하는, 시스템.2. In the first paragraph, the system, wherein the reference truth branch point assigns a first label (e.g., 0) to the corresponding variant classified into the gene expression alteration class, and assigns a second label (e.g., 1) to the corresponding variant classified into the gene expression preservation class. 3. 제2항에 있어서, 상기 기준 진리 분기점은 상기 유전자 발현 변경 클래스로 분류된 해당 변이체에 제1 라벨(예: 0)을 할당하고, 상기 유전자 발현 보존 클래스로 분류된 해당 변이체에 제2 라벨(예: 1)을 할당하는, 시스템. 3. In the second paragraph, the system, wherein the reference truth branch point assigns a first label (e.g., 0) to the corresponding variant classified into the gene expression alteration class, and assigns a second label (e.g., 1) to the corresponding variant classified into the gene expression preservation class. 다른 구현에서, 기준 진리 분기점은 상기 변이체를 3개의 카테고리: -1, 0, 1로 분기하며, 이는 유전자 발현 감소, 유전자 발현에 변화 없음, 유전자 발현 증가에 해당한다. 즉, 단일 분류기는 이러한 구현에서 3-방향 분류를 할 수 있다.In another implementation, the ground truth branch points divide the variants into three categories: -1, 0, 1, corresponding to decreased gene expression, no change in gene expression, and increased gene expression. That is, a single classifier can do a three-way classification in this implementation. 4. 제3항에 있어서, 제1 벡터에서 상기 기준 진리 분기점을 인코딩하고, 제2 벡터에서 상기 추론된 분기점을 인코딩하도록 추가로 구성되는, 시스템. 4. A system in the third paragraph, further configured to encode the reference truth branch point in the first vector and to encode the inferred branch point in the second vector. 5. 제4항에 있어서, 상기 제1 벡터 및 상기 제2 벡터의 요소별 비교에 기초하여 상기 모델의 성능 측정을 결정하도록 추가로 구성되는, 시스템. 5. A system according to claim 4, further configured to determine a performance measurement of the model based on an element-wise comparison of the first vector and the second vector. 6. 제1항에 있어서, 상기 추론된 분기점에 의해 상기 유전자 발현 변경 클래스로 분류되는 변이체 수, 상기 추론된 분기점에 의해 상기 유전자 발현 보존 클래스로 분류되는 변이체의 수, 상기 기준 진리 분기점에 의해 상기 유전자 발현 변경 클래스로 분류되는 변이체의 수, 및 상기 기준 진리 분기점에 의해 상기 유전자 발현 보존 클래스로 분류되는 변이체의 수의 승산비에 기초하여 상기 모델의 성능 측정을 결정하도록 추가로 구성되는, 시스템. 6. A system further configured to determine a performance measure of the model based on an odds ratio of the number of variants classified into the gene expression alteration class by the inferred branch point, the number of variants classified into the gene expression conservation class by the inferred branch point, the number of variants classified into the gene expression alteration class by the reference truth branch point, and the number of variants classified into the gene expression conservation class by the reference truth branch point, in the first paragraph. 7. 제1항에 있어서, 상기 변이체 세트는 과소 발현 인과성 점수 세트를 가지며, 과소 발현 인과성 점수는 유전자 발현을 감소시킬 수 있는 통계적으로 교란되지 않은 가능성을 특정하는, 시스템. 7. A system according to claim 1, wherein the set of variants has a set of under-expression causality scores, wherein the under-expression causality scores specify a statistically unconfounded probability of reducing gene expression . 8. 제7항에 있어서, 상기 검증 세트 이산화 로직은 상기 과소 발현 인과성 점수 세트에서의 각각의 과소 발현 인과성 점수를 상기 과소 발현 인과성 점수 세트에 대한 과소 발현 컷오프의 적용에 기초하여 유전자 발현 감소 클래스 또는 유전자 발현 비감소 클래스로 분류하여, 상기 변이체 세트의 과소 발현 기준 진리 분기점을 상기 유전자 발현 감소 클래스 및 상기 유전자 발현 비감소 클래스로 생성하도록 추가로 구성되는, 시스템.8. In the 7th paragraph, the verification set discretization logic is further configured to classify each under-expression causality score in the set of under-expression causality scores into a gene expression reduction class or a gene expression non-reduced class based on the application of an under-expression cutoff to the set of under-expression causality scores, and generate an under-expression criterion truth branch point of the set of variants into the gene expression reduction class and the gene expression non-reduced class. 9. 제8항에 있어서, 상기 추론 로직은 상기 모델이 상기 변이체 세트에 대한 과소 발현 예측 점수 세트를 생성하게 하도록 추가로 구성되고, 상기 과소 발현 예측 점수 세트에서의 하나의 과소 발현 예측 점수는 유전자 발현을 감소시킬 수 있는 추론된 가능성을 특정하고, 상기 모델을 훈련시켜 변이체의 유전자 발현 감소가능성을 결정하고, 9. In the 8th paragraph, the inference logic is further configured to cause the model to generate a set of under-expression prediction scores for the set of variants, wherein one under-expression prediction score in the set of under-expression prediction scores specifies an inferred likelihood that gene expression can be reduced, and trains the model to determine the likelihood of gene expression reduction of the variant. 상기 모델 점수 이산화 로직은 상기 과소 발현 예측 점수 세트에서의 각각의 과소 발현 예측 점수를 상기 과소 발현 예측 점수 세트에 대한 과소 발현 임계치의 적용에 기초하여 상기 유전자 발현 감소 클래스 또는 상기 유전자 발현 비감소 클래스로 분류하여, 상기 변이체 세트의 과소 발현 추론된 분기점을 상기 유전자 발현 감소 클래스 및 상기 유전자 발현 비감소 클래스로 생성하도록 추가로 구성되고,The above model score discretization logic is further configured to classify each under-expression prediction score in the set of under-expression prediction scores into the gene expression reduction class or the gene expression non-reduced class based on application of an under-expression threshold to the set of under-expression prediction scores, thereby generating the under-expression inferred branch points of the variant set into the gene expression reduction class and the gene expression non-reduced class. 상기 검증 로직은 상기 과소 발현 기준 진리 분기점에 대한 상기 과소 발현 추론된 분기점의 비교에 기초하여 상기 모델의 과소 발현 성능 측정을 결정하도록 추가로 구성되는, 시스템.The system wherein the verification logic is further configured to determine an under-expression performance measure of the model based on a comparison of the under-expression inferred branch point to the under-expression criterion truth branch point. 10. 제9항에 있어서, 상기 모델 점수 이산화 로직은 상기 과소 발현 예측 점수 세트를 감소 순서로 정렬하고, 상기 정렬된 과소 발현 예측 점수 세트에서 N개의 가장 낮은 과소 발현 예측 점수 하위 세트를 상기 유전자 발현 감소 클래스로 분류하고, 상기 정렬된 과소 발현 예측 점수 세트에서 나머지 과소 발현 예측 점수 하위 세트를 상기 유전자 발현 비감소 클래스로 분류하도록 추가로 구성되는, 시스템. 10. The system of claim 9, wherein the model score discretization logic is further configured to sort the set of under-expression prediction scores in decreasing order, classify N lowest subsets of under-expression prediction scores from the sorted set of under-expression prediction scores into the gene-expression reduced class, and classify the remaining subsets of under-expression prediction scores from the sorted set of under-expression prediction scores into the gene-expression non-reduced class. 11. 제8항에 있어서, 상기 과소 발현 기준 진리 분기점은 상기 유전자 발현 감소 클래스로 분류된 해당 변이체에 제1 라벨(예: 0)을 할당하고, 상기 유전자 발현 비감소 클래스로 분류된 해당 변이체에 제2 라벨(예: 1)을 할당하는, 시스템. 11. In the 8th paragraph, the system, wherein the under-expression criterion truth branch point assigns a first label (e.g., 0) to the corresponding variant classified into the gene expression reduction class, and assigns a second label (e.g., 1) to the corresponding variant classified into the gene expression non-reduced class. 12. 제11항에 있어서, 상기 과소 발현 추론된 분기점은 상기 유전자 발현 감소 클래스로 분류된 해당 변이체에 상기 제1 라벨(예: 0)을 할당하고, 상기 유전자 발현 비감소 클래스로 분류된 해당 변이체에 상기 제2 라벨(예: 1)을 할당하는, 시스템. 12. In the 11th paragraph, the system inferred under-expression branch point assigns the first label (e.g., 0) to the corresponding variant classified into the gene expression reduction class, and assigns the second label (e.g., 1) to the corresponding variant classified into the gene expression non-reduced class. 13. 제12항에 있어서, 제1 벡터에서 상기 과소 발현 기준 진리 분기점을 인코딩하고, 제2 벡터에서 상기 추론된 분기점을 인코딩하도록 추가로 구성되는, 시스템. 13. A system according to claim 12, further configured to encode the underexpressed criterion truth branch point in the first vector and to encode the inferred branch point in the second vector. 14. 제13항에 있어서, 상기 제1 벡터 및 상기 제2 벡터의 요소별 비교에 기초하여 상기 모델의 과소 발현 성능 측정을 결정하도록 추가로 구성되는, 시스템. 14. A system according to claim 13, further configured to determine an under-expression performance measurement of the model based on an element-wise comparison of the first vector and the second vector. 15. 제9항에 있어서, 상기 과소 발현 추론된 분기점에 의해 상기 유전자 발현 감소 클래스로 분류되는 변이체 수, 상기 과소 발현 추론된 분기점에 의해 상기 유전자 발현 비감소 클래스로 분류되는 변이체의 수, 상기 과소 발현 기준 진리 분기점에 의해 상기 유전자 발현 감소 클래스로 분류되는 변이체의 수, 및 상기 과소 발현 기준 진리 분기점에 의해 상기 유전자 발현 비감소 클래스로 분류되는 변이체의 수의 승산비에 기초하여 상기 모델의 과소 발현 성능 측정을 결정하도록 추가로 구성되는, 시스템. 15. A system according to claim 9, further configured to determine an under-expression performance measure of the model based on an odds ratio of a number of variants classified into the gene expression reduction class by the under-expression inferred branch point, a number of variants classified into the gene expression non-reduced class by the under-expression inferred branch point, a number of variants classified into the gene expression reduction class by the under-expression criterion truth branch point, and a number of variants classified into the gene expression non-reduced class by the under-expression criterion truth branch point. 16. 제1항에 있어서, 상기 변이체 세트는 과다 발현 인과성 점수 세트를 가지며, 과다 발현 인과성 점수는 유전자 발현을 증가시킬 수 있는 통계적으로 교란되지 않은 가능성을 특정하는, 시스템. 16. A system according to claim 1, wherein the set of variants has a set of overexpression causality scores, wherein the overexpression causality scores specify a statistically unperturbed probability of increasing gene expression . 17. 제16항에 있어서, 상기 검증 세트 이산화 로직은 상기 과다 발현 인과성 점수 세트에서의 각각의 과다 발현 인과성 점수를 상기 과다 발현 인과성 점수 세트에 대한 과다 발현 컷오프의 적용에 기초하여 유전자 발현 증가 클래스 또는 유전자 발현 비증가 클래스로 분류하여, 상기 변이체 세트의 과다 발현 기준 진리 분기점을 상기 유전자 발현 증가 클래스 및 상기 유전자 발현 비증가 클래스로 생성하도록 추가로 구성되는, 시스템.17. In the 16th paragraph, the verification set discretization logic is further configured to classify each overexpression causality score in the overexpression causality score set into a gene expression increase class or a gene expression non-increase class based on the application of an overexpression cutoff to the overexpression causality score set, and generate an overexpression criterion truth branch point of the variant set into the gene expression increase class and the gene expression non-increase class. 18. 제17항에 있어서, 상기 추론 로직은 상기 모델이 상기 변이체 세트에 대한 과다 발현 예측 점수 세트를 생성하게 하도록 추가로 구성되고, 상기 과다 발현 예측 점수 세트에서의 하나의 과다 발현 예측 점수는 유전자 발현을 증가시킬 수 있는 추론된 가능성을 특정하고, 상기 모델을 훈련시켜 변이체의 유전자 발현 증가가능성을 결정하고, 18. In the 17th paragraph, the inference logic is further configured to cause the model to generate a set of overexpression prediction scores for the set of variants, wherein one overexpression prediction score in the set of overexpression prediction scores specifies an inferred likelihood that gene expression can be increased, and trains the model to determine the likelihood of increased gene expression of the variant. 상기 모델 점수 이산화 로직은 상기 과다 발현 예측 점수 세트에서의 각각의 과다 발현 예측 점수를 상기 과다 발현 예측 점수 세트에 대한 과다 발현 임계치의 적용에 기초하여 상기 유전자 발현 증가 클래스 또는 상기 유전자 발현 비증가 클래스로 분류하여, 상기 변이체 세트의 과다 발현 추론된 분기점을 상기 유전자 발현 증가 클래스 및 상기 유전자 발현 비증가 클래스로 생성하도록 추가로 구성되고,The above model score discretization logic is further configured to classify each over-expression prediction score in the above over-expression prediction score set into the gene expression increase class or the gene expression non-increase class based on application of an over-expression threshold to the above over-expression prediction score set, and generate the over-expression inferred branch points of the variant set into the gene expression increase class and the gene expression non-increase class. 상기 검증 로직은 상기 과다 발현 기준 진리 분기점에 대한 상기 과다 발현 추론된 분기점의 비교에 기초하여 상기 모델의 과다 발현 성능 측정을 결정하도록 추가로 구성되는, 시스템.The system wherein the verification logic is further configured to determine a measure of the overexpression performance of the model based on a comparison of the overexpression inferred branch point to the overexpression criterion truth branch point. 19. 제18항에 있어서, 상기 모델 점수 이산화 로직은 상기 과다 발현 예측 점수 세트를 감소 순서로 정렬하고, 상기 정렬된 과다 발현 예측 점수 세트에서 N개의 가장 높은 과다 발현 예측 점수 하위 세트를 상기 유전자 발현 증가 클래스로 분류하고, 상기 정렬된 과다 발현 예측 점수 세트에서 나머지 과다 발현 예측 점수 하위 세트를 상기 유전자 발현 비증가 클래스로 분류하도록 추가로 구성되는, 시스템. 19. The system of claim 18, wherein the model score discretization logic is further configured to sort the set of overexpression prediction scores in decreasing order, classify a subset of N highest overexpression prediction scores from the sorted set of overexpression prediction scores into the gene-expression increased class, and classify the remaining subset of overexpression prediction scores from the sorted set of overexpression prediction scores into the gene-expression non-increased class. 20. 제17항에 있어서, 상기 과다 발현 기준 진리 분기점은 상기 유전자 발현 증가 클래스로 분류된 해당 변이체에 제1 라벨(예: 0)을 할당하고, 상기 유전자 발현 비증가 클래스로 분류된 해당 변이체에 제2 라벨(예: 1)을 할당하는, 시스템. 20. In the 17th paragraph, the system, wherein the overexpression criterion truth branch point assigns a first label (e.g., 0) to the corresponding variant classified into the gene expression increase class, and assigns a second label (e.g., 1) to the corresponding variant classified into the gene expression non-increase class. 21. 제20항에 있어서, 상기 과다 발현 추론된 분기점은 상기 유전자 발현 증가 클래스로 분류된 해당 변이체에 상기 제1 라벨(예: 0)을 할당하고, 상기 유전자 발현 비증가 클래스로 분류된 해당 변이체에 상기 제2 라벨(예: 1)을 할당하는, 시스템. 21. A system in claim 20, wherein the over-expression inferred branch point assigns the first label (e.g., 0) to the corresponding variant classified into the gene expression increase class, and assigns the second label (e.g., 1) to the corresponding variant classified into the gene expression non-increase class. 22. 제21항에 있어서, 제1 벡터에서 상기 과다 발현 기준 진리 분기점을 인코딩하고, 제2 벡터에서 상기 추론된 분기점을 인코딩하도록 추가로 구성되는, 시스템. 22. A system according to claim 21, further configured to encode the overexpressed criterion truth branch point in the first vector, and to encode the inferred branch point in the second vector. 23. 제22항에 있어서, 상기 제1 벡터 및 상기 제2 벡터의 요소별 비교에 기초하여 상기 모델의 과다 발현 성능 측정을 결정하도록 추가로 구성되는, 시스템. 23. A system according to claim 22, further configured to determine an overexpression performance measurement of the model based on an element-wise comparison of the first vector and the second vector. 24. 제18항에 있어서, 상기 과다 발현 추론된 분기점에 의해 상기 유전자 발현 증가 클래스로 분류되는 변이체 수, 상기 과다 발현 추론된 분기점에 의해 상기 유전자 발현 비증가 클래스로 분류되는 변이체의 수, 상기 과다 발현 기준 진리 분기점에 의해 상기 유전자 발현 증가 클래스로 분류되는 변이체의 수, 및 상기 과다 발현 기준 진리 분기점에 의해 상기 유전자 발현 비증가 클래스로 분류되는 변이체의 수의 승산비에 기초하여 상기 모델의 과다 발현 성능 측정을 결정하도록 추가로 구성되는, 시스템. 24. A system according to claim 18, further configured to determine an overexpression performance measure of the model based on an odds ratio of a number of variants classified into the gene expression increase class by the overexpression inferred branch point, a number of variants classified into the gene expression non-increased class by the overexpression inferred branch point, a number of variants classified into the gene expression increase class by the overexpression criterion truth branch point, and a number of variants classified into the gene expression non-increased class by the overexpression criterion truth branch point. 25. 제1항에 있어서, 상기 추론 로직은 제1 모델이 상기 변이체 세트에 대한 제1 예측 점수 세트를 생성하게 하도록 추가로 구성되고, 상기 제1 예측 점수 세트에서의 하나의 예측 점수는 유전자 발현을 변경시킬 수 있는 추론된 가능성을 특정하고, 상기 제1 모델을 훈련시켜 변이체의 유전자 발현 변경가능성을 결정하고, 25. In the first paragraph, the inference logic is further configured to cause the first model to generate a first set of prediction scores for the set of variants, wherein one prediction score in the first set of prediction scores specifies an inferred likelihood of altering gene expression, and trains the first model to determine the likelihood of altering gene expression of the variant. 상기 모델 점수 이산화 로직은 상기 제1 예측 점수 세트에서의 각각의 예측 점수를 상기 제1 예측 점수 세트에 대한 제1 임계치의 적용에 기초하여 상기 유전자 발현 변경 클래스 또는 상기 유전자 발현 보존 클래스로 분류하여, 상기 변이체 세트의 제1 추론된 분기점을 상기 유전자 발현 변경 클래스 및 상기 유전자 발현 보존 클래스로 생성하도록 추가로 구성되고,The above model score discretization logic is further configured to classify each prediction score in the first prediction score set into the gene expression altered class or the gene expression preserved class based on application of a first threshold to the first prediction score set, thereby generating a first inferred branch point of the variant set into the gene expression altered class and the gene expression preserved class. 상기 검증 로직은 상기 기준 진리 분기점에 대한 상기 제1 추론된 분기점의 비교에 기초하여 상기 제1 모델의 제1 성능 측정을 결정하도록 추가로 구성되는, 시스템The system wherein the verification logic is further configured to determine a first performance measure of the first model based on a comparison of the first inferred branch point to the reference truth branch point. 26. 제25항에 있어서, 상기 추론 로직은 제2 모델이 상기 변이체 세트에 대한 제2 예측 점수 세트를 생성하게 하도록 추가로 구성되고, 상기 제2 예측 점수 세트에서의 하나의 예측 점수는 유전자 발현을 변경시킬 수 있는 추론된 가능성을 특정하고, 상기 제2 모델을 훈련시켜 변이체의 유전자 발현 변경가능성을 결정하고, 26. In the 25th paragraph, the inference logic is further configured to cause the second model to generate a second set of prediction scores for the set of variants, wherein one prediction score in the second set of prediction scores specifies an inferred likelihood of altering gene expression, and trains the second model to determine the likelihood of altering gene expression of the variant. 상기 모델 점수 이산화 로직은 상기 제2 예측 점수 세트에서의 각각의 예측 점수를 상기 제2 예측 점수 세트에 대한 제2 임계치의 적용에 기초하여 상기 유전자 발현 변경 클래스 또는 상기 유전자 발현 보존 클래스로 분류하여, 상기 변이체 세트의 제2 추론된 분기점을 상기 유전자 발현 변경 클래스 및 상기 유전자 발현 보존 클래스로 생성하도록 추가로 구성되고, 상기 검증 로직은 상기 기준 진리 분기점에 대한 상기 제2 추론된 분기점의 비교에 기초하여 상기 제2 모델의 제2 성능 측정을 결정하도록 추가로 구성되는, 시스템The system wherein the model score discretization logic is further configured to classify each prediction score in the second prediction score set into the gene expression altered class or the gene expression preserved class based on application of a second threshold to the second prediction score set, thereby generating a second inferred branch point of the variant set into the gene expression altered class and the gene expression preserved class, and the verification logic is further configured to determine a second performance measure of the second model based on a comparison of the second inferred branch point to the reference truth branch point. 27. 제26항에 있어서, 상기 추론 로직은 제3 모델이 상기 변이체 세트에 대한 제3 예측 점수 세트를 생성하게 하도록 추가로 구성되고, 상기 제3 예측 점수 세트에서의 하나의 예측 점수는 유전자 발현을 변경시킬 수 있는 추론된 가능성을 특정하고, 상기 제3 모델을 훈련시켜 변이체의 유전자 발현 변경가능성을 결정하고, 27. In the 26th paragraph, the inference logic is further configured to cause the third model to generate a third set of prediction scores for the set of variants, wherein one prediction score in the third set of prediction scores specifies an inferred likelihood of altering gene expression, and trains the third model to determine the likelihood of altering gene expression of the variant. 상기 모델 점수 이산화 로직은 상기 제3 예측 점수 세트에서의 각각의 예측 점수를 상기 제3 예측 점수 세트에 대한 제3 임계치의 적용에 기초하여 상기 유전자 발현 변경 클래스 또는 상기 유전자 발현 보존 클래스로 분류하여, 상기 변이체 세트의 제3 추론된 분기점을 상기 유전자 발현 변경 클래스 및 상기 유전자 발현 보존 클래스로 생성하도록 추가로 구성되고,The above model score discretization logic is further configured to classify each prediction score in the third prediction score set into the gene expression altered class or the gene expression preserved class based on application of a third threshold to the third prediction score set, thereby generating a third inferred branch point of the variant set into the gene expression altered class and the gene expression preserved class. 상기 검증 로직은 상기 기준 진리 분기점에 대한 상기 제3 추론된 분기점의 비교에 기초하여 상기 제3 모델의 제3 성능 측정을 결정하도록 추가로 구성되는, 시스템The system further comprises a verification logic configured to determine a third performance measure of the third model based on a comparison of the third inferred branch point to the reference truth branch point. 28. 제27항에 있어서, 상기 제1, 제2, 및 제3 추론된 분기점이 상기 유전자 발현 변경 클래스에서 동일한 수의 변이체를 분류하도록 요구함으로써, 상기 제1, 제2, 및 제3 성능 측정치를 서로 비교할 수 있게 하도록 추가로 구성되는, 시스템. 28. A system according to claim 27, further configured to allow the first, second, and third performance measures to be compared with each other by requiring that the first, second, and third inferred branch points classify an equal number of variants in the gene expression alteration class. 29. 제28항에 있어서, 상기 제1, 제2, 및 제3 성능 측정치의 비교에 기초하여 상기 검증 데이터에 대한 상기 제1, 제2, 및 제3 모델의 각각의 성능을 비교하도록 추가로 구성되는, 시스템. 29. A system according to claim 28, further configured to compare the performance of each of the first, second, and third models for the verification data based on the comparison of the first, second, and third performance measurements. 30. 제27항에 있어서, 상기 제1, 제2, 및 제3 임계치는 상이한, 시스템. 30. In the 27th paragraph, the first, second, and third thresholds are different, system. 31. 제27항에 있어서, 상기 제1, 제2, 및 제3 임계치의 적어도 일부는 동일한, 시스템. 31. In clause 27, the system wherein at least some of the first, second, and third thresholds are the same. 32. 제1항에 있어서, 상기 변이체 세트의 각각의 기준 진리 분기점을 상기 인과성 점수 세트에 대한 상이한 컷오프의 각각의 적용에 기초하여 상기 유전자 발현 변경 클래스 및 상기 유전자 발현 보존 클래스로 생성하도록 추가로 구성되는, 시스템. 32. A system according to claim 1, further configured to generate each reference truth branch point of the set of variants into the gene expression altered class and the gene expression preserved class based on each application of different cutoffs to the set of causality scores. 33. 제1항에 있어서, 상기 변이체 세트의 기준 진리 삼분지를 유전자 발현 감소 클래스, 유전자 발현 증가 클래스, 및 유전자 발현 보존 클래스로 생성하도록 추가로 구성되는, 시스템. 33. A system according to claim 1, further configured to generate a reference truth triad of the set of mutants into a gene expression decrease class, a gene expression increase class, and a gene expression preservation class. 34. 제33항에 있어서, 상기 변이체 세트의 추론된 삼분지를 상기 유전자 발현 감소 클래스, 상기 유전자 발현 증가 클래스, 및 상기 유전자 발현 보존 클래스로 생성하도록 추가로 구성되는, 시스템. 34. A system according to claim 33, further configured to generate inferred triplets of the set of variants into the gene expression decrease class, the gene expression increase class, and the gene expression preservation class . 35. 제1항에 있어서, 상기 변이체 세트에서의 각각의 변이체는 이상치 개체의 코호트 중 단지 하나의 이상치 개체에서 발생하는 단독 변이체이고,35. In the first paragraph, each variant in the set of variants is a single variant occurring in only one outlier individual among the cohort of outlier individuals, 상기 이상치 개체의 코호트에서 이상치 개체는 극한 수준의 유전자 발현을 나타내는, 시스템.A system in which outlier individuals in a cohort of outlier individuals exhibit extreme levels of gene expression. 36. 제35항에 있어서, 극한 수준의 유전자 발현은 정규화된 유전자 발현 수준의 꼬리 사분위로부터 결정되는, 시스템. 36. A system in accordance with claim 35, wherein the extreme levels of gene expression are determined from tail quartiles of normalized gene expression levels. 37. 제35항에 있어서, 극한 수준의 유전자 발현은 과다 유전자 발현 및 과소 유전자 발현을 포함하는, 시스템. 37. A system according to claim 35, wherein the extreme levels of gene expression include over-expression and under-expression of the gene. 38. 제35항에 있어서, 상기 단일 변이체는 코딩 변이체인, 시스템. 38. A system according to claim 35, wherein the single variant is a coding variant. 39. 제35항에 있어서, 상기 단일 변이체는 비-코딩 변이체인, 시스템. 39. A system according to claim 35, wherein the single variant is a non-coding variant. 40. 제39항에 있어서, 상기 비-코딩 변이체는 5개의 프라임 비변환 영역(UTR) 변이체, 3개의 프라임 UTR 변이체, 인핸서 변이체, 또는 프로모터 변이체인, 시스템. 40. A system in claim 39, wherein the non-coding variants are five prime untranslated region (UTR) variants, three prime UTR variants, an enhancer variant, or a promoter variant. 41. 제1항에 있어서, 상기 변이체 세트는 복수의 조직 유형에 걸쳐 있는, 시스템. 41. A system according to claim 1, wherein the set of mutants spans multiple tissue types. 42. 제1항에 있어서, 상기 변이체 세트는 복수의 셀 유형에 걸쳐 있는, 시스템. 42. A system according to claim 1, wherein the set of mutants spans multiple cell types. 43. 시스템으로서, 43. As a system, 기준 진리 점수 세트를 갖는 관측치 세트를 갖는 검증 데이터;Validation data having a set of observations with a set of ground truth scores; 상기 기준 진리 점수 세트에서의 각각의 기준 진리 점수를 상기 기준 진리 점수 세트에 대한 컷오프의 적용에 기초하여 제1 클래스 또는 제2 클래스로 분류하여, 상기 관측치 세트의 기준 진리 분기점을 상기 제1 클래스 및 상기 제2 클래스로 생성하도록 구성되는 검증 세트 이산화 로직;A validation set discretization logic configured to classify each reference truth score in the above reference truth score set into a first class or a second class based on application of a cutoff to the above reference truth score set, thereby generating reference truth branch points of the observation set into the first class and the second class; 훈련된 모델이 상기 관측치 세트에 대한 예측 점수 세트를 생성하게 하도록 구성되는 추론 로직;Inference logic configured to cause the trained model to generate a set of predicted scores for said set of observations; 상기 예측 점수 세트에서의 각각의 예측 점수를 상기 예측 점수 세트에 대한 임계치의 적용에 기초하여 상기 제1 클래스 또는 상기 제2 클래스에 분류하여, 상기 관측치 세트의 추론된 분기점을 상기 제1 클래스 및 상기 제2 클래스로 생성하도록 구성되는 모델 점수 이산화 로직; 및A model score discretization logic configured to classify each prediction score in the set of prediction scores into the first class or the second class based on application of a threshold to the set of prediction scores, thereby generating inferred branch points of the set of observations into the first class and the second class; and 상기 기준 진리 분기점에 대한 상기 추론된 분기점의 비교에 기초하여 상기 훈련된 모델의 성능 측정을 결정하도록 구성되는 검증 로직을 포함하는, 시스템.A system comprising verification logic configured to determine a performance measure of the trained model based on a comparison of the inferred branch point to the reference truth branch point. 44. 제43항에 있어서, 각각의 모델에 대한 각각의 추론된 분기점을 생성하여 상기 각각의 추론된 분기점 각각은 상기 제1 클래스에 대해 동일한 수의 관측치를 분류하도록 요구되고, 그에 의해 상기 각각의 모델의 각각의 성능 측정치를 서로 비교하게 하도록 추가로 구성되는, 시스템. 44. In the 43rd paragraph, the system is further configured to generate each inferred branch point for each model, each of said inferred branch points being required to classify the same number of observations to the first class, thereby allowing each performance measure of each of said respective models to be compared with each other . 45. 제44항에 있어서, 상기 각각의 성능 측정치의 비교에 기초하여 상기 검증 데이터에 대한 상기 각각의 모델의 각각의 성능을 비교하도록 추가로 구성되는, 시스템.45. In clause 44, a system further configured to compare the performance of each of the models for the verification data based on the comparison of each of the performance measurements. 본 발명이 상기에 상술된 바람직한 구현 및 예를 참조하여 개시되지만, 이러한 예는 제한적인 의미가 아니라 예시적인 것으로 의도됨이 이해될 것이다. 수정 및 조합이 당업자에게 쉽게 떠오를 것이며, 이러한 수정 및 조합은 본 발명의 사상 및 하기의 청구범위의 범주 내에 있을 것이라는 것이 고려된다.While the present invention has been disclosed with reference to the preferred embodiments and examples described above, it will be understood that such examples are intended to be illustrative rather than limiting. Modifications and combinations will readily occur to those skilled in the art, and it is contemplated that such modifications and combinations will fall within the spirit of the present invention and the scope of the claims below.

### Claims

Claims (117) Translated from Korean 염기 해상도에서 유전자 발현의 변화를 검출하는 인공 지능 기반 시스템으로서,시퀀스 데이터베이스에 액세스하고 입력 염기 시퀀스를 생성하는 입력 생성 로직으로서, 상기 입력 염기 시퀀스는 타깃 염기 시퀀스를 포함하고, 상기 타깃 염기 시퀀스는 하류 컨텍스트 염기를 갖는 우측 염기 시퀀스 및 상류 컨텍스트 염기를 갖는 좌측 염기 시퀀스에 의해 플랭킹되는, 상기 입력 생성 로직;상기 입력 염기 시퀀스를 처리하고 상기 입력 염기 시퀀스의 대체 표현을 생성하는 생물학적 양 모델; 및상기 입력 염기 시퀀스의 대체 표현을 처리하고 복수의 생물학적 양 출력 시퀀스를 생성하는 생물학적 양 출력 생성 로직을 포함하되, 상기 복수의 생물학적 양 출력 시퀀스에서 제1 생물학적 양 출력 시퀀스는 상기 타깃 염기 시퀀스에서의 각각의 타깃 염기에 대한 제1 각각의 염기 당 생물학적 양 출력을 포함하고, 상기 제1 각각의 염기 당 생물학적 양 출력은 복수의 종에 걸쳐 상기 각각의 타깃 염기의 각각의 진화 보존 측정치를 특정하고, 상기 복수의 생물학적 양 출력 시퀀스에서 제2 생물학적 양 출력 시퀀스는 상기 타깃 염기 시퀀스에서의 각각의 타깃 염기에 대한 제2 각각의 염기 당 생물학적 양 출력을 포함하고, 상기 제2 각각의 염기 당 생물학적 양 출력은 상기 타깃 염기 시퀀스에서의 각각의 위치에서 상기 각각의 타깃 염기의 각각의 전사 개시 측정치를 특정하는, 인공 지능 기반 시스템.An artificial intelligence-based system for detecting changes in gene expression at base resolution, An input generating logic for accessing a sequence database and generating an input base sequence, wherein the input base sequence includes a target base sequence, and the target base sequence is flanked by a right base sequence having a downstream context base and a left base sequence having an upstream context base; A biological quantity model that processes the input base sequence and generates an alternative representation of the input base sequence; and Comprising biological quantity output generation logic for processing alternative representations of the input base sequence and generating a plurality of biological quantity output sequences, In the above plurality of biological quantity output sequences, the first biological quantity output sequence comprises a first respective base-wise biological quantity output for each target base in the target base sequence, The biological quantity output per each of the first bases specifies a measure of evolutionary conservation of each of the target bases across multiple species, In the above plurality of biological quantity output sequences, the second biological quantity output sequence comprises a second respective base-wise biological quantity output for each target base in the target base sequence, An artificial intelligence-based system wherein each of the second respective base-specific biological quantity outputs specifies a measurement of each transcription initiation of each of the target bases at each position in the target base sequence. 제1항에 있어서, 상기 각각의 진화 보존 측정치는 신경 치환의 널 모델로부터의 편차를 특정하여 보존으로서 상기 타깃 염기 시퀀스에서의 주어진 위치에서 주어진 타깃 염기의 치환율의 감소를 검출하고, 가속으로서 상기 주어진 위치에서 상기 주어진 타깃 염기의 치환율의 증가를 검출하는 계통 P-값(phyloP) 점수인, 인공 지능 기반 시스템.In the first aspect, the artificial intelligence-based system, wherein each of the evolutionary conservation measures is a phyloP score that detects a decrease in the substitution rate of a given target base at a given position in the target base sequence as conservation by specifying a deviation from a null model of neural substitution, and an increase in the substitution rate of a given target base at a given position as acceleration. 제2항에 있어서, 상기 각각의 진화 보존 측정치는 보존된 상태 또는 비보존된 상태를 갖는 주어진 위치에서 주어진 목표 염기의 사후 확률을 특정하는 phastCons 점수인, 인공 지능 기반 시스템.An artificial intelligence-based system, wherein in the second paragraph, each of said evolutionary conservation measures is a phastCons score that specifies the posterior probability of a given target base at a given position having a conserved state or a non-conserved state. 제2항에 있어서, 상기 각각의 진화 보존 측정치는 상기 복수의 종에 걸쳐 상기 주어진 위치에서 상기 주어진 타깃 염기의 치환의 수 감소를 특정하는 게놈 진화 속도 프로파일링(GERP) 점수인, 인공 지능 기반 시스템.An artificial intelligence-based system in the second aspect, wherein each of said evolutionary conservation measures is a genome evolution rate profiling (GERP) score that specifies a decrease in the number of substitutions of a given target base at a given position across the plurality of species. 제1항에 있어서, 상기 각각의 전사 개시 측정치는 상기 주어진 위치에서 상기 주어진 타깃 염기의 전사 개시 빈도를 특정하는 유전자 발현(CAGE) 점수의 캡 분석인, 인공 지능 기반 시스템.An artificial intelligence-based system, wherein in the first aspect, each of the transcription initiation measurements is a cap analysis of gene expression (CAGE) score that specifies the transcription initiation frequency of the given target base at the given position. 제1항에 있어서, 상기 복수의 생물학적 양 출력 시퀀스에서의 제3 생물학적 양 출력 시퀀스는 상기 타깃 염기 시퀀스에서의 상기 각각의 타깃 염기에 대한 제3 각각의 염기 당 생물학적 양 출력을 포함하고,상기 제3 각각의 염기 당 생물학적 양 출력은 상기 타깃 염기 시퀀스에서의 각각의 위치에서 상기 각각의 타깃 염기의 각각의 후생적 신호 수준 측정치를 특정하는, 인공 지능 기반 시스템.In the first paragraph, the third biological quantity output sequence in the plurality of biological quantity output sequences includes a biological quantity output per third base for each of the target bases in the target base sequence, An artificial intelligence-based system wherein each of the third base-specific biological quantity outputs specifies a measurement of the level of each epigenetic signal of each target base at each position in the target base sequence. 제6항에 있어서, 상기 후생적 신호 수준은 DNase I-과민성 부위(DHS) 또는 시퀀싱을 이용한 트랜스포사제 접근가능 염색질에 대한 분석(ATAC-Seq)을 특정하는, 인공 지능 기반 시스템.In the sixth paragraph, the level of the epigenetic signal is an artificial intelligence-based system that specifies analysis of DNase I-hypersensitive sites (DHS) or transposase accessible chromatin using sequencing (ATAC-Seq). 제6항에 있어서, 상기 후생적 신호 수준은 전사 인자(TF) 결합을 특정하는, 인공 지능 기반 시스템.In the sixth paragraph, the artificial intelligence-based system, wherein the epigenetic signal level specifies transcription factor (TF) binding. 제6항에 있어서, 후생적 신호 수준은 히스톤 수정(HM) 마크를 특정하는, 인공 지능 기반 시스템.In the sixth paragraph, an artificial intelligence-based system, wherein the epigenetic signal level specifies a histone modification (HM) mark. 제1항에 있어서, 추가로:상기 복수의 생물학적 양 출력 시퀀스를 처리하고 상기 복수의 생물학적 양 출력 시퀀스의 대체 표현을 생성하는 유전자 발현 모델; 및상기 복수의 생물학적 양 출력 시퀀스의 대체 표현을 처리하고, 상기 타깃 염기 시퀀스에서의 상기 각각의 타깃 염기에 대한 각각의 염기 당 유전자 발현 출력의 유전자 발현 출력 시퀀스를 생성하는 유전자 발현 출력 생성 로직을 포함하도록 구성되되; 상기 주어진 위치에서 상기 주어진 타깃 염기에 대한 상기 유전자 발현 출력 시퀀스에서 주어진 염기 당 유전자 발현 출력은 상기 주어진 위치에서 상기 주어진 타깃 염기의 유전자 발현 수준의 척도를 특정하는, 인공 지능 기반 시스템.In paragraph 1, additionally: A gene expression model that processes the plurality of biological quantity output sequences and generates alternative representations of the plurality of biological quantity output sequences; and A gene expression output generating logic configured to process alternative representations of the above plural biological quantity output sequences and generate a gene expression output sequence of gene expression outputs per base for each of the target bases in the target base sequence; An artificial intelligence-based system, wherein the gene expression output per base in the gene expression output sequence for the given target base at the given position specifies a measure of the gene expression level of the given target base at the given position. 제10항에 있어서, 상기 유전자 발현 수준은 CAGE 전사 시작 부위(CTSS)와 같은 염기 당 메트릭으로 측정되는, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 10, wherein the gene expression level is measured by a per-base metric such as CAGE transcription start site (CTSS). 제10항에 있어서, 상기 유전자 발현 수준은 TPM(transcripts per million) 또는 RPKM(reads per kilobase of transcript)과 같은 유전자 당 메트릭으로 측정되는, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 10, wherein the gene expression level is measured by a per-gene metric such as transcripts per million (TPM) or reads per kilobase of transcript (RPKM). 제10항에 있어서, 상기 유전자 발현 수준은 FPKM(fragments per kilobase million)과 같은 유전자 당 메트릭으로 측정되는, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 10, wherein the gene expression level is measured by a per-gene metric such as fragments per kilobase million (FPKM). 제1항에 있어서, 변이체 분류 로직을 포함하도록 추가로 구성되는, 인공 지능 기반 시스템.An artificial intelligence-based system, further configured to include variant classification logic in the first paragraph. 제14항에 있어서, 상기 변이체 분류 로직은 상기 시퀀스 데이터베이스에 액세스하고 기준 염기 시퀀스를 생성하는 기준 입력 생성 로직을 포함하도록 추가로 구성되고, 상기 기준 염기 시퀀스는 기준 타깃 염기 시퀀스를 포함하고, 상기 기준 타깃 염기 시퀀스는 분석 중인 위치에서의 기준 염기를 포함하고, 상기 기준 염기는 하류 컨텍스트 염기를 갖는 우측 염기 시퀀스 및 상류 컨텍스트 염기를 갖는 좌측 염기 시퀀스에 의해 플랭킹되는, 인공 지능 기반 시스템.In the 14th paragraph, the variant classification logic is further configured to include a reference input generation logic that accesses the sequence database and generates a reference base sequence, the reference base sequence including a reference target base sequence, the reference target base sequence including a reference base at a position being analyzed, the reference base being flanked by a right base sequence having a downstream context base and a left base sequence having an upstream context base. An artificial intelligence-based system. 제15항에 있어서, 상기 변이체 분류 로직은 상기 시퀀스 데이터베이스에 액세스하고 대체 염기 시퀀스를 생성하는 대체 입력 생성 로직을 포함하도록 추가로 구성되고, 상기 대체 염기 시퀀스는 대체 타깃 염기 시퀀스를 포함하고, 상기 대체 타깃 염기 시퀀스는 상기 분석 중 위치에서 대체 염기를 포함하고, 상기 대체 염기는 상기 하류 컨텍스트 염기를 갖는 상기 우측 염기 시퀀스 및 상기 상류 컨텍스트 염기를 갖는 상기 좌측 염기 시퀀스에 의해 플랭킹되는, 인공 지능 기반 시스템.In the 15th paragraph, the variant classification logic is further configured to include a replacement input generating logic that accesses the sequence database and generates a replacement base sequence, the replacement base sequence including a replacement target base sequence, the replacement target base sequence including a replacement base at a position during the analysis, the replacement base being flanked by the right base sequence having the downstream context base and the left base sequence having the upstream context base. An artificial intelligence-based system. 제15항에 있어서, 상기 변이체 분류 로직은, 상기 생물학적 양 모델이 상기 기준 염기 시퀀스를 처리하고 상기 기준 염기 시퀀스의 대체 표현을 생성하게 하고, 추가로 상기 생물학적 양 출력 생성 로직이 상기 기준 염기 시퀀스의 대체 표현을 처리하고 복수의 기준 생물학적 양 출력 시퀀스를 생성하게 하는 기준 처리 로직을 포함하도록 추가로 구성되고,상기 복수의 기준 생물학적 양 출력 시퀀스에서 각각의 기준 생물학적 양 출력 시퀀스는 상기 기준 타깃 염기 시퀀스에서의 각각의 기준 타깃 염기에 대한 각각의 염기 당 기준 생물학적 양 출력을 포함하는, 인공 지능 기반 시스템.In the 15th paragraph, the variant classification logic is further configured to include reference processing logic that causes the biological quantity model to process the reference base sequence and generate an alternative representation of the reference base sequence, and further causes the biological quantity output generation logic to process the alternative representation of the reference base sequence and generate a plurality of reference biological quantity output sequences. An artificial intelligence-based system, wherein each of the plurality of reference biological quantity output sequences includes a reference biological quantity output per base for each reference target base in the reference target base sequence. 제17항에 있어서, 상기 복수의 기준 생물학적 양 출력 시퀀스에서의 제1 기준 생물학적 양 출력 시퀀스는 상기 기준 타깃 염기 시퀀스에서의 각각의 기준 타깃 염기에 대한 제1 각각의 염기 당 기준 생물학적 양 출력을 포함하고,상기 제1 각각의 염기 당 기준 생물학적 양 출력은 상기 복수의 종에 걸쳐 상기 각각의 기준 타깃 염기의 각각의 진화 보존 측정치를 특정하는, 인공 지능 기반 시스템.In the 17th paragraph, the first reference biological quantity output sequence in the plurality of reference biological quantity output sequences comprises a first respective base-specific reference biological quantity output for each reference target base in the reference target base sequence, An artificial intelligence-based system wherein each of the first base-specific reference biological quantity outputs specifies a measure of evolutionary conservation of each of the reference target bases across the plurality of species. 제17항에 있어서, 상기 복수의 기준 생물학적 양 출력 시퀀스에서의 제2 기준 생물학적 양 출력 시퀀스는 상기 기준 타깃 염기 시퀀스에서의 상기 각각의 기준 타깃 염기에 대한 제2 각각의 염기 당 기준 생물학적 양 출력을 포함하고,상기 제2 각각의 염기 당 기준 생물학적 양 출력은 상기 기준 타깃 염기 시퀀스에서의 각각의 위치에서 상기 각각의 기준 타깃 염기의 각각의 전사 개시 측정치를 특정하는, 인공 지능 기반 시스템.In the 17th paragraph, the second reference biological quantity output sequence in the plurality of reference biological quantity output sequences comprises a second respective base-specific reference biological quantity output for each of the reference target bases in the reference target base sequence, An artificial intelligence-based system wherein each of the second respective base-specific biological quantity outputs specifies a respective transcription initiation measurement of each of the respective reference target bases at each position in the reference target base sequence. 제16항에 있어서, 상기 변이체 분류 로직은, 상기 생물학적 양 모델이 상기 대체 염기 시퀀스를 처리하고 상기 대체 염기 시퀀스의 대체 표현을 생성하게 하고, 추가로 상기 생물학적 양 출력 생성 로직이 상기 대체 염기 시퀀스의 대체 표현을 처리하고 복수의 대체 생물학적 양 출력 시퀀스를 생성하게 하는 대체 처리 로직을 포함하도록 추가로 구성되고,상기 복수의 대체 생물학적 양 출력 시퀀스에서 각각의 대체 생물학적 양 출력 시퀀스는 상기 대체 타깃 염기 시퀀스에서의 각각의 대체 타깃 염기에 대한 각각의 염기 당 대체 생물학적 양 출력을 포함하는, 인공 지능 기반 시스템.In the 16th paragraph, the variant classification logic is further configured to include replacement processing logic that causes the biological quantity model to process the replacement base sequence and generate a replacement representation of the replacement base sequence, and further causes the biological quantity output generation logic to process the replacement representation of the replacement base sequence and generate a plurality of replacement biological quantity output sequences. An artificial intelligence-based system, wherein each alternative biological quantity output sequence in said plurality of alternative biological quantity output sequences comprises an alternative biological quantity output per base for each alternative target base in said alternative target base sequence. 제20항에 있어서, 상기 복수의 대체 생물학적 양 출력 시퀀스에서의 제1 대체 생물학적 양 출력 시퀀스는 상기 대체 타깃 염기 시퀀스에서의 각각의 대체 타깃 염기에 대한 제1 각각의 염기 당 대체 생물학적 양 출력을 포함하고,상기 제1 각각의 염기 당 대체 생물학적 양 출력은 상기 복수의 종에 걸쳐 상기 각각의 대체 타깃 염기의 각각의 진화 보존 측정치를 특정하는, 인공 지능 기반 시스템.In the 20th paragraph, the first alternative biological quantity output sequence in the plurality of alternative biological quantity output sequences comprises a first respective base-wise alternative biological quantity output for each alternative target base in the alternative target base sequence, An artificial intelligence-based system wherein each of the first respective base alternative biological quantity outputs specifies a measure of evolutionary conservation of each of the respective alternative target bases across the plurality of species. 제20항에 있어서, 상기 복수의 대체 생물학적 양 출력 시퀀스에서의 제2 대체 생물학적 양 출력 시퀀스는 상기 대체 타깃 염기 시퀀스에서의 상기 각각의 대체 타깃 염기에 대한 제2 각각의 염기 당 대체 생물학적 양 출력을 포함하고,상기 제2 각각의 염기 당 대체 생물학적 양 출력은 상기 대체 타깃 염기 시퀀스에서의 각각의 위치에서 상기 각각의 대체 타깃 염기의 각각의 전사 개시 측정치를 특정하는, 인공 지능 기반 시스템.In the 20th paragraph, the second alternative biological quantity output sequence in the plurality of alternative biological quantity output sequences comprises a second respective base-wise alternative biological quantity output for each of the alternative target bases in the alternative target base sequence, An artificial intelligence-based system wherein each of the second respective base-specific alternative biological quantity outputs specifies a respective transcription initiation measurement of each of the respective alternative target bases at each position in the alternative target base sequence. 제20항에 있어서, 상기 변이체 분류 로직은 상기 제1 기준 생물학적 양 출력 시퀀스 및 상기 제1 대체 생물학적 양 출력 시퀀스를 위치별로 비교하고, 상기 제1 기준 생물학적 양 출력 시퀀스 및 상기 제1 대체 생물학적 양 출력 시퀀스에서의 위치에 대한 제1 위치별 시퀀스 차이를 갖는 제1 델타 시퀀스를 생성하는 병원성 예측 로직을 포함하도록 추가로 구성되는, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 20, wherein the variant classification logic is further configured to include pathogenicity prediction logic that compares the first reference biological quantity output sequence and the first alternative biological quantity output sequence by position, and generates a first delta sequence having a first position-by-position sequence difference with respect to a position in the first reference biological quantity output sequence and the first alternative biological quantity output sequence. 제23항에 있어서, 상기 병원성 예측 로직은 상기 제2 기준 생물학적 양 출력 시퀀스 및 상기 제2 대체 생물학적 양 출력 시퀀스를 위치별로 비교하고, 상기 제2 기준 생물학적 양 출력 시퀀스 및 상기 제2 대체 생물학적 양 출력 시퀀스에서의 위치에 대한 제2 위치별 시퀀스 차이를 갖는 제2 델타 시퀀스를 생성하도록 추가로 구성되는, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 23, wherein the pathogenicity prediction logic is further configured to compare the second reference biological quantity output sequence and the second alternative biological quantity output sequence by position, and generate a second delta sequence having a second position-wise sequence difference with respect to a position in the second reference biological quantity output sequence and the second alternative biological quantity output sequence. 제24항에 있어서, 상기 병원성 예측 로직은 상기 제1 델타 시퀀스 및 상기 제2 델타 시퀀스에 의존하여 상기 대체 염기에 대한 병원성 예측을 생성하도록 추가로 구성되는, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 24, wherein the pathogenicity prediction logic is further configured to generate a pathogenicity prediction for the substituted base based on the first delta sequence and the second delta sequence. 제24항에 있어서, 상기 병원성 예측 로직은 상기 제1 위치별 시퀀스 차이를 제1 누적 시퀀스 값으로 누적하고, 상기 제2 위치별 시퀀스 차이를 제2 누적 시퀀스 값으로 누적하도록 추가로 구성되는, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 24, wherein the pathogenicity prediction logic is further configured to accumulate the first location-specific sequence difference as a first accumulated sequence value, and to accumulate the second location-specific sequence difference as a second accumulated sequence value. 제26항에 있어서, 상기 제1 누적 시퀀스 값은 상기 제1 위치별 시퀀스 차이의 평균이고, 상기 제2 누적 시퀀스 값은 상기 제2 위치별 시퀀스 차이의 평균인, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 26, wherein the first accumulated sequence value is an average of the sequence differences by the first location, and the second accumulated sequence value is an average of the sequence differences by the second location. 제26항에 있어서, 상기 제1 누적 시퀀스 값은 상기 제1 위치별 시퀀스 차이의 합이고, 상기 제2 누적 시퀀스 값은 상기 제2 위치별 시퀀스 차이의 합인, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 26, wherein the first accumulated sequence value is a sum of the sequence differences for each first location, and the second accumulated sequence value is a sum of the sequence differences for each second location. 제26항에 있어서, 상기 병원성 예측 로직은 상기 제1 누적 시퀀스 값 및 상기 제2 누적 시퀀스 값에 의존하여 상기 대체 염기에 대한 상기 병원성 예측을 생성하도록 추가로 구성되는, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 26, wherein the pathogenicity prediction logic is further configured to generate the pathogenicity prediction for the replacement base based on the first cumulative sequence value and the second cumulative sequence value. 제29항에 있어서, 상기 병원성 예측 로직은 상기 제1 누적 시퀀스 값 및 상기 제2 누적 시퀀스 값의 평균에 의존하여 상기 대체 염기에 대한 상기 병원성 예측을 생성하도록 추가로 구성되는, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 29, wherein the pathogenicity prediction logic is further configured to generate the pathogenicity prediction for the replacement base based on an average of the first cumulative sequence value and the second cumulative sequence value. 제29항에 있어서, 상기 병원성 예측 로직은 상기 제1 누적 시퀀스 값 및 상기 제2 누적 시퀀스 값의 합에 의존하여 상기 대체 염기에 대한 상기 병원성 예측을 생성하도록 추가로 구성되는, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 29, wherein the pathogenicity prediction logic is further configured to generate the pathogenicity prediction for the replacement base based on a sum of the first cumulative sequence value and the second cumulative sequence value. 제23항에 있어서, 상기 병원성 예측 로직은 상기 제1 위치별 시퀀스 차이에 기초하여 상기 제1 델타 시퀀스에서의 위치를 보존된 상태 또는 비보존된 상태에 속하는 것으로 분류하도록 추가로 구성되는, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 23, wherein the pathogenicity prediction logic is further configured to classify a position in the first delta sequence as belonging to a conserved state or a non-conserved state based on the first position-specific sequence difference. 제32항에 있어서, 상기 병원성 예측 로직은 상기 제2 델타 시퀀스에서의 해당 위치를 상기 보존 상태에 속하는 것으로 분류된 상기 제1 델타 시퀀스에서의 해당 위치와 일치하는 신호 상태에 속하는 것으로 분류하고, 상기 제2 델타 시퀀스에서의 해당 위치를 상기 비보존 상태에 속하는 것으로 분류된 상기 제1 델타 시퀀스에서의 해당 위치와 일치하는 노이즈 상태에 속하는 것으로 분류하도록 추가로 구성되는, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 32, wherein the pathogenicity prediction logic is further configured to classify a corresponding position in the second delta sequence as belonging to a signal state matching a corresponding position in the first delta sequence classified as belonging to the preserved state, and to classify a corresponding position in the second delta sequence as belonging to a noise state matching a corresponding position in the first delta sequence classified as belonging to the non-preserved state. 제33항에 있어서, 상기 병원성 예측 로직은 상기 제2 위치별 시퀀스 차이의 하위 세트를 변조된 누적 시퀀스 값으로 누적하도록 추가로 구성되고,상기 제2 위치별 시퀀스 차이의 하위 세트에서의 제2 위치별 시퀀스 차이는 상기 신호 상태에 속하는 것으로 분류되는 상기 제2 델타 시퀀스에서의 해당 위치에 위치하는, 인공 지능 기반 시스템.In the 33rd paragraph, the pathogenicity prediction logic is further configured to accumulate a subset of the second location-specific sequence differences into a modulated cumulative sequence value, An artificial intelligence-based system wherein a second positional sequence difference in a subset of the second positional sequence differences is located at a corresponding position in the second delta sequence that is classified as belonging to the signal state. 제34항에 있어서, 상기 병원성 예측 로직은 상기 변조된 누적 시퀀스 값에 의존하여 상기 대체 염기에 대한 상기 병원성 예측을 생성하도록 추가로 구성되는, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 34, wherein the pathogenicity prediction logic is further configured to generate the pathogenicity prediction for the substituted base based on the modified cumulative sequence value. 제34항에 있어서, 상기 변조된 누적 시퀀스 값은 상기 제2 위치별 시퀀스 차이의 하위 세트에서의 상기 제2 위치별 시퀀스 차이의 평균인, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 34, wherein the modulated accumulated sequence value is an average of the second position-specific sequence differences in a subset of the second position-specific sequence differences. 제34항에 있어서, 상기 변조된 누적 시퀀스 값은 상기 제2 위치별 시퀀스 차이의 하위 세트에서의 상기 제2 위치별 시퀀스 차이의 합인, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 34, wherein the modulated accumulated sequence value is a sum of the second position-specific sequence differences in a subset of the second position-specific sequence differences. 제24항에 있어서, 상기 병원성 예측 로직은 상기 제1 기준 생물학적 양 출력 시퀀스 및 상기 제1 대체 생물학적 양 출력 시퀀스의 각각의 부분을 위치별로 비교하고, 상기 각각의 부분에서의 위치에 대해 제1 위치별 하위 시퀀스 차이를 갖는 제1 델타 하위 시퀀스를 생성하도록 추가로 구성되는, 인공 지능 기반 시스템.In claim 24, the artificial intelligence-based system further comprises: the pathogenicity prediction logic configured to compare, position-wise, each portion of the first reference biological quantity output sequence and the first alternative biological quantity output sequence, and generate a first delta subsequence having a first position-wise subsequence difference for each portion. 제38항에 있어서, 상기 병원성 예측 로직은 상기 제2 기준 생물학적 양 출력 시퀀스 및 상기 제2 대체 생물학적 양 출력 시퀀스의 각각의 부분을 위치별로 비교하고, 상기 각각의 부분에서의 위치에 대해 제2 위치별 하위 시퀀스 차이를 갖는 제2 델타 하위 시퀀스를 생성하도록 추가로 구성되는, 인공 지능 기반 시스템.In claim 38, the artificial intelligence-based system is further configured to: compare, by position, each portion of the second reference biological quantity output sequence and the second alternative biological quantity output sequence; and generate a second delta subsequence having a second position-wise subsequence difference for each portion. 제39항에 있어서, 상기 각각의 부분은 상기 분석 중 위치 주위의 우측 및 좌측 플랭킹 위치에 걸쳐 있는, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 39, wherein each of said portions spans right and left flanking locations around said analysis location. 제40항에 있어서, 상기 병원성 예측 로직은 상기 제1 델타 하위 시퀀스 및 상기 제2 델타 하위 시퀀스에 의존하여 상기 대체 염기에 대한 병원성 예측을 생성하도록 추가로 구성되는, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 40, wherein the pathogenicity prediction logic is further configured to generate a pathogenicity prediction for the substituted base based on the first delta subsequence and the second delta subsequence. 제40항에 있어서, 상기 병원성 예측 로직은 상기 제1 위치별 하위 시퀀스 차이를 제1 누적된 하위 시퀀스 값으로 누적하고, 상기 제2 위치별 하위 시퀀스 차이를 제2 누적된 하위 시퀀스 값으로 누적하도록 추가로 구성되는, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 40, wherein the pathogenicity prediction logic is further configured to accumulate the first location-specific sub-sequence difference as a first accumulated sub-sequence value, and to accumulate the second location-specific sub-sequence difference as a second accumulated sub-sequence value. 제42항에 있어서, 상기 제1 누적 하위 시퀀스 값은 상기 제1 위치별 하위 시퀀스 차이의 평균이고, 상기 제2 누적 하위 시퀀스 값은 상기 제2 위치별 하위 시퀀스 차이의 평균인, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 42, wherein the first cumulative sub-sequence value is an average of the sub-sequence differences by the first location, and the second cumulative sub-sequence value is an average of the sub-sequence differences by the second location. 제42항에 있어서, 상기 제1 누적 하위 시퀀스 값은 상기 제1 위치별 하위 시퀀스 차이의 합이고, 상기 제2 누적 하위 시퀀스 값은 상기 제2 위치별 하위 시퀀스 차이의 합인, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 42, wherein the first cumulative sub-sequence value is a sum of the sub-sequence differences for each first location, and the second cumulative sub-sequence value is a sum of the sub-sequence differences for each second location. 제42항에 있어서, 상기 병원성 예측 로직은 상기 제1 축적된 하위 시퀀스 값 및 상기 제2 축적된 하위 시퀀스 값에 의존하여 상기 대체 염기에 대한 상기 병원성 예측을 생성하도록 추가로 구성되는, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 42, wherein the pathogenicity prediction logic is further configured to generate the pathogenicity prediction for the substituted base based on the first accumulated sub-sequence value and the second accumulated sub-sequence value. 제45항에 있어서, 상기 병원성 예측 로직은 상기 제1 누적 하위 시퀀스 값 및 상기 제2 누적 하위 시퀀스 값의 평균에 의존하여 상기 대체 염기에 대한 상기 병원성 예측을 생성하도록 추가로 구성되는, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 45, wherein the pathogenicity prediction logic is further configured to generate the pathogenicity prediction for the replacement base by relying on an average of the first cumulative subsequence value and the second cumulative subsequence value. 제45항에 있어서, 상기 병원성 예측 로직은 상기 제1 누적 하위 시퀀스 값 및 상기 제2 누적 하위 시퀀스 값의 합에 의존하여 상기 대체 염기에 대한 상기 병원성 예측을 생성하도록 추가로 구성되는, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 45, wherein the pathogenicity prediction logic is further configured to generate the pathogenicity prediction for the replacement base based on a sum of the first cumulative subsequence value and the second cumulative subsequence value. 제38항에 있어서, 상기 병원성 예측 로직은 상기 제1 위치별 하위 시퀀스 차이에 기초하여 상기 제1 델타 하위 시퀀스에서의 위치를 보존된 상태 또는 비보존된 상태에 속하는 것으로 분류하도록 추가로 구성되는, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 38, wherein the pathogenicity prediction logic is further configured to classify a position in the first delta subsequence as belonging to a conserved state or a non-conserved state based on the subsequence difference by the first position. 제48항에 있어서, 상기 병원성 예측 로직은 상기 제2 델타 하위 시퀀스에서의 해당 위치를 상기 보존 상태에 속하는 것으로 분류된 상기 제1 델타 하위 시퀀스에서의 해당 위치와 일치하는 신호 상태에 속하는 것으로 분류하고, 상기 제2 델타 하위 시퀀스에서의 해당 위치를 상기 비보존 상태에 속하는 것으로 분류된 상기 제1 델타 하위 시퀀스에서의 해당 위치와 일치하는 노이즈 상태에 속하는 것으로 분류하도록 추가로 구성되는, 인공 지능 기반 시스템.In claim 48, the artificial intelligence-based system is further configured to classify the corresponding position in the second delta subsequence as belonging to a signal state matching the corresponding position in the first delta subsequence classified as belonging to the preserved state, and to classify the corresponding position in the second delta subsequence as belonging to a noise state matching the corresponding position in the first delta subsequence classified as belonging to the non-preserved state. 제49항에 있어서, 상기 병원성 예측 로직은 상기 제2 위치별 하위 시퀀스 차이의 하위 세트를 변조된 누적 하위 시퀀스 값으로 누적하도록 추가로 구성되고,상기 제2 위치별 하위 시퀀스 차이의 하위 세트에서의 제2 위치별 하위 시퀀스 차이는 상기 신호 상태에 속하는 것으로 분류되는 상기 제2 델타 하위 시퀀스에서의 해당 위치에 위치하는, 인공 지능 기반 시스템.In clause 49, the pathogenicity prediction logic is further configured to accumulate a subset of the second position-specific subsequence differences into modulated cumulative subsequence values, An artificial intelligence-based system, wherein the second positional sub-sequence difference in a subset of the second positional sub-sequence differences is located at a corresponding position in the second delta sub-sequence that is classified as belonging to the signal state. 제50항에 있어서, 상기 병원성 예측 로직은 상기 변조된 누적 하위 시퀀스 값에 의존하여 상기 대체 염기에 대한 상기 병원성 예측을 생성하도록 추가로 구성되는, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 50, wherein the pathogenicity prediction logic is further configured to generate the pathogenicity prediction for the substituted base based on the modified cumulative subsequence value. 제50항에 있어서, 상기 변조된 누적 하위 시퀀스 값은 상기 제2 위치별 하위 시퀀스 차이의 하위 세트에서의 상기 제2 위치별 하위 시퀀스 차이의 평균인, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 50, wherein the modulated accumulated sub-sequence value is an average of the second position-specific sub-sequence differences in a subset of the second position-specific sub-sequence differences. 제50항에 있어서, 상기 변조된 누적 하위 시퀀스 값은 상기 제2 위치별 하위 시퀀스 차이의 하위 세트에서의 상기 제2 위치별 하위 시퀀스 차이의 합인, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 50, wherein the modulated accumulated sub-sequence value is a sum of the second position-wise sub-sequence differences in a subset of the second position-wise sub-sequence differences. 제1항에 있어서, 상기 타깃 염기 시퀀스는 유전자의 코딩 영역인, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 1, wherein the target base sequence is a coding region of a gene. 제1항에 있어서, 상기 타깃 염기 시퀀스는 유전자의 비-코딩 영역인, 인공 지능 기반 시스템.An artificial intelligence-based system, wherein the target base sequence is a non-coding region of a gene. 제55항에 있어서, 상기 비-코딩 영역은 전사 시작 부위, 5개의 프라임 비변환 영역(UTR), 3개의 프라임 UTR, 인핸서 및 프로모터에 걸쳐 있는, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 55, wherein the non-coding region spans a transcription start site, five prime untranslated regions (UTRs), three prime UTRs, an enhancer, and a promoter. 제16항에 있어서, 상기 대체 염기는 이상치 개체의 코호트 중 하나의 이상치 개체에서만 발생하는 단일 변이체이고,이상치 개체의 코호트에서 이상치 개체는 극한 수준의 유전자 발현을 나타내는, 인공 지능 기반 시스템.In claim 16, the replacement base is a single variant occurring only in one outlier individual among the cohort of outlier individuals, An artificial intelligence-based system that identifies outliers in a cohort of outlier individuals, where the outlier individuals exhibit extreme levels of gene expression. 제56항에 있어서, 극한 수준의 유전자 발현은 정규화된 유전자 발현 수준의 꼬리 사분위로부터 결정되는, 인공 지능 기반 시스템.An artificial intelligence-based system, wherein in clause 56, extreme levels of gene expression are determined from tail quartiles of normalized gene expression levels. 제57항에 있어서, 극한 수준의 유전자 발현은 과다 유전자 발현 및 과소 유전자 발현을 포함하는, 인공 지능 기반 시스템.An artificial intelligence-based system, wherein extreme levels of gene expression in clause 57 include over-expression and under-expression of gene. 제56항에 있어서, 상기 단일 변이체는 코딩 변이체인, 인공 지능 기반 시스템.An artificial intelligence-based system, wherein the single variant is a coding variant in claim 56. 제56항에 있어서, 상기 단일 변이체는 비-코딩 변이체인, 인공 지능 기반 시스템.An artificial intelligence-based system, wherein the single variant in claim 56 is a non-coding variant. 제60항에 있어서, 상기 비-코딩 변이체는 프로모터 변이체인, 인공 지능 기반 시스템.An artificial intelligence-based system, wherein the non-coding variant is a promoter variant in claim 60. 제60항에 있어서, 상기 비-코딩 변이체는 인핸서 변이체인, 인공 지능 기반 시스템.An artificial intelligence-based system, wherein the non-coding variant is an enhancer variant in claim 60. 제1항에 있어서, 상기 생물학적 양 모델은 제1 가중치 세트를 갖고,상기 생물학적 양 출력 생성 로직은 제2 가중치 세트를 갖는, 인공 지능 기반 시스템.In the first paragraph, the biological quantity model has a first weight set, An artificial intelligence-based system wherein the above biological quantity output generating logic has a second set of weights. 제62항에 있어서, 훈련 동안, 상기 생물학적 양 모델의 제1 가중치 세트를 처음부터 훈련시켜 상기 입력 염기 시퀀스를 처리하고 상기 입력 염기 시퀀스의 대체 표현을 생성하고,상기 생물학적 양 출력 생성 로직의 제2 가중치 세트를 상기 생물학적 양 모델의 제1 가중치 세트로 처음부터 그리고 종단 간으로 훈련시켜 상기 입력 염기 시퀀스의 대체 표현을 처리하고 상기 복수의 생물학적 양 출력 시퀀스를 생성하는, 인공 지능 기반 시스템.In paragraph 62, during training, the first weight set of the biological quantity model is trained from scratch to process the input base sequence and generate an alternative representation of the input base sequence, An artificial intelligence-based system, wherein the second set of weights of the biological quantity output generating logic is trained from the beginning and end-to-end with the first set of weights of the biological quantity model to process alternative representations of the input base sequence and generate the plurality of biological quantity output sequences. 제63항에 있어서, 추론 동안, 상기 생물학적 양 모델은 상기 훈련된 제1 가중치 세트를 사용하고,상기 추론 동안, 상기 생물학적 양 출력 생성 로직은 상기 훈련된 제2 가중치 세트를 사용하는, 인공 지능 기반 시스템.In clause 63, during inference, the biological quantity model uses the trained first weight set, During said inference, said biological quantity output generating logic is an artificial intelligence-based system using said trained second weight set. 제1항에 있어서, 상기 유전자 발현 모델은 제3 가중치 세트를 갖고,상기 유전자 발현 출력 생성 로직은 제4 가중치 세트를 갖는, 인공 지능 기반 시스템.In the first paragraph, the gene expression model has a third weight set, The above gene expression output generation logic is an artificial intelligence-based system having a fourth weight set. 제65항에 있어서, 상기 유전자 발현 모델의 제3 가중치 세트를 처음부터 훈련시켜 상기 복수의 생물학적 양 출력 시퀀스를 처리하고 상기 복수의 생물학적 양 출력 시퀀스의 대체 표현을 생성하고,상기 유전자 발현 출력 생성 로직의 제4 가중치 세트를 상기 유전자 발현 모델의 제3 가중치 세트로 처음부터 그리고 종단 간으로 훈련시켜 상기 복수의 생물학적 양 출력 시퀀스의 대체 표현을 처리하고 상기 유전자 발현 출력 시퀀스를 생성하는, 인공 지능 기반 시스템.In the 65th paragraph, the third weight set of the gene expression model is trained from scratch to process the plurality of biological quantity output sequences and generate alternative representations of the plurality of biological quantity output sequences, An artificial intelligence-based system, wherein the fourth weight set of the gene expression output generation logic is trained from the beginning and end-to-end with the third weight set of the gene expression model to process alternative representations of the plurality of biological quantity output sequences and generate the gene expression output sequences. 제66항에 있어서, 추론 동안, 상기 유전자 발현 모델은 상기 훈련된 제3 가중치 세트를 사용하고,상기 추론 동안, 상기 유전자 발현 출력 생성 로직은 상기 훈련된 제4 가중치 세트를 사용하는, 인공 지능 기반 시스템.In clause 66, during inference, the gene expression model uses the trained third weight set, During the above inference, the gene expression output generation logic is an artificial intelligence-based system that uses the trained fourth weight set. 제65항에 있어서, 훈련 동안, 상기 생물학적 양 모델의 제1 가중치 세트를 먼저 처음부터 훈련시켜 상기 입력 염기 시퀀스를 처리하고 상기 입력 염기 시퀀스의 대체 표현을 생성한 다음, 상기 유전자 발현 모델의 제3 가중치 세트의 치환으로 재훈련시켜 상기 복수의 생물학적 양 출력 시퀀스를 처리하고 상기 복수의 생물학적 양 출력 시퀀스의 대체 표현을 생성하고,상기 유전자 발현 출력 생성 로직의 제4 가중치 세트를 상기 유전자 발현 모델에서 치환된 상기 훈련된 제1 가중치 세트로 처음부터 그리고 종단 간으로 훈련시켜 상기 유전자 발현 모델에서 치환된 상기 훈련된 제1 가중치 세트에 의해 생성된 상기 복수의 생물학적 양 출력 시퀀스의 대체 표현을 처리하고 상기 유전자 발현 출력 시퀀스를 생성하는, 인공 지능 기반 시스템.In the 65th paragraph, during training, the first weight set of the biological quantity model is first trained from scratch to process the input base sequence and generate an alternative representation of the input base sequence, and then retrained with the substitution of the third weight set of the gene expression model to process the plurality of biological quantity output sequences and generate an alternative representation of the plurality of biological quantity output sequences. An artificial intelligence-based system for processing alternative representations of the plurality of biological quantity output sequences generated by the trained first weight set substituted in the gene expression model and generating the gene expression output sequences by training the fourth weight set of the gene expression output generating logic from the beginning and end-to-end with the trained first weight set substituted in the gene expression model. 제68항에 있어서, 추론 동안, 상기 생물학적 양 모델은 상기 재훈련된 제1 가중치 세트를 사용하고,상기 추론 동안, 상기 생물학적 양 출력 생성 로직은 상기 훈련된 제2 가중치 세트를 사용하고,추론 동안, 상기 유전자 발현 모델은 상기 재훈련된 제1 가중치 세트를 사용하고,상기 추론 동안, 상기 유전자 발현 출력 생성 로직은 상기 훈련된 제4 가중치 세트를 사용하는, 인공 지능 기반 시스템.In clause 68, during inference, the biological quantity model uses the retrained first weight set, During the above inference, the biological quantity output generation logic uses the second set of trained weights, During inference, the gene expression model uses the retrained first set of weights, During the above inference, the gene expression output generation logic is an artificial intelligence-based system that uses the trained fourth weight set. 제17항에 있어서, 훈련 동안, 상기 생물학적 양 모델의 제1 가중치 세트를 먼저 처음부터 훈련시켜 상기 입력 염기 시퀀스를 처리하고 상기 입력 염기 시퀀스의 대체 표현을 생성하고,상기 훈련 동안, 상기 생물학적 양 출력 생성 로직의 제2 가중치 세트를 먼저 상기 생물학적 양 모델의 제1 가중치 세트로 처음부터 그리고 종단 간으로 훈련시켜 상기 입력 염기 시퀀스의 대체 표현을 처리하고 상기 복수의 생물학적 양 출력 시퀀스를 생성하고,상기 훈련 동안, 그 다음, 상기 생물학적 양 모델의 훈련된 제1 가중치 세트를 재훈련시켜 상기 기준 염기 시퀀스를 처리하고 상기 기준 염기 시퀀스의 대체 표현을 생성하고, 상기 대체 염기 시퀀스를 처리하고 상기 대체 염기 시퀀스의 대체 표현을 생성하고,상기 훈련 동안, 그 다음, 상기 생물학적 양 출력 생성 로직의 훈련된 제2 가중치 세트를 상기 생물학적 양 모델의 훈련된 제1 가중치 세트로 종단 간으로 재훈련시켜 상기 기준 염기 시퀀스의 대체 표현을 처리하고 상기 복수의 기준 생물학적 양 출력 시퀀스를 생성하고, 상기 대체 염기 시퀀스의 대체 표현을 처리하고 상기 복수의 대체 생물학적 양 출력 시퀀스를 생성하는, 인공 지능 기반 시스템.In the 17th paragraph, during training, the first weight set of the biological quantity model is first trained from scratch to process the input base sequence and generate an alternative representation of the input base sequence, During the training, the second weight set of the biological quantity output generation logic is first trained from the beginning and end-to-end with the first weight set of the biological quantity model to process alternative representations of the input base sequence and generate the plurality of biological quantity output sequences. During said training, then, the trained first weight set of said biological quantity model is retrained to process said reference base sequence and generate an alternative representation of said reference base sequence, and to process said alternative base sequence and generate an alternative representation of said alternative base sequence. An artificial intelligence-based system, wherein during said training, the trained second weight set of the biological quantity output generating logic is then end-to-end retrained with the trained first weight set of the biological quantity model to process alternative representations of the reference base sequence and generate the plurality of reference biological quantity output sequences, and to process alternative representations of the alternative base sequences and generate the plurality of alternative biological quantity output sequences. 제70항에 있어서, 추론 동안, 상기 생물학적 양 모델은 상기 훈련된 제1 가중치 세트를 사용하고,상기 추론 동안, 상기 생물학적 양 출력 생성 로직은 상기 재훈련된 제2 가중치 세트를 사용하는, 인공 지능 기반 시스템.In clause 70, during inference, the biological quantity model uses the trained first weight set, An artificial intelligence-based system wherein during said inference, said biological quantity output generating logic uses said retrained second weight set. 제23항에 있어서, 상기 병원성 예측 로직은 제5 가중치 세트를 갖는, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 23, wherein the pathogenicity prediction logic has a fifth weight set. 제72항에 있어서, 훈련 동안, 상기 생물학적 양 모델의 제1 가중치 세트를 먼저 처음부터 훈련시켜 상기 입력 염기 시퀀스를 처리하고 상기 입력 염기 시퀀스의 대체 표현을 생성하고,상기 훈련 동안, 상기 생물학적 양 출력 생성 로직의 제2 가중치 세트를 먼저 상기 생물학적 양 모델의 제1 가중치 세트로 처음부터 그리고 종단 간으로 훈련시켜 상기 입력 염기 시퀀스의 대체 표현을 처리하고 상기 복수의 생물학적 양 출력 시퀀스를 생성하고,상기 훈련 동안, 그 다음, 상기 생물학적 양 모델의 훈련된 제1 가중치 세트 및 상기 생물학적 양 출력 생성 로직의 훈련된 제2 가중치 세트를 종단 간으로 재훈련시켜 대체 염기에 대한 병원성 예측을 생성하는, 인공 지능 기반 시스템.In clause 72, during training, the first weight set of the biological quantity model is first trained from scratch to process the input base sequence and generate an alternative representation of the input base sequence, During the training, the second weight set of the biological quantity output generation logic is first trained from the beginning and end-to-end with the first weight set of the biological quantity model to process alternative representations of the input base sequence and generate the plurality of biological quantity output sequences. An artificial intelligence-based system, wherein during said training, the trained first set of weights of said biological quantity model and the trained second set of weights of said biological quantity output generating logic are then retrained end-to-end to generate pathogenicity predictions for alternative bases. 제73항에 있어서, 추론 동안, 상기 생물학적 양 모델은 상기 재훈련된 제1 가중치 세트를 사용하고,상기 추론 동안, 상기 생물학적 양 출력 생성 로직은 상기 재훈련된 제2 가중치 세트를 사용하고,상기 추론 동안, 상기 병원성 예측 로직은 상기 훈련된 제5 가중치 세트를 사용하는, 인공 지능 기반 시스템.In clause 73, during inference, the biological quantity model uses the retrained first weight set, During the above inference, the biological quantity output generation logic uses the second set of retrained weights, During the above inference, the pathogenicity prediction logic is an artificial intelligence-based system using the trained fifth weight set. 제18항에 있어서, 상기 제1 각각의 염기 당 기준 생물학적 양 출력은 상기 기준 타깃 염기 시퀀스에서의 상기 각각의 위치에서 상기 각각의 기준 타깃 염기의 각각의 제1 기준 후생적 신호 수준 측정치를 특정하는, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 18, wherein the first respective base-specific reference biological quantity output specifies a first respective reference epigenetic signal level measurement of each respective reference target base at each respective position in the reference target base sequence. 제29항에 있어서, 상기 제2 각각의 염기 당 기준 생물학적 양 출력은 상기 기준 타깃 염기 시퀀스에서의 상기 각각의 위치에서 상기 각각의 기준 타깃 염기의 각각의 제2 기준 후생적 신호 수준 측정치를 특정하는, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 29, wherein the second respective base-specific reference biological quantity output specifies a second respective reference epigenetic signal level measurement of each respective reference target base at each respective position in the reference target base sequence. 제21항에 있어서, 상기 제1 각각의 염기 당 대체 생물학적 양 출력은 상기 대체 타깃 염기 시퀀스에서의 상기 각각의 위치에서 상기 각각의 대체 타깃 염기의 각각의 제1 대체 후생적 신호 수준 측정치를 특정하는, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 21, wherein each of the first alternative biological quantity outputs specifies a first alternative epigenetic signal level measurement of each of the alternative target bases at each of the positions in the alternative target base sequence. 제22항에 있어서, 상기 제2 각각의 염기 당 대체 생물학적 양 출력은 상기 대체 타깃 염기 시퀀스에서의 상기 각각의 위치에서 상기 각각의 대체 타깃 염기의 각각의 제2 대체 후생적 신호 수준 측정치를 특정하는, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 22, wherein the second respective base-specific alternative biological quantity output specifies a respective second alternative epigenetic signal level measurement of each respective alternative target base at each respective position in the alternative target base sequence. 제1항에 있어서, 훈련 동안, 상기 생물학적 양 모델 및 상기 생물학적 양 출력 생성 로직을 먼저 처음부터 그리고 종단 간으로 훈련시켜 입력 염기 시퀀스의 분석을 염기별 진화 보존 염색질 시퀀스로 변환한 다음, 종단 간으로 재훈련시켜 입력 염기 시퀀스의 분석을 염기별 전사 개시 빈도 염색질 시퀀스로 변환하는, 인공 지능 기반 시스템.An artificial intelligence-based system in accordance with claim 1, wherein during training, the biological quantity model and the biological quantity output generation logic are first trained from scratch and end-to-end to convert analysis of an input base sequence into base-wise evolutionary conserved chromatin sequences, and then retrained end-to-end to convert analysis of an input base sequence into base-wise transcription initiation frequency chromatin sequences. 제1항에 있어서, 훈련 동안, 상기 생물학적 양 모델 및 상기 생물학적 양 출력 생성 로직을 먼저 처음부터 그리고 종단 간으로 훈련시켜 입력 염기 시퀀스의 분석을 염기별 후생적 신호 수준 염색질 시퀀스로 변환한 다음, 종단 간으로 재훈련시켜 입력 염기 시퀀스의 분석을 염기별 진화 보존 염색질 시퀀스로 변환하는, 인공 지능 기반 시스템.An artificial intelligence-based system, wherein, during training in the first aspect, the biological quantity model and the biological quantity output generation logic are first trained from scratch and end-to-end to convert the analysis of the input base sequence into a base-wise epigenetic signal level chromatin sequence, and then retrained end-to-end to convert the analysis of the input base sequence into a base-wise evolutionary conserved chromatin sequence. 제1항에 있어서, 훈련 동안, 상기 생물학적 양 모델 및 상기 생물학적 양 출력 생성 로직을 먼저 처음부터 그리고 종단 간으로 훈련시켜 입력 염기 시퀀스의 분석을 염기별 후생적 신호 수준 염색질 시퀀스로 변환한 다음, 종단 간으로 재훈련시켜 입력 염기 시퀀스의 분석을 염기별 전사 개시 빈도 염색질 시퀀스로 변환하는, 인공 지능 기반 시스템.An artificial intelligence-based system, wherein, during training in the first paragraph, the biological quantity model and the biological quantity output generation logic are first trained from scratch and end-to-end to convert the analysis of the input base sequence into a base-wise epigenetic signal level chromatin sequence, and then retrained end-to-end to convert the analysis of the input base sequence into a base-wise transcription initiation frequency chromatin sequence. 제1항에 있어서, 훈련 동안, 상기 생물학적 양 모델 및 상기 생물학적 양 출력 생성 로직을 먼저 처음부터 그리고 종단 간으로 훈련시켜 입력 염기 시퀀스의 분석을 염기별 후생적 신호 수준 염색질 시퀀스로 변환한 다음, 종단 간으로 재훈련시켜 입력 염기 시퀀스의 분석을 염기별 진화 보존 염색질 시퀀스 및 염기별 전사 개시 빈도 염색질 시퀀스로 변환하는, 인공 지능 기반 시스템.An artificial intelligence-based system, wherein, during training in the first aspect, the biological quantity model and the biological quantity output generation logic are first trained from scratch and end-to-end to convert the analysis of the input base sequence into a base-wise epigenetic signal level chromatin sequence, and then retrained end-to-end to convert the analysis of the input base sequence into a base-wise evolutionary conserved chromatin sequence and a base-wise transcription initiation frequency chromatin sequence. 제1항에 있어서, 복수의 후생적 효과에 의해 교란된 변이체를 포함하는 훈련 입력 기본 시퀀스의 제1 훈련 세트를 포함하도록 추가로 구성되는, 인공 지능 기반 시스템.An artificial intelligence-based system, further configured to include a first training set of training input base sequences comprising variants perturbed by multiple epigenetic effects, in the first aspect. 제83항에 있어서, 상기 복수의 후생적 효과에서의 후생적 효과는 염색체 간 효과, 유전자 내 효과, 집단 구조 및 조상 효과, 발현 잔차의 확률적 추정(PEER) 효과, 환경 효과, 성별 효과, 일괄 효과, 유전자형 플랫폼 효과, 및/또는 라이브러리 구축 프로토콜 효과를 포함하고, PEER는 '발현 잔차의 확률적 추정'을 의미하고, 인자 분석 방법을 사용하여 유전자 발현 프로파일에서 숨겨진 결정요인과 해당 효과를 추론하는 Bayesian 접근법의 모음인, 인공 지능 기반 시스템.In claim 83, the epigenetic effects in the plurality of epigenetic effects include interchromosomal effects, intragenic effects, population structure and ancestral effects, probabilistic estimation of expression residuals (PEER) effects, environmental effects, sex effects, batch effects, genotype platform effects, and/or library construction protocol effects, where PEER stands for 'probabilistic estimation of expression residuals' and is an artificial intelligence-based system that is a collection of Bayesian approaches for inferring hidden determinants and their effects from gene expression profiles using factor analysis methods. 제83항에 있어서, 상기 복수의 후생적 효과에 의해 교란되지 않은 변이체를 포함하는 훈련 입력 염기 시퀀스의 제2 훈련 세트를 포함하도록 추가로 구성되는, 인공 지능 기반 시스템.An artificial intelligence-based system, further configured to include a second training set of training input base sequences comprising variants that are not perturbed by the plurality of epigenetic effects, in claim 83. 제85항에 있어서, 상기 제2 훈련 세트에서의 상기 변이체는 유전자 발현을 변경하고 극한 수준의 유전자 발현을 유발하도록 신뢰성 있게 결정되는, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 85, wherein the variant in the second training set is reliably determined to alter gene expression and cause extreme levels of gene expression. 제86항에 있어서, 상기 제2 훈련 세트에서의 상기 변이체는 유전자 발현 수준을 증가시키는 과다 발현 유발 변이체를 포함하는, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 86, wherein the variants in the second training set include overexpression-inducing variants that increase gene expression levels. 제86항에 있어서, 상기 제2 훈련 세트에서의 상기 변이체는 유전자 발현 수준을 감소시키는 과소 발현 유발 변이체를 포함하는, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 86, wherein the variants in the second training set include underexpression-causing variants that reduce gene expression levels. 제87항에 있어서, 상기 제2 훈련 세트는 상기 유발 유전자 과다 발현 가능성을 특정하는 상기 변이체에 대한 과다 발현 확률을 특정하는, 인공 지능 기반 시스템.In claim 87, the second training set is an artificial intelligence-based system that specifies an overexpression probability for the variant that specifies the possibility of overexpression of the causative gene. 제88항에 있어서, 상기 제2 훈련 세트는 상기 변이체 유발 유전자 과소 발현 가능성을 특정하는 상기 변이체에 대한 과소 발현 확률을 특정하는, 인공 지능 기반 시스템.In claim 88, the second training set is an artificial intelligence-based system that specifies an underexpression probability for the variant that specifies the possibility of underexpression of the gene causing the variant. 제86항에 있어서, 상기 제2 훈련 세트에서의 각각의 변이체는 이상치 개체의 코호트 중 단지 하나의 이상치 개체에서 발생하는 단독 변이체이고,이상치 개체의 코호트에서 이상치 개체는 극한 수준의 유전자 발현을 나타내는, 인공 지능 기반 시스템.In clause 86, each variant in the second training set is a single variant occurring in only one outlier individual among the cohort of outlier individuals, An artificial intelligence-based system that identifies outliers in a cohort of outlier individuals, where the outlier individuals exhibit extreme levels of gene expression. 제91항에 있어서, 극한 수준의 유전자 발현은 정규화된 유전자 발현 수준의 꼬리 사분위로부터 결정되는, 인공 지능 기반 시스템.An artificial intelligence-based system, wherein in clause 91, extreme levels of gene expression are determined from tail quartiles of normalized gene expression levels. 제91항에 있어서, 극한 수준의 유전자 발현은 과다 유전자 발현 및 과소 유전자 발현을 포함하는, 인공 지능 기반 시스템.An artificial intelligence-based system, wherein extreme levels of gene expression in claim 91 include over-expression and under-expression of the gene. 제91항에 있어서, 상기 단일 변이체는 코딩 변이체인, 인공 지능 기반 시스템.An artificial intelligence-based system, wherein the single variant is a coding variant in claim 91. 제91항에 있어서, 상기 단일 변이체는 비-코딩 변이체인, 인공 지능 기반 시스템.An artificial intelligence-based system, wherein the single variant in claim 91 is a non-coding variant. 제95항에 있어서, 상기 비-코딩 변이체는 5개의 프라임 비변환 영역(UTR) 변이체, 3개의 프라임 UTR 변이체, 인핸서 변이체, 또는 프로모터 변이체인, 인공 지능 기반 시스템.An artificial intelligence-based system according to claim 95, wherein the non-coding variants are five prime untranslated region (UTR) variants, three prime UTR variants, an enhancer variant, or a promoter variant. 제85항에 있어서, 상기 제2 훈련 세트에서의 상기 변이체는 복수의 조직 유형에 걸쳐 있는, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 85, wherein the variants in the second training set span multiple tissue types. 제85항에 있어서, 상기 제2 훈련 세트에서의 상기 변이체는 복수의 셀 유형에 걸쳐 있는, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 85, wherein the variants in the second training set span multiple cell types. 제1항에 있어서, 상기 입력 염기 시퀀스 및 상기 복수의 생물학적 양 출력 시퀀스는 상기 복수의 조직 유형에 걸쳐 있는, 인공 지능 기반 시스템.An artificial intelligence-based system in the first aspect, wherein the input base sequence and the plurality of biological quantity output sequences span the plurality of tissue types. 제1항에 있어서, 상기 입력 염기 시퀀스 및 상기 복수의 생물학적 양 출력 시퀀스는 상기 복수의 셀 유형에 걸쳐 있는, 인공 지능 기반 시스템.An artificial intelligence-based system in the first aspect, wherein the input base sequence and the plurality of biological quantity output sequences span the plurality of cell types. 제10항에 있어서, 상기 유전자 발현 출력 시퀀스는 상기 복수의 조직 유형에 걸쳐 있는, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 10, wherein the gene expression output sequence spans the plurality of tissue types. 제10항에 있어서, 상기 유전자 발현 출력 시퀀스는 상기 복수의 셀 유형에 걸쳐 있는, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 10, wherein the gene expression output sequence spans the plurality of cell types. 제1항에 있어서, 상기 생물학적 양 모델 및 상기 생물학적 양 출력 생성 로직을 먼저 상기 제1 훈련 세트에서 종단 간으로 훈련시킨 다음, 상기 제2 훈련 세트에서 재훈련시키는, 인공 지능 기반 시스템.An artificial intelligence-based system, wherein in the first paragraph, the biological quantity model and the biological quantity output generation logic are first trained end-to-end in the first training set, and then retrained in the second training set. 제1항에 있어서, 상기 제2 훈련 세트에서의 상기 변이체는 유전자 발현 변동을 나타내는 제1 기준 진리 라벨로 라벨링된 병원성 세트로서 사용되고, 공통 변이체는 유전자 발현 비변동을 나타내는 제2 기준 진리 라벨로 라벨링된 양성 세트로서 사용되는, 인공 지능 기반 시스템.An artificial intelligence-based system in the first aspect, wherein the variants in the second training set are used as a pathogenic set labeled with a first reference truth label indicating gene expression variation, and the common variants are used as a benign set labeled with a second reference truth label indicating gene expression non-variation. 제104항에 있어서, 상기 양성 세트는 트리뉴클레오티드 컨텍스트, 동종 중합체, k-mer, 이웃 GC 빈도 및 시퀀싱 깊이에 대해 균형을 이루는, 인공 지능 기반 시스템.An artificial intelligence-based system, wherein the positive set is balanced for trinucleotide context, homopolymer, k-mer, neighboring GC frequency and sequencing depth in clause 104. 제104항에 있어서, 상기 과다 발현 확률 및 상기 과소 발현 확률에 적용된 컷오프 확률에 기초하여, 상기 제2 훈련 세트에서의 상기 변이체는 유전자 발현 증가를 나타내는 제1 기준 진리 라벨을 갖는 과다 발현 변이체 훈련 세트, 유전자 발현 감소를 나타내는 제2 기준 진리 라벨을 갖는 과다 발현 변이체 훈련 세트, 및 유전자 발현 유지를 나타내는 신경 발현 변이체 훈련 세트로 분할되는, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 104, wherein, based on the cutoff probabilities applied to the overexpression probability and the underexpression probability, the variants in the second training set are divided into a training set of overexpressed variants having a first reference truth label indicating increased gene expression, a training set of overexpressed variants having a second reference truth label indicating decreased gene expression, and a training set of neural expression variants indicating maintained gene expression. 제10항에 있어서, 상기 유전자 발현 모델 및 상기 유전자 발현 출력 생성 로직은 먼저 상기 제1 훈련 세트에서 종단 간으로 훈련시킨 다음, 상기 제2 훈련 세트에서 재훈련되는, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 10, wherein the gene expression model and the gene expression output generation logic are first trained end-to-end in the first training set and then retrained in the second training set. 제1항에 있어서, 상기 생물학적 양 모델 및 상기 생물학적 양 출력 생성 로직을 먼저 상기 제1 훈련 세트에서 종단 간으로 훈련시킨 다음, 홀수 염색체에서 발생하는 상기 제2 훈련 세트에서의 해당 변이체에 대해 재훈련시키는, 인공 지능 기반 시스템.An artificial intelligence-based system, wherein in the first paragraph, the biological quantity model and the biological quantity output generation logic are first trained end-to-end in the first training set, and then retrained for the corresponding variants in the second training set occurring in odd chromosomes. 제10항에 있어서, 상기 유전자 발현 모델 및 상기 유전자 발현 출력 생성 로직을 먼저 상기 제1 훈련 세트에서 종단 간으로 훈련시킨 다음, 홀수 염색체에서 발생하는 상기 제2 훈련 세트에서의 해당 변이체에 대해 재훈련시키는, 인공 지능 기반 시스템.An artificial intelligence-based system in claim 10, wherein the gene expression model and the gene expression output generation logic are first trained end-to-end in the first training set, and then retrained for the corresponding variants occurring in odd chromosomes in the second training set. 제85항에 있어서, 상기 제2 훈련 세트에서의 상기 변이체를 훈련을 위해 사용하지 않고 대신에 상기 훈련된 생물학적 양 모델(124), 상기 훈련된 생물학적 양 출력 생성 로직, 상기 훈련된 유전자 발현 모델, 및 상기 훈련된 유전자 발현 출력 생성 로직의 성능을 평가하기 위한 검증 세트로서 사용하는, 인공 지능 기반 시스템.An artificial intelligence-based system, wherein in clause 85, the variants in the second training set are not used for training, but instead are used as a validation set to evaluate the performance of the trained biological quantity model (124), the trained biological quantity output generation logic, the trained gene expression model, and the trained gene expression output generation logic. 제110항에 있어서, 짝수 염색체에서 발생하는 상기 제2 훈련 세트에서의 해당 변이체를 상기 검증 세트로 사용하는, 인공 지능 기반 시스템.An artificial intelligence-based system, wherein the variants in the second training set occurring in even-numbered chromosomes are used as the validation set in clause 110. 제1항에 있어서, 상기 타깃 염기 시퀀스의 크기는 전사 시작 부위(TSS)의 다양한 오프셋 위치를 고려하기 위해 훈련 동안 가변되는, 인공 지능 기반 시스템.An artificial intelligence-based system, wherein the size of the target base sequence in the first aspect is varied during training to consider various offset positions of the transcription start site (TSS). 염기 해상도에서 유전자 발현의 변화를 검출하는 인공 지능 기반 시스템으로서,시퀀스 데이터베이스에 액세스하고 입력 염기 시퀀스를 생성하는 입력 생성 로직으로서, 상기 입력 염기 시퀀스는 타깃 염기 시퀀스를 포함하고, 상기 타깃 염기 시퀀스는 하류 컨텍스트 염기를 갖는 우측 염기 시퀀스 및 상류 컨텍스트 염기를 갖는 좌측 염기 시퀀스에 의해 플랭킹되는, 상기 입력 생성 로직;상기 입력 염기 시퀀스를 처리하고 상기 입력 염기 시퀀스의 대체 표현을 생성하는 생물학적 양 모델; 및상기 입력 염기 시퀀스의 대체 표현을 처리하고 복수의 생물학적 양 출력 시퀀스를 생성하는 생물학적 양 출력 생성 로직을 포함하되, 상기 복수의 생물학적 양 출력 시퀀스에서 각각의 생물학적 양 출력 시퀀스는 상기 타깃 염기 시퀀스에서의 각각의 타깃 염기에 대한 각각의 염기 당 생물학적 양 출력을 포함하는, 인공 지능 기반 시스템.An artificial intelligence-based system for detecting changes in gene expression at base resolution, An input generating logic for accessing a sequence database and generating an input base sequence, wherein the input base sequence includes a target base sequence, and the target base sequence is flanked by a right base sequence having a downstream context base and a left base sequence having an upstream context base; A biological quantity model that processes the input base sequence and generates an alternative representation of the input base sequence; and Comprising biological quantity output generation logic for processing alternative representations of the input base sequence and generating a plurality of biological quantity output sequences, An artificial intelligence-based system, wherein each biological quantity output sequence in the plurality of biological quantity output sequences comprises a biological quantity output per base for each target base in the target base sequence. 제113항에 있어서, 상기 복수의 생물학적 양 출력 시퀀스에서의 제1 생물학적 양 출력 시퀀스는 상기 타깃 염기 시퀀스에서의 상기 각각의 타깃 염기에 대한 제1 각각의 염기 당 생물학적 양 출력을 포함하고,상기 제1 각각의 염기 당 생물학적 양 출력은 복수의 종에 걸쳐 상기 각각의 타깃 염기의 각각의 진화 보존 측정치를 특정하는, 인공 지능 기반 시스템.In the 113th paragraph, the first biological quantity output sequence in the plurality of biological quantity output sequences comprises a first biological quantity output per base for each of the target bases in the target base sequence, An artificial intelligence-based system wherein each of the first base-specific biological quantity outputs specifies a measure of evolutionary conservation of each of the target bases across multiple species. 제113항에 있어서, 상기 복수의 생물학적 양 출력 시퀀스에서의 제2 생물학적 양 출력 시퀀스는 상기 타깃 염기 시퀀스에서의 상기 각각의 타깃 염기에 대한 제2 각각의 염기 당 생물학적 양 출력을 포함하고,상기 제2 각각의 염기 당 생물학적 양 출력은 상기 타깃 염기 시퀀스에서의 각각의 위치에서 상기 각각의 타깃 염기의 각각의 전사 개시 측정치를 특정하는, 인공 지능 기반 시스템.In the 113th paragraph, the second biological quantity output sequence in the plurality of biological quantity output sequences comprises a second respective base-wise biological quantity output for each target base in the target base sequence, An artificial intelligence-based system wherein each of the second respective base-specific biological quantity outputs specifies a measurement of each transcription initiation of each of the target bases at each position in the target base sequence.
